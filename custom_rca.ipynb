{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7903093c-238a-46d8-9629-1c94c98888c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "# from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebca610b-10e3-4b06-9bbc-ea4d93769b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('pmodata_300k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3352b1b-9997-4832-894c-3efa9b9d0022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147007, 27)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df[df['initforward'].isna()==False]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51ec117-68ef-46ee-938f-0511ae9656a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b0779c-15cf-4312-9f29-7e002a9fdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords1=set(stopwords.words('english'))\n",
    "stopwords2=set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "all_stopwords=set(stopwords1|stopwords2)\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "def decontract(sentence):\n",
    "    sentence=re.sub(r\"n\\'t\",\" not\",sentence)\n",
    "    sentence=re.sub(r\"\\'re\",\" are\",sentence)\n",
    "    sentence=re.sub(r\"\\'s\",\" is\",sentence)\n",
    "    sentence=re.sub(r\"\\'d\",\" would\",sentence)\n",
    "    sentence=re.sub(r\"\\'ll\",\" will\",sentence)\n",
    "    sentence=re.sub(r\"\\'ve\",\" have\",sentence)\n",
    "    sentence=re.sub(r\"\\'m\",\" am\",sentence) \n",
    "    return sentence\n",
    "\n",
    "def preprocess(desc):\n",
    "    desc=re.sub(r\"http\\S+\",\"\",desc)\n",
    "    desc=decontract(desc)\n",
    "    desc=re.sub(\"\\S*\\d\\S*\",\"\",desc).strip()\n",
    "    desc=re.sub('[^A-Za-z]+',' ',desc)\n",
    "    desc=' '.join(m.lower() for m in word_tokenize(desc) if m.lower() not in all_stopwords)\n",
    "    desc=' '.join(lemmatizer.lemmatize(m,pos='a') for m in word_tokenize(desc) if m not in all_stopwords and len(m)>2)\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdf45de-34c4-48b1-b241-b3b7aecf5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description'] = data['description'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcccf529-e106-4998-830b-424ee3b66855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERTopic model\n",
    "model = BERTopic()\n",
    "\n",
    "# Fit the model on the entire dataset to obtain embeddings\n",
    "descriptions = data['description'].tolist()\n",
    "topics, embeddings = model.fit_transform(descriptions)\n",
    "\n",
    "# Store the embeddings of the complete dataset\n",
    "stored_embeddings = embeddings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de83d9a-9445-4016-9fbd-dc89142f1f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Specify the filename for the saved file\n",
    "filename = 'embeddings.pkl'\n",
    "\n",
    "# Save the embeddings using pickle\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(stored_embeddings, file)\n",
    "\n",
    "print(f\"Embeddings saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3839157b-5b7c-4988-b997-a5868f4e6f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1. DEABD - Department of Financial Services (Banking Division)',\n",
       "       '1. GOVAP - Government of Andhra Pradesh',\n",
       "       '1. GOVUP - Government of Uttar Pradesh', ...,\n",
       "       '1. DHRES - Department of Health Research, 2. GOVGJ - Government of Gujarat',\n",
       "       '1. DOPAT - Department of Personnel and Training, 2. DOTEL - Department of Telecommunications',\n",
       "       '1. DORVU - Department of Revenue, 2. DOTEL - Department of Telecommunications'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(data)\n",
    "#data.iloc[12099]['regdate']\n",
    "unique_values = data['initforward'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2f4b3-3360-4aa5-b4ef-b410ec55840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.columns\n",
    "#data.iloc[10013]['initforward']\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings):\n",
    "    # Convert the 'regdate' column to datetime\n",
    "    data['regdate'] = pd.to_datetime(data['regdate'])\n",
    "\n",
    "    # Filter the data based on the date range and initforward match\n",
    "    filtered_data = data[(data['regdate'] >= pd.to_datetime(starting_date)) &\n",
    "                         (data['regdate'] <= pd.to_datetime(ending_date)) &\n",
    "                         (data['initforward'] == initforward_str)]\n",
    "\n",
    "    # Get the indices of the filtered data\n",
    "    indices = filtered_data.index.tolist()\n",
    "\n",
    "    # Ensure indices are within the range of stored_embeddings length\n",
    "    valid_indices = [index for index in indices if index < len(stored_embeddings)]\n",
    "\n",
    "    # Retrieve embeddings for the filtered data using valid indices\n",
    "    obtained_embeddings = stored_embeddings[valid_indices]\n",
    "\n",
    "    return obtained_embeddings\n",
    "\n",
    "# User inputs\n",
    "starting_date = input(\"Enter the starting date (YYYY-MM-DD): \")\n",
    "ending_date = input(\"Enter the ending date (YYYY-MM-DD): \")\n",
    "initforward_str = input(\"Enter the initforward string: \")\n",
    "\n",
    "# Filter the DataFrame to remove rows where 'initforward' is NaN\n",
    "data = df[df['initforward'].isna() == False]\n",
    "\n",
    "# Get the embeddings\n",
    "obtained_embeddings = get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8eacedce-997a-4cc0-b228-95014e5901de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 22:32:21,712 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-01-23 22:32:58,365 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-01-23 22:32:58,369 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-01-23 22:32:59,259 - BERTopic - Cluster - Completed ✓\n",
      "2024-01-23 22:32:59,261 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-01-23 22:32:59,484 - BERTopic - Representation - Completed ✓\n",
      "2024-01-23 22:32:59,486 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-01-23 22:32:59,647 - BERTopic - Topic reduction - Reduced number of topics from 183 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  doc0, doc5771, doc5815, doc5818, doc5803, doc5825, doc5770, doc580, doc5799, doc5798\n",
      "Topic 1:  doc1119, doc1393, doc7557, doc7512, doc6962, doc6848, doc615, doc5946, doc5664, doc5482\n",
      "Topic 2:  doc3778, doc4680, doc7929, doc7746, doc7119, doc6820, doc653, doc6345, doc6281, doc5551\n",
      "Topic 3:  doc3474, doc3799, doc7710, doc6926, doc6669, doc654, doc637, doc5781, doc5632, doc5353\n",
      "Topic 4:  doc2824, doc2990, doc898, doc786, doc780, doc6726, doc6625, doc5911, doc5626, doc4846\n",
      "Topic 5:  doc117, doc1282, doc6554, doc6507, doc6429, doc5851, doc5453, doc4879, doc4825, doc4261\n",
      "Topic 6:  doc192, doc2379, doc7015, doc6748, doc6237, doc620, doc6061, doc553, doc4596, doc4469\n",
      "Topic 7:  doc86, doc7588, doc7188, doc7157, doc6654, doc5614, doc5458, doc4987, doc4464, doc2755\n",
      "Topic 8:  doc7897, doc6127, doc4494, doc4332, doc4294, doc4090, doc3761, doc3285, doc3208, doc1995\n"
     ]
    }
   ],
   "source": [
    "# obtained_embeddings now contains the embeddings for the filtered data\n",
    "#obtained_embeddings.shape\n",
    "#from bertopic import BERTopic\n",
    "# Check if the embeddings are 1D and reshape them to 2D if necessary\n",
    "if len(obtained_embeddings.shape) == 1:\n",
    "    obtained_embeddings = obtained_embeddings.reshape(-1, 1)\n",
    "\n",
    "# Initialize a new BERTopic model\n",
    "topic_model = BERTopic(nr_topics=10, calculate_probabilities=False, verbose=True)\n",
    "\n",
    "# Create dummy documents with the same length as obtained_embeddings\n",
    "dummy_documents = [\"doc\" + str(i) for i in range(len(obtained_embeddings))]\n",
    "\n",
    "# Fit the BERTopic model on the dummy documents and obtained embeddings\n",
    "topics, _ = topic_model.fit_transform(dummy_documents, obtained_embeddings)\n",
    "\n",
    "# Extract the topics and their frequencies\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Get the top 10 topics\n",
    "top_topics = topic_info.head(11)  # Get 11 as one topic might be -1 (outliers)\n",
    "\n",
    "# Print the top 10 topics and their top 10 words\n",
    "for topic in top_topics['Topic']:\n",
    "    if topic != -1:  # excluding the outlier cluster\n",
    "        # Get top 10 words for the topic\n",
    "        topic_words = topic_model.get_topic(topic)[:10]\n",
    "        # Extract only the words (not the scores)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        # Print the topic and its words\n",
    "        print(f\"Topic {topic}: \", \", \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815ac0b-3901-480b-b293-ba22c88c297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters (topics)\n",
    "num_clusters = 10\n",
    "\n",
    "# Fit KMeans to the embeddings\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(obtained_embeddings)\n",
    "\n",
    "# Predict the clusters\n",
    "clusters = kmeans.predict(obtained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edf123-f0d8-4a7e-94f5-0643b7c6d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the cluster labels to the original data\n",
    "data['Cluster'] = clusters\n",
    "\n",
    "# Examine and interpret each cluster\n",
    "for i in range(num_clusters):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    cluster_data = data[data['Cluster'] == i]\n",
    "    \n",
    "    # Here you might want to print out some representative documents or\n",
    "    # any other statistics that could help you understand what the cluster is about\n",
    "    # For example:\n",
    "    print(cluster_data['description'].head())  # Adjust based on your data structure\n",
    "    \n",
    "    # Add any additional analysis you find necessary\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27365933-75a1-4f7a-b3b3-89c90f874fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'initforward' value with the maximum data points is: '1. DEABD - Department of Financial Services (Banking Division)'\n",
      "Count: 12979\n"
     ]
    }
   ],
   "source": [
    "stored_embeddings.shape\n",
    "# Get the counts of unique values in the 'initforward' column\n",
    "initforward_counts = data['initforward'].value_counts()\n",
    "\n",
    "# Retrieve the top value (the one with the maximum count)\n",
    "top_initforward = initforward_counts.idxmax()\n",
    "\n",
    "# Print the value and its count\n",
    "print(f\"The 'initforward' value with the maximum data points is: '{top_initforward}'\")\n",
    "print(f\"Count: {initforward_counts[top_initforward]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45bb0e0-8f7f-41b8-a3bd-6519fcb5cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "model = BERTopic(embedding_model=embedding_model)\n",
    "\n",
    "# Fit the model \n",
    "descriptions = data['description'].tolist()\n",
    "topics, embeddings = model.fit_transform(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1f2c530-53cb-44bf-b5b3-36cfc3504963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of topic embeddings: (10, 1)\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Shape of embeddings: (147007, 1)\n"
     ]
    }
   ],
   "source": [
    "# # After fitting, you can access the topic embeddings\n",
    "# topic_embeddings = model.topic_embeddings\n",
    "\n",
    "# # Store the topic embeddings\n",
    "# stored_embeddings = topic_embeddings.copy()\n",
    "# Store the embeddings of the complete dataset\n",
    "#stored_embeddings = embeddings.copy()\n",
    "\n",
    "# # Calculate topic embeddings by averaging document embeddings per topic\n",
    "# topic_embeddings = np.zeros((len(set(topics)), embeddings.shape[1]))\n",
    "\n",
    "# for topic in set(topics):\n",
    "#     if topic != -1:  # Exclude the outlier topic if present\n",
    "#         indices = [i for i, t in enumerate(topics) if t == topic]\n",
    "#         topic_embeddings[topic] = np.mean(embeddings[indices], axis=0)\n",
    "\n",
    "# # Store the topic embeddings\n",
    "# stored_embeddings = topic_embeddings.copy()\n",
    "\n",
    "# # Check the shape of the topic embeddings\n",
    "# print(\"Shape of topic embeddings:\", stored_embeddings.shape)\n",
    "\n",
    "# Ensure embeddings are in a 2D numpy array\n",
    "if isinstance(embeddings, list):\n",
    "    embeddings = np.array(embeddings)\n",
    "elif len(embeddings.shape) == 1:\n",
    "    embeddings = embeddings.reshape(-1, 1)\n",
    "\n",
    "# Calculate topic embeddings by averaging document embeddings per topic\n",
    "topic_embeddings = np.zeros((len(set(topics)), embeddings.shape[1]))\n",
    "\n",
    "for topic in set(topics):\n",
    "    if topic != -1:  # Exclude the outlier topic if present\n",
    "        indices = [i for i, t in enumerate(topics) if t == topic]\n",
    "        topic_embeddings[topic] = np.mean(embeddings[indices], axis=0)\n",
    "\n",
    "# Store the topic embeddings\n",
    "stored_topic_embeddings = topic_embeddings.copy()\n",
    "\n",
    "# Check the shape of the topic embeddings\n",
    "print(\"Shape of topic embeddings:\", stored_topic_embeddings.shape)\n",
    "\n",
    "print(embeddings[:5])  # Print the first 5 embeddings to inspect their structure\n",
    "print(\"Shape of embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935326da-2b3f-49a5-9b49-8abc5ece6299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a7b388e0d94d30b02e8811f3eeb8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of the first document: [ 3.33634801e-02  3.26836035e-02 -1.47670329e-01 -5.74986413e-02\n",
      " -1.50998291e-02  9.03493986e-02 -1.49062956e-02  3.87717523e-02\n",
      " -6.60005882e-02 -2.33104248e-02  4.36904095e-02 -3.26045975e-02\n",
      "  1.02416873e-02 -3.21899168e-02  3.10279876e-02  4.64709140e-02\n",
      " -2.35015675e-02  5.66720869e-03 -1.33569585e-02 -1.71295926e-02\n",
      " -6.29320815e-02 -5.44602610e-03 -4.39039879e-02 -7.34623969e-02\n",
      "  6.01073168e-02  1.06643289e-02 -6.87981769e-03 -3.61237861e-02\n",
      "  8.59590471e-02  2.41204724e-02  8.99317563e-02  3.37226968e-03\n",
      " -8.59604254e-02  9.26114339e-03  5.33274375e-02 -5.64609468e-03\n",
      " -5.13846204e-02  4.01779637e-02  1.33804064e-02 -3.41936275e-02\n",
      "  6.69373497e-02 -9.23117995e-03 -3.76868956e-02 -5.05258106e-02\n",
      "  8.15688893e-02 -6.59956015e-04 -1.52508682e-02  3.76336537e-02\n",
      " -2.72375327e-02 -9.93224513e-03 -3.60711291e-02 -1.07922591e-01\n",
      "  3.27603593e-02  2.42191833e-02 -2.83576902e-02 -2.51412056e-02\n",
      "  1.07735604e-01  2.52739526e-02 -7.39892013e-03 -5.91449700e-02\n",
      "  4.30783490e-03  4.78509218e-02 -4.57275622e-02 -6.45280778e-02\n",
      " -9.90909990e-03 -2.22418942e-02  1.62733182e-01  2.60480382e-02\n",
      "  9.88186374e-02  3.33565697e-02 -7.48971803e-03 -5.17663732e-02\n",
      " -7.77705088e-02  1.24068381e-02 -7.31391832e-02  4.49449308e-02\n",
      "  8.50767887e-04  5.81623465e-02  2.10178504e-03 -2.60126237e-02\n",
      "  2.67513469e-02 -7.99448118e-02 -6.28060708e-03  2.44130213e-02\n",
      " -2.58570220e-02  2.93225460e-02 -5.43162599e-03 -4.48666373e-03\n",
      "  9.96306632e-03 -5.54599501e-02  4.88988198e-02  6.05290420e-02\n",
      "  4.22655381e-02 -7.40050003e-02  2.83347606e-03 -3.96359572e-03\n",
      " -2.74481922e-02 -2.21216325e-02 -2.42317673e-02  2.42628828e-02\n",
      " -5.10869361e-02 -2.05272459e-03 -2.27974858e-02 -3.58018242e-02\n",
      " -8.84638801e-02  4.40401509e-02 -4.54085916e-02  4.09947261e-02\n",
      " -3.73975076e-02  1.41745387e-02 -4.27747257e-02 -4.71249484e-02\n",
      " -2.16516219e-02  4.24824841e-02  5.84009383e-03  3.01104598e-02\n",
      " -1.38243690e-01 -3.98933142e-03 -5.21972515e-02 -2.50813216e-02\n",
      "  3.35669629e-02  1.13758340e-01 -7.37647414e-02 -5.00195324e-02\n",
      " -7.69172534e-02  2.52897032e-02  1.33453105e-02  4.87280612e-33\n",
      "  4.15816456e-02  6.31936034e-03 -8.60380679e-02 -1.70042310e-02\n",
      "  6.52705878e-02  4.63622157e-03  1.01223014e-01 -4.40584533e-02\n",
      "  7.68847705e-04 -3.06688827e-02  5.49477562e-02 -6.05915487e-02\n",
      "  8.37193429e-03 -1.28981337e-01 -1.44680128e-01 -1.49719650e-02\n",
      "  2.96426062e-02  2.90870965e-02 -4.95823333e-03  6.58210441e-02\n",
      "  3.12909409e-02  5.64341396e-02  8.69794190e-02 -3.41394693e-02\n",
      "  7.77511969e-02  1.00172106e-02 -1.66412559e-03 -5.50594814e-02\n",
      "  2.83107851e-02  1.41681060e-02  2.92278565e-02  3.41135338e-02\n",
      "  2.53431462e-02 -4.60214615e-02  1.43168503e-02 -1.35121346e-02\n",
      "  2.59308107e-02 -6.95462748e-02 -3.21767256e-02 -2.86571793e-02\n",
      "  5.46149425e-02  3.61730307e-02  4.27463762e-02  6.47441670e-02\n",
      " -2.97586177e-03  6.03559194e-03  2.23639086e-02  5.36925159e-02\n",
      "  4.53744642e-02  3.09159160e-02 -7.93798193e-02 -2.46991217e-02\n",
      " -1.34562790e-01 -3.82598042e-02 -1.12096788e-02 -5.00693768e-02\n",
      " -6.74136132e-02 -1.43184122e-02 -1.67693626e-02 -6.42782375e-02\n",
      "  1.75598618e-02 -8.25428069e-02 -5.14872260e-02  1.01271020e-02\n",
      " -2.52325460e-02 -9.31671634e-02 -7.42921140e-03  1.83829237e-02\n",
      " -5.37094772e-02 -6.42713010e-02  4.67547439e-02 -2.45604403e-02\n",
      "  1.03955440e-01 -2.22850908e-02 -2.77826339e-02  6.29877253e-03\n",
      "  1.97240654e-02  7.02449381e-02 -6.77042007e-02  1.34256305e-02\n",
      " -2.28953119e-02  3.76866236e-02 -2.02746764e-02 -8.85372609e-03\n",
      "  7.05468059e-02  9.21257287e-02  3.24217379e-02 -4.13980260e-02\n",
      " -4.03573737e-02 -3.29442807e-02 -2.28347797e-02 -6.30561449e-03\n",
      "  1.18037783e-01  4.50439192e-02  5.91956936e-02 -6.07464073e-33\n",
      " -1.38161157e-03 -4.16550562e-02 -9.66656115e-03 -1.28245596e-02\n",
      "  9.42409039e-02  1.03059448e-02  3.40342857e-02 -4.07014899e-02\n",
      "  3.92049029e-02  2.56516528e-03 -1.13959657e-02  2.69750599e-02\n",
      "  2.48064287e-02  1.38422083e-02 -2.80136224e-02 -9.63671412e-03\n",
      "  6.75269887e-02  7.29910806e-02  3.38596404e-02 -2.05983017e-02\n",
      "  5.19762412e-02  4.41273637e-02  7.62852328e-03  1.33744493e-01\n",
      "  5.66053949e-02  3.59522155e-03  2.79883947e-02 -1.02478676e-02\n",
      " -2.36371476e-02  6.93930462e-02  5.22497296e-02 -5.39646670e-02\n",
      " -4.18737009e-02  1.36561953e-02 -4.38336022e-02 -5.06119654e-02\n",
      "  7.69425705e-02 -4.90034595e-02 -2.37735175e-03  4.53719385e-02\n",
      " -3.59648839e-02  3.57142538e-02 -1.55231915e-02 -3.38808261e-02\n",
      "  3.32990848e-02 -1.77953243e-02  2.85394397e-02  4.16867621e-03\n",
      "  1.00876346e-01 -9.95653644e-02  9.01950002e-02 -1.57876033e-02\n",
      "  9.15840119e-02  2.02489458e-02  5.43938810e-03  1.84762217e-02\n",
      "  5.44336438e-02 -2.74265017e-02 -5.73022943e-03 -7.45430142e-02\n",
      "  1.51921576e-02  5.40425330e-02 -2.46409196e-02  6.97904378e-02\n",
      " -1.48619513e-03 -2.42820829e-02  1.40309408e-02 -1.10100903e-01\n",
      "  6.41229656e-03 -8.25487897e-02  7.65983993e-03 -5.47535941e-02\n",
      " -3.61692673e-03  8.60216692e-02  5.52234389e-02  5.38977273e-02\n",
      " -4.07413095e-02  6.83545992e-02  6.06583841e-02  2.63631623e-02\n",
      "  6.58041891e-03  8.26079622e-02 -6.08423129e-02 -2.19405964e-02\n",
      "  3.25788110e-02 -8.49711150e-02  5.06782010e-02 -3.61836925e-02\n",
      "  9.05922279e-02 -8.90507177e-02 -3.90780419e-02 -2.55856495e-02\n",
      "  9.16672647e-02  6.55198703e-03  8.27637166e-02 -4.00497484e-08\n",
      "  7.35990554e-02 -7.88962618e-02 -1.46350339e-02 -1.46809239e-02\n",
      "  1.18621709e-02 -4.57980260e-02 -1.89203899e-02  3.37524638e-02\n",
      " -1.59549117e-02 -1.75640695e-02 -8.95392150e-02  2.27739606e-02\n",
      " -8.61050468e-03 -7.06041977e-02 -1.19483117e-02 -1.08181179e-01\n",
      "  3.73652671e-03 -2.43435875e-02 -4.54409420e-02 -1.04198895e-01\n",
      "  2.63454709e-02 -1.00357663e-02  3.46608832e-02 -4.56274487e-02\n",
      " -4.05859016e-02  1.87619198e-02  6.49302006e-02  4.54522930e-02\n",
      " -2.03793403e-02 -1.22347400e-02 -7.22320378e-02  6.75510336e-03\n",
      "  5.67935966e-02 -5.71236163e-02  3.24255950e-03  1.90645698e-02\n",
      "  2.07569376e-01  5.98068163e-02 -1.68597400e-02  3.37917882e-04\n",
      " -1.66660286e-02 -5.97164519e-02  4.78548603e-03 -5.59718721e-02\n",
      " -1.79766528e-02  3.27668674e-02 -2.57440079e-02  1.02198347e-02\n",
      " -8.81614909e-03 -3.72549109e-02  4.90323044e-02 -6.34066835e-02\n",
      "  3.91777568e-02  2.21587587e-02 -2.62452755e-02 -6.48058802e-02\n",
      " -6.72891587e-02 -2.26455778e-02  8.30271188e-03 -3.34947333e-02\n",
      "  7.86647052e-02 -2.00298298e-02  4.42467034e-02 -3.19107696e-02]\n",
      "Shape of embeddings: (147007, 384)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Get embeddings from the sentence-transformer model\n",
    "embeddings = embedding_model.encode(data['description'].tolist(), show_progress_bar=True)\n",
    "\n",
    "\n",
    "print(\"Embedding of the first document:\", embeddings[0])\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95f7ff2f-5370-4bd2-a977-40e458ae8451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to final_embedding.pkl\n"
     ]
    }
   ],
   "source": [
    "# Specify the filename for the saved file\n",
    "filename = 'final_embedding.pkl'\n",
    "\n",
    "# Save the embeddings using pickle\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(embeddings, file)\n",
    "\n",
    "print(f\"Embeddings saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "772216e4-9d0a-46da-a97e-d65789f93565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(147007, 384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the filename for the file to be loaded\n",
    "filename = 'final_embedding.pkl'\n",
    "\n",
    "# Load the embeddings from the file\n",
    "with open(filename, 'rb') as file:\n",
    "    stored_embeddings = pickle.load(file)\n",
    "\n",
    "print(\"Embeddings loaded successfully.\")\n",
    "stored_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "50b9fd00-710a-4ef0-aa55-94d5554ae127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the starting date (YYYY-MM-DD):  2021-01-01\n",
      "Enter the ending date (YYYY-MM-DD):  2021-12-31\n",
      "Enter the initforward string:  1. DEABD - Department of Financial Services (Banking Division)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings):\n",
    "    # Convert the 'regdate' column to datetime\n",
    "    data['regdate'] = pd.to_datetime(data['regdate'])\n",
    "\n",
    "    # Filter the data based\n",
    "    filtered_data = data[(data['regdate'] >= pd.to_datetime(starting_date)) &\n",
    "                         (data['regdate'] <= pd.to_datetime(ending_date)) &\n",
    "                         (data['initforward'] == initforward_str)]\n",
    "\n",
    "    # Get indices of filtered data\n",
    "    indices = filtered_data.index.tolist()\n",
    "\n",
    "    # Ensure indices are within the range of stored_embeddings length\n",
    "    valid_indices = [index for index in indices if index < len(stored_embeddings)]\n",
    "\n",
    "    # Retrieve embeddings for the filtered data \n",
    "    obtained_embeddings = stored_embeddings[valid_indices]\n",
    "\n",
    "    return obtained_embeddings\n",
    "\n",
    "\n",
    "starting_date = input(\"Enter the starting date (YYYY-MM-DD): \")\n",
    "ending_date = input(\"Enter the ending date (YYYY-MM-DD): \")\n",
    "initforward_str = input(\"Enter the initforward string: \")\n",
    "\n",
    "\n",
    "data = df[df['initforward'].isna() == False]\n",
    "\n",
    "# Get the embeddings\n",
    "obtained_embeddings = get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97169f77-8405-42df-824f-1be9ecb74926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data['regdate'] = pd.to_datetime(data['regdate'])\n",
    "\n",
    "\n",
    "filtered_data = data[(data['regdate'] >= pd.to_datetime(starting_date)) &\n",
    "                     (data['regdate'] <= pd.to_datetime(ending_date)) &\n",
    "                     (data['initforward'] == initforward_str)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8cd77d-0c55-4d14-8a11-9c74bd34164c",
   "metadata": {},
   "source": [
    "# GMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0fced14a-22a4-4b1c-ad35-ba6ddf4bd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Number of topics\n",
    "n_components = 10\n",
    "\n",
    "# Apply GMM to the obtained embeddings\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "gmm.fit(obtained_embeddings)\n",
    "topic_probabilities = gmm.predict_proba(obtained_embeddings)\n",
    "\n",
    "# Assign the most likely topic to each document in the filtered dataset\n",
    "filtered_data['Topic'] = topic_probabilities.argmax(axis=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "# Print top words for each topic in the filtered data\n",
    "for i in range(n_components):\n",
    "    topic_documents = filtered_data[filtered_data['Topic'] == i]['description']\n",
    "    top_words = get_top_words(topic_documents, 10)\n",
    "    print(f\"Topic {i}: {', '.join([word for word, freq in top_words])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bf68b1d5-a7c7-4c1f-adca-ec563db894cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c07af5b4-035a-4b33-80fa-bc488e2a17d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0:\n",
      "Topic 0:  ['india', 'rs', '39', 'branch', 'sbi', '2020', 'account', 'sir', 'loan', 'bank']\n",
      "\n",
      "Cluster 1:\n",
      "Topic 0:  ['request', 'rs', '39', 'sbi', 'branch', '2020', 'account', 'sir', 'loan', 'bank']\n",
      "\n",
      "Cluster 2:\n",
      "Topic 0:  ['request', 'branch', 'rs', '2020', '39', 'account', '160', 'sir', 'loan', 'bank']\n",
      "\n",
      "Cluster 3:\n",
      "Topic 0:  ['39', 'branch', 'card', '160', 'account', '2020', 'sir', 'loan', 'rs', 'bank']\n",
      "\n",
      "Cluster 4:\n",
      "Topic 0:  ['2020', 'request', 'branch', 'india', '39', '160', 'account', 'loan', 'sir', 'bank']\n",
      "\n",
      "Cluster 5:\n",
      "Topic 0:  ['hai', 'india', 'branch', '2020', 'rs', '39', 'account', 'sir', 'loan', 'bank']\n",
      "\n",
      "Cluster 6:\n",
      "Topic 0:  ['money', '39', 'rs', 'branch', '2020', '160', 'account', 'loan', 'sir', 'bank']\n",
      "\n",
      "Cluster 7:\n",
      "Topic 0:  ['help', '2020', 'request', 'branch', 'rs', '39', 'account', 'sir', 'loan', 'bank']\n",
      "\n",
      "Cluster 8:\n",
      "Topic 0:  ['india', 'sbi', 'branch', '39', 'rs', 'account', '2020', 'sir', 'loan', 'bank']\n",
      "\n",
      "Cluster 9:\n",
      "Topic 0:  ['india', 'rs', 'sbi', '2020', '39', 'branch', 'account', 'sir', 'loan', 'bank']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Apply GMM to the obtained embeddings\n",
    "gmm = GaussianMixture(n_components=10, random_state=42)\n",
    "gmm.fit(obtained_embeddings)\n",
    "clusters = gmm.predict(obtained_embeddings)\n",
    "\n",
    "# Assign cluster labels to the filtered data\n",
    "filtered_data['Cluster'] = clusters\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "n_topics_per_cluster = 1  # Number of subtopics you want in each cluster\n",
    "\n",
    "for cluster in range(10):\n",
    "    # Filter the documents for each cluster\n",
    "    cluster_docs = filtered_data[filtered_data['Cluster'] == cluster]['description']\n",
    "\n",
    "    # Vectorize the documents\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
    "    doc_term_matrix = vectorizer.fit_transform(cluster_docs)\n",
    "\n",
    "    # Apply LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics_per_cluster, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "\n",
    "    # Display the top words in each subtopic\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic {idx}: \", [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0e072b77-ece4-43bd-8f8b-d0e12e1a1524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 Topics:\n",
      "Topic 0: bank, loan, sir, account, sbi, 39, 2020, branch, rs, help\n",
      "\n",
      "\n",
      "Cluster 1 Topics:\n",
      "Topic 0: bank, loan, sir, account, branch, 39, 2020, sbi, rs, home\n",
      "\n",
      "\n",
      "Cluster 2 Topics:\n",
      "Topic 0: bank, loan, sir, account, branch, 39, 2020, help, rs, sbi\n",
      "\n",
      "\n",
      "Cluster 3 Topics:\n",
      "Topic 0: bank, rs, loan, account, 2020, card, credit, branch, sbi, charges\n",
      "\n",
      "\n",
      "Cluster 4 Topics:\n",
      "Topic 0: bank, loan, sir, account, 39, request, pay, help, india, sbi\n",
      "\n",
      "\n",
      "Cluster 5 Topics:\n",
      "Topic 0: bank, loan, account, sir, 39, branch, rs, 2020, money, help\n",
      "\n",
      "\n",
      "Cluster 6 Topics:\n",
      "Topic 0: bank, loan, sir, account, branch, money, 39, rs, 2020, help\n",
      "\n",
      "\n",
      "Cluster 7 Topics:\n",
      "Topic 0: bank, loan, sir, account, 39, branch, rs, help, pay, request\n",
      "\n",
      "\n",
      "Cluster 8 Topics:\n",
      "Topic 0: bank, loan, sir, account, 39, rs, help, branch, 2020, sbi\n",
      "\n",
      "\n",
      "Cluster 9 Topics:\n",
      "Topic 0: bank, loan, sir, 39, account, branch, sbi, 2020, rs, help\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "num_topics_per_cluster = 1  # Number of topics in each cluster\n",
    "num_words_per_topic = 10     # Number of words per topic\n",
    "\n",
    "# Process each cluster\n",
    "for cluster_num in range(10):  # Assuming 10 clusters from GMM\n",
    "    # Filter documents in the current cluster\n",
    "    cluster_documents = filtered_data[filtered_data['Cluster'] == cluster_num]['description']\n",
    "\n",
    "    # Create a TF-IDF representation of the documents\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform(cluster_documents)\n",
    "\n",
    "    # Apply NMF\n",
    "    nmf = NMF(n_components=num_topics_per_cluster, random_state=42)\n",
    "    nmf.fit(tfidf)\n",
    "\n",
    "    # Display the top words for each topic\n",
    "    print(f\"Cluster {cluster_num} Topics:\")\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        top_features_idx = topic.argsort()[-num_words_per_topic:]\n",
    "        top_features = [vectorizer.get_feature_names_out()[i] for i in top_features_idx]\n",
    "        top_features.reverse()  # To get the top words in descending order of weight\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_features)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "83b37c23-73ba-41ab-a0f7-1dde4e7a519a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in Topic 0:\n",
      "772     MR. MODI, PM INDIA,THE GRIEVANCE PMOPG/E/2020/...\n",
      "1062    Dear TeamGreetings for the day.... --- Clarifi...\n",
      "1065    Sir please i request you to take strict action...\n",
      "1144    Sir, I A Nirmala am requesting you that I have...\n",
      "1413    RESPECTED SIR I AM FARMAR AT VILLAGE TIWARIPUR...\n",
      "Name: description, dtype: object\n",
      "Documents in Topic 1:\n",
      "101    Extension of cut of date from 1st April 2020 t...\n",
      "194    Dear sir I closed my NSS account. after 20% de...\n",
      "354    Pension scheme introduced in 1993 but around 5...\n",
      "365    Respected Sir,I am approaching your office for...\n",
      "495    INSURANCE COMPANY WAS NOT FOLLOW THE IRDA RULE...\n",
      "Name: description, dtype: object\n",
      "Documents in Topic 2:\n",
      "13     Ref.No.2020-21/L/12/0306 TO PMORef :- a) PMO C...\n",
      "105    I have taken home loan from Central Bank of In...\n",
      "121    The Axis Bank Ganesh Chandra Avenue Branch is ...\n",
      "188    Hon&#39;ble Prime Minister ModijiWrites to kno...\n",
      "200    Sir, My earlier letter Ref: Your Registration ...\n",
      "Name: description, dtype: object\n",
      "Documents in Topic 3:\n",
      "100    Dear Sir,we are facing a common problem from G...\n",
      "162    have withdrawal Money from AEPS service due to...\n",
      "167    Respected Sir/Madam, I would have brought your...\n",
      "529    Sir,  submission of grievances from  last five...\n",
      "640    ToThe Hon&#39;ble prime minister of IndiaNew D...\n",
      "Name: description, dtype: object\n",
      "Documents in Topic 4:\n",
      "22     Respected PM SirI am Arundev. I took a persona...\n",
      "183    My request for restructuring as per RBI scheme...\n",
      "328    Sir, 1 month ago I applied for a loan (mudra) ...\n",
      "710    To,          Honorable Prime Minister,        ...\n",
      "794    This is to inform you that my pension account ...\n",
      "Name: description, dtype: object\n",
      "Documents in Topic 5:\n",
      "2      MY FATHER H S RAMANNA RETIRED ON 30/04/1990 WA...\n",
      "104    Anyhow Running a proprietorship Company ASIT S...\n",
      "158    Sub: Business Loan Claim RegardsRef: Bank pass...\n",
      "347    Maney.prime minister ji namskar m gRib mAjdur ...\n",
      "424    Very Dear and people&#39;s Prime Minister:We a...\n",
      "Name: description, dtype: object\n",
      "Documents in Topic 6:\n",
      "143    Sir I am V K Ramachandran from Kerala a centra...\n",
      "172    *From* : BURRI VINOD *To*Mr. Rishi Kumar Shukl...\n",
      "450    Hamara bijanesh band ho gaya hai hamara lon ka...\n",
      "552    Hi sir,I, Murugeswari D/O Ramar brought edu lo...\n",
      "581    Dear sirWe are still awaiting for your valuabl...\n",
      "Name: description, dtype: object\n",
      "Documents in Topic 7:\n",
      "120    Reminder Letter-2Hon’ble, Dear Madam-Sir,     ...\n",
      "462    I am Krutika Prajapati from kim. My father fac...\n",
      "478    To,Mr. Honorable Prime Minister,Government of ...\n",
      "653    Respected prime ministerI am writing to you wi...\n",
      "707    INSURANCE COMPANY HAS MIS USE OF IRDA RULE OF ...\n",
      "Name: description, dtype: object\n",
      "Documents in Topic 8:\n",
      "54     Respected Prime Minister of India Sree Narendr...\n",
      "57     I am 2019 batch PO in Union Bank Of India post...\n",
      "173    Sir,Mera naam babban ojha h mai noida sector 4...\n",
      "339    Dear Sir, Pradhan Mantri MUDRA Yojana (PMMY) i...\n",
      "436    ToHonourable Prime MinisterGovernment of India...\n",
      "Name: description, dtype: object\n",
      "Documents in Topic 9:\n",
      "592     Dear Sir My father availed a mortgage loan of ...\n",
      "593     Dear Sir My father availed a mortgage loan of ...\n",
      "599     Subject: CREDIT CARD NO. 5459 6489 0669 3446 -...\n",
      "1111    AXIS BANKCredit Card 5593420002826925Request y...\n",
      "1174    sub-Disappointment for covid 19 .dear respecte...\n",
      "Name: description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "\n",
    "# Apply KMeans \n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(obtained_embeddings)\n",
    "\n",
    "# Assign the cluster labels to data\n",
    "filtered_data['Topic'] = clusters\n",
    "for i in range(num_clusters):\n",
    "    print(f\"Documents in Topic {i}:\")\n",
    "    # Retrieve documents in the cluster\n",
    "    cluster_docs = filtered_data[filtered_data['Topic'] == i]['description']\n",
    "    \n",
    "   \n",
    "    print(cluster_docs.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0cec467f-6c02-4cf1-aeb1-0924d96b3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in Topic 0:\n",
      "bank, loan, sir, account, sbi, rs, branch, 39, 2020, request\n",
      "Top words in Topic 1:\n",
      "bank, loan, sir, account, branch, sbi, 2020, 39, 160, rs\n",
      "Top words in Topic 2:\n",
      "bank, loan, sir, account, 2020, rs, branch, 39, india, help\n",
      "Top words in Topic 3:\n",
      "bank, loan, sir, account, 2020, sbi, branch, 39, rs, india\n",
      "Top words in Topic 4:\n",
      "bank, loan, sir, account, rs, 39, branch, request, 2020, help\n",
      "Top words in Topic 5:\n",
      "bank, loan, sir, account, 39, rs, 2020, branch, hai, india\n",
      "Top words in Topic 6:\n",
      "bank, loan, sir, account, 39, 2020, sbi, branch, rs, india\n",
      "Top words in Topic 7:\n",
      "bank, loan, sir, account, 2020, 39, branch, rs, india, request\n",
      "Top words in Topic 8:\n",
      "bank, loan, sir, account, 2020, 39, 160, branch, rs, help\n",
      "Top words in Topic 9:\n",
      "bank, rs, loan, sir, 2020, account, 160, card, branch, 39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "# Print top words for each topic\n",
    "for i in range(num_clusters):\n",
    "    print(f\"Top words in Topic {i}:\")\n",
    "    cluster_docs = filtered_data[filtered_data['Topic'] == i]['description']\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    print(\", \".join([word for word, freq in top_words]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8d223643-70d7-46ab-921b-b3815f31d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in Topic 0:\n",
      "bank, loan, sir, account, sbi, rs, branch, 39, 2020, request\n",
      "\n",
      "\n",
      "Top words in Topic 1:\n",
      "bank, loan, sir, account, branch, sbi, 2020, 39, 160, rs\n",
      "\n",
      "\n",
      "Top words in Topic 2:\n",
      "bank, loan, sir, account, 2020, rs, branch, 39, india, help\n",
      "\n",
      "\n",
      "Top words in Topic 3:\n",
      "bank, loan, sir, account, 2020, sbi, branch, 39, rs, india\n",
      "\n",
      "\n",
      "Top words in Topic 4:\n",
      "bank, loan, sir, account, rs, 39, branch, request, 2020, help\n",
      "\n",
      "\n",
      "Top words in Topic 5:\n",
      "bank, loan, sir, account, 39, rs, 2020, branch, hai, india\n",
      "\n",
      "\n",
      "Top words in Topic 6:\n",
      "bank, loan, sir, account, 39, 2020, sbi, branch, rs, india\n",
      "\n",
      "\n",
      "Top words in Topic 7:\n",
      "bank, loan, sir, account, 2020, 39, branch, rs, india, request\n",
      "\n",
      "\n",
      "Top words in Topic 8:\n",
      "bank, loan, sir, account, 2020, 39, 160, branch, rs, help\n",
      "\n",
      "\n",
      "Top words in Topic 9:\n",
      "bank, rs, loan, sir, 2020, account, 160, card, branch, 39\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 10 \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    \n",
    "    #print(f\"Documents in Topic {i}:\")\n",
    "    cluster_docs = filtered_data[filtered_data['Topic'] == i]['description']\n",
    "    \n",
    "    #print(cluster_docs.head())  # Display some documents for each topic\n",
    "\n",
    "    print(f\"Top words in Topic {i}:\")\n",
    "    top_words = get_top_words(cluster_docs, 10)  # Get top 10 words\n",
    "    print(\", \".join([word for word, freq in top_words]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67a6fc-9799-4810-bf1b-66905e24a1f2",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a5eaa7aa-0e81-4035-a1d8-49986fcd3576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 39, sir, pay, help, emi, business, situation, banks, covid, people\n",
      "Topic 1: enclosures, scanned, shortly, initiated, fully, receipt, pg, copy, hard, sent\n",
      "Topic 2: hai, se, ki, ke, ka, nahi, mai, mera, bhi, ko\n",
      "Topic 3: salaries, salary, settlement, iba, bank, arrears, bipartite, paid, unions, board\n",
      "Topic 4: bank, account, branch, complaint, rs, number, 2021, money, india, transaction\n",
      "Topic 5: subsidy, pmay, home, application, loan, id, housing, scheme, received, clss\n",
      "Topic 6: 160, dhfl, details, permanent, payment, madam, like, housing, process, quot\n",
      "Topic 7: card, credit, rs, charges, sbi, payment, hdfc, outstanding, debit, pay\n",
      "Topic 8: pension, sbi, branch, cppc, 2020, 2021, ppo, quot, da, office\n",
      "Topic 9: loan, education, bank, mudra, home, manager, sir, branch, applied, help\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "filtered_data = data[data['initforward'] == '1. DEABD - Department of Financial Services (Banking Division)']\n",
    "\n",
    "# Convert descriptions to a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(filtered_data['description'])\n",
    "\n",
    "# Apply NMF\n",
    "num_topics = 10  # Number of topics\n",
    "nmf = NMF(n_components=num_topics, random_state=0)\n",
    "nmf.fit(tfidf)\n",
    "\n",
    "# Display the topics\n",
    "num_top_words = 10\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Topic {topic_idx}: \", end='')\n",
    "    top_features_idx = topic.argsort()[-num_top_words:][::-1]\n",
    "    top_features = [vectorizer.get_feature_names_out()[i] for i in top_features_idx]\n",
    "    print(\", \".join(top_features))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1914b-754f-42bc-87ec-bdd68349f4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fe5cb6cb-0353-4ec8-8f4f-de57c9afa1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in Cluster 0: sbi, dhfl, life, fd, holders, bank, ibc, sir, resolution, process\n",
      "\n",
      "\n",
      "Top words in Cluster 1: bank, 160, loan, rs, 2020, 39, sir, branch, passbook, charges\n",
      "\n",
      "\n",
      "Top words in Cluster 2: senior, citizens, saab, 2020, credit, card, mera, pmopg, bank, quot\n",
      "\n",
      "\n",
      "Top words in Cluster 3: bank, rs, loan, branch, 2020, account, charges, baroda, sbi, complain\n",
      "\n",
      "\n",
      "Top words in Cluster 4: card, credit, sir, bank, 39, kindly, loan, active, charges, apply\n",
      "\n",
      "\n",
      "Top words in Cluster 5: sir, bank, customer, risk, customers, mai, rbi, banks, payment, help\n",
      "\n",
      "\n",
      "Top words in Cluster 6: bank, account, branch, credit, loan, request, 39, rs, customers, pay\n",
      "\n",
      "\n",
      "Top words in Cluster 7: rs, fd, dhfl, transaction, bank, public, holders, money, account, charges\n",
      "\n",
      "\n",
      "Top words in Cluster 8: loan, sir, bank, hai, taken, business, help, hdfc, want, time\n",
      "\n",
      "\n",
      "Top words in Cluster 9: loan, bank, 39, education, sir, sbi, account, manager, da, banks\n",
      "\n",
      "\n",
      "Top words in Cluster 10: bank, loan, account, 39, subsidy, online, company, money, matter, job\n",
      "\n",
      "\n",
      "Top words in Cluster 11: bank, loan, kapoor, property, husband, rent, help, business, 39, punjab\n",
      "\n",
      "\n",
      "Top words in Cluster 12: 160, bank, sir, loan, money, 39, business, home, time, 2020\n",
      "\n",
      "\n",
      "Top words in Cluster 13: bank, loan, sir, account, request, transfer, india, mr, education, 2020\n",
      "\n",
      "\n",
      "Top words in Cluster 14: bank, sir, loan, hai, account, request, branch, se, sbi, ki\n",
      "\n",
      "\n",
      "Top words in Cluster 15: bank, loan, account, rs, india, sir, paytm, request, money, branch\n",
      "\n",
      "\n",
      "Top words in Cluster 16: bank, rs, account, 39, 2020, loan, sir, sbi, charges, branch\n",
      "\n",
      "\n",
      "Top words in Cluster 17: bank, loan, 2020, sir, india, 39, manager, rs, coins, branch\n",
      "\n",
      "\n",
      "Top words in Cluster 18: rs, bank, 2020, branch, charges, loan, 39, account, 19, manager\n",
      "\n",
      "\n",
      "Top words in Cluster 19: bank, loan, sir, account, 2020, 39, branch, sbi, rs, india\n",
      "\n",
      "\n",
      "Top words in Cluster 20: bank, sir, account, loan, hai, sbi, rbi, 39, india, ke\n",
      "\n",
      "\n",
      "Top words in Cluster 21: 160, bank, loan, india, branch, sir, account, 39, request, minor\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "\n",
    "# Apply HDBSCAN to the obtained embeddings\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True)\n",
    "clusters = clusterer.fit_predict(obtained_embeddings)\n",
    "\n",
    "# Filter the original data to include only those rows that were clustered\n",
    "clustered_indices = [i for i, label in enumerate(clusters) if label != -1]\n",
    "filtered_clustered_data = filtered_data.iloc[clustered_indices]\n",
    "\n",
    "# Assign cluster labels to the filtered clustered data\n",
    "filtered_clustered_data['Cluster'] = clusters[clusters != -1]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "for cluster_num in set(clusters) - {-1}:  # Exclude noise (-1)\n",
    "    # print(f\"Documents in Cluster {cluster_num}:\")\n",
    "    cluster_docs = filtered_clustered_data[filtered_clustered_data['Cluster'] == cluster_num]['description']\n",
    "    # print(cluster_docs.head())  # Display a few documents from the cluster\n",
    "    \n",
    "    # Get and print the top words for each cluster\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    print(f\"Top words in Cluster {cluster_num}: {', '.join([word for word, freq in top_words])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fcc368ab-352c-47f2-993e-c43e167c0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe929aab-0156-49d7-a559-c60660c2a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import HdpModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in stopwords.words('english'):\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'description' is the column with text data\n",
    "processed_docs = data['description'].map(preprocess)\n",
    "\n",
    "# Create a dictionary and corpus needed for topic modeling\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Apply HDP model\n",
    "hdp_model = HdpModel(corpus, dictionary)\n",
    "\n",
    "# Display the topics\n",
    "for i, topic in enumerate(hdp_model.print_topics(num_topics=10, num_words=10)):\n",
    "    print(f\"Topic {i}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88dcba-429e-468c-9e20-4b58c28bb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic_info in enumerate(hdp_model.print_topics(num_topics=10, num_words=10)):\n",
    "    # The topic_info is a tuple of topic number and the weighted words\n",
    "    # We split the string to extract only words\n",
    "    words = topic_info[1].split(\"+\")\n",
    "    \n",
    "    # Extract and clean each word\n",
    "    topic_words = [word.split(\"*\")[1].strip() for word in words]\n",
    "    \n",
    "    # Combine the words back into a single string\n",
    "    print(f\"Topic {i}: {', '.join(topic_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a5efdcd4-c5af-478b-bae0-436b9855a276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the starting date (YYYY-MM-DD):  2021-01-01\n",
      "Enter the ending date (YYYY-MM-DD):  2021-12-31\n",
      "Enter the initforward string:  1. DEABD - Department of Financial Services (Banking Division)\n"
     ]
    }
   ],
   "source": [
    "# topic_model = BERTopic()\n",
    "# docs = filtered_data['description'].tolist()\n",
    "# topics, probs = topic_model.fit_transform(docs, obtained_embeddings)\n",
    "# len(filtered_data['description'].tolist())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings):\n",
    "    # Convert the 'regdate' column to datetime\n",
    "    data['regdate'] = pd.to_datetime(data['regdate'])\n",
    "\n",
    "    # Filter the data based on the conditions\n",
    "    filtered_data = data[(data['regdate'] >= pd.to_datetime(starting_date)) &\n",
    "                         (data['regdate'] <= pd.to_datetime(ending_date)) &\n",
    "                         (data['initforward'] == initforward_str)]\n",
    "\n",
    "    # Get indices of filtered data\n",
    "    indices = filtered_data.index.tolist()\n",
    "\n",
    "    # Ensure indices are within the range of stored_embeddings length\n",
    "    valid_indices = [index for index in indices if index < len(stored_embeddings)]\n",
    "\n",
    "    # Retrieve embeddings for the filtered data\n",
    "    obtained_embeddings = np.array(stored_embeddings)[valid_indices]\n",
    "\n",
    "    # Retrieve descriptions for the filtered data\n",
    "    descriptions = filtered_data.loc[valid_indices, 'description'].tolist()\n",
    "\n",
    "    return obtained_embeddings, descriptions\n",
    "\n",
    "# Inputs from user\n",
    "starting_date = input(\"Enter the starting date (YYYY-MM-DD): \")\n",
    "ending_date = input(\"Enter the ending date (YYYY-MM-DD): \")\n",
    "initforward_str = input(\"Enter the initforward string: \")\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'embeddings' is the list/array of stored embeddings\n",
    "data = df[df['initforward'].isna() == False]  # Filter data where 'initforward' is not NaN\n",
    "\n",
    "# Get the embeddings and descriptions\n",
    "obtained_embeddings, descriptions = get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, embeddings)\n",
    "\n",
    "# Now `obtained_embeddings` will contain the embeddings and `descriptions` will contain the corresponding descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f45e1-5586-41ab-9678-c54136a6b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(descriptions)\n",
    "topic_model = BERTopic()\n",
    "docs = descriptions\n",
    "topics, probs = topic_model.fit_transform(docs, obtained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "82004444-8783-47c6-894e-1286e46718fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic  Count                                   Name  \\\n",
      "0       -1   2555                       -1_to_the_of_and   \n",
      "1        0    983                        0_the_and_to_of   \n",
      "2        1    293                        1_the_of_to_and   \n",
      "3        2    277                         2_the_to_is_of   \n",
      "4        3    258                         3_to_of_and_in   \n",
      "..     ...    ...                                    ...   \n",
      "103    102     11   102_rs_anandnagar_complain_ahmedabad   \n",
      "104    103     11        103_autodebit_ssc_payment_pmsby   \n",
      "105    104     11    104_bachchon_excess_remarks_manager   \n",
      "106    105     10           105_nai_muje_hai_outstanding   \n",
      "107    106     10  106_psb39s_bavjut_scholarship_barauni   \n",
      "\n",
      "                                        Representation  \\\n",
      "0       [to, the, of, and, my, in, bank, is, for, not]   \n",
      "1      [the, and, to, of, bank, my, is, for, in, this]   \n",
      "2      [the, of, to, and, for, is, my, in, have, bank]   \n",
      "3        [the, to, is, of, and, my, in, for, me, have]   \n",
      "4     [to, of, and, in, the, bank, my, have, loan, is]   \n",
      "..                                                 ...   \n",
      "103  [rs, anandnagar, complain, ahmedabad, remittan...   \n",
      "104  [autodebit, ssc, payment, pmsby, alkapuri, rat...   \n",
      "105  [bachchon, excess, remarks, manager, register,...   \n",
      "106  [nai, muje, hai, outstanding, hu, vapas, bedau...   \n",
      "107  [psb39s, bavjut, scholarship, barauni, honorab...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [Dear Sir/Madam,I write to you requesting your...  \n",
      "1    [Dear Sir/Madam,I write to you requesting your...  \n",
      "2    [Dear Sir/Madam,I write to you requesting your...  \n",
      "3    [Most Respected Sir,                     I am ...  \n",
      "4    [Dear Sir/Madam,I write to you requesting your...  \n",
      "..                                                 ...  \n",
      "103  [Dear Sir My father availed a mortgage loan of...  \n",
      "104  [I retired from Indian Bank , erstwhile Allaha...  \n",
      "105  [SUBJECT: excess  interest  charged  on my  lo...  \n",
      "106  [AGONY OF CASHEW INDUSTRY WORKERS AND ENTREPRE...  \n",
      "107  [Honorable Prime Minister,I, Komal Kumari resi...  \n",
      "\n",
      "[108 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# len(filtered_data['description'].tolist())\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Display the topics\n",
    "print(topic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "97dede26-c43c-4a74-8337-6ad81c7a153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  the, and, to, bank, of, my, is, in, for, this\n",
      "Topic 1:  the, to, of, is, and, my, in, for, me, bank\n",
      "Topic 2:  of, the, to, and, for, is, in, bank, my, have\n",
      "Topic 3:  the, my, to, and, bank, of, not, is, me, have\n",
      "Topic 4:  the, to, of, and, for, that, in, my, he, is\n",
      "Topic 5:  and, the, in, to, of, we, for, is, my, will\n",
      "Topic 6:  160, and, the, to, we, of, in, are, my, for\n",
      "Topic 7:  of, to, and, the, my, in, for, loan, this, bank\n",
      "Topic 8:  bank, of, the, and, in, for, my, to, is, this\n",
      "Topic 9:  the, to, and, you, my, of, that, bank, in, is\n"
     ]
    }
   ],
   "source": [
    "#bu\n",
    "\n",
    "\n",
    "\n",
    "topic_model = BERTopic()\n",
    "docs = descriptions\n",
    "# Fit the BERTopic model on the dummy documents and obtained embeddings\n",
    "topics, _ = topic_model.fit_transform(docs, obtained_embeddings)\n",
    "\n",
    "# Extract the topics and their frequencies\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Get the top 10 topics\n",
    "top_topics = topic_info.head(11)  # Get 11 as one topic might be -1 (outliers)\n",
    "\n",
    "# Print the top 10 topics and their top 10 words\n",
    "for topic in top_topics['Topic']:\n",
    "    if topic != -1:  # excluding the outlier cluster\n",
    "        # Get top 10 words for the topic\n",
    "        topic_words = topic_model.get_topic(topic)[:10]\n",
    "        # Extract only the words (not the scores)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        # Print the topic and its words\n",
    "        print(f\"Topic {topic}: \", \", \".join(words))\n",
    "        \n",
    "\n",
    "#1. DORVU - Department of Revenue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0101b184-5e06-44c2-bced-a49978939f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  hai, se, ki, ke, ka, ko, nahi, mai, mera, bhi\n",
      "Topic 1:  subsidy, pmay, home, loan, housing, application, for, scheme, id, nhb\n",
      "Topic 2:  account, branch, bank, of, the, is, to, open, not, address\n",
      "Topic 3:  atm, transaction, amount, account, money, but, my, no, bank, debited\n",
      "Topic 4:  mudra, business, loan, start, for, to, am, project, sir, my\n",
      "Topic 5:  banks, privatisation, sector, public, are, private, banking, of, privatization, will\n",
      "Topic 6:  pension, cppc, ppo, of, delhi, no, sbi, from, the, family\n",
      "Topic 7:  loan, my, pay, family, am, help, me, job, to, amount\n",
      "Topic 8:  pmegp, loan, project, under, the, for, application, manager, applied, unit\n",
      "Topic 9:  education, 160, college, loan, my, university, fees, father, for, studies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_model = BERTopic()\n",
    "docs = descriptions\n",
    "\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "# Extract the topics and their frequencies\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Get the top 10 topics\n",
    "top_topics = topic_info.head(11)  # Get 11 as one topic might be -1 (outliers)\n",
    "\n",
    "# Print the top 10 topics and their top 10 words\n",
    "for topic in top_topics['Topic']:\n",
    "    if topic != -1:  # excluding the outlier cluster\n",
    "        # Get top 10 words for the topic\n",
    "        topic_words = topic_model.get_topic(topic)[:10]\n",
    "        # Extract only the words (not the scores)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        # Print the topic and its words\n",
    "        print(f\"Topic {topic}: \", \", \".join(words))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438cb07-ca32-4515-97eb-00db1654dbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "57b81a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the starting date (YYYY-MM-DD):  2021-01-01\n",
      "Enter the ending date (YYYY-MM-DD):  2021-12-31\n",
      "Enter the initforward string:  1. DORVU - Department of Revenue\n"
     ]
    }
   ],
   "source": [
    "#1. DORVU - Department of Revenue\n",
    "# topic_model = BERTopic()\n",
    "# docs = filtered_data['description'].tolist()\n",
    "# topics, probs = topic_model.fit_transform(docs, obtained_embeddings)\n",
    "# len(filtered_data['description'].tolist())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings):\n",
    "    # Convert the 'regdate' column to datetime\n",
    "    data['regdate'] = pd.to_datetime(data['regdate'])\n",
    "\n",
    "    # Filter the data based on the conditions\n",
    "    filtered_data = data[(data['regdate'] >= pd.to_datetime(starting_date)) &\n",
    "                         (data['regdate'] <= pd.to_datetime(ending_date)) &\n",
    "                         (data['initforward'] == initforward_str)]\n",
    "\n",
    "    # Get indices of filtered data\n",
    "    indices = filtered_data.index.tolist()\n",
    "\n",
    "    # Ensure indices are within the range of stored_embeddings length\n",
    "    valid_indices = [index for index in indices if index < len(stored_embeddings)]\n",
    "\n",
    "    # Retrieve embeddings for the filtered data\n",
    "    obtained_embeddings = np.array(stored_embeddings)[valid_indices]\n",
    "\n",
    "    # Retrieve descriptions for the filtered data\n",
    "    descriptions = filtered_data.loc[valid_indices, 'description'].tolist()\n",
    "\n",
    "    return obtained_embeddings, descriptions\n",
    "\n",
    "# Inputs from user\n",
    "starting_date = input(\"Enter the starting date (YYYY-MM-DD): \")\n",
    "ending_date = input(\"Enter the ending date (YYYY-MM-DD): \")\n",
    "initforward_str = input(\"Enter the initforward string: \")\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'embeddings' is the list/array of stored embeddings\n",
    "data = df[df['initforward'].isna() == False]  # Filter data where 'initforward' is not NaN\n",
    "\n",
    "# Get the embeddings and descriptions\n",
    "obtained_embeddings, descriptions = get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, embeddings)\n",
    "\n",
    "# Now `obtained_embeddings` will contain the embeddings and `descriptions` will contain the corresponding descriptions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7b6591e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  the, of, and, to, in, is, refund, tax, income, my\n",
      "Topic 1:  gst, to, we, the, and, my, is, for, our, not\n",
      "Topic 2:  pan, card, my, to, the, name, application, is, in, it\n",
      "Topic 3:  due, to, and, the, for, of, are, we, in, date\n",
      "Topic 4:  and, customs, to, the, is, have, in, we, india, for\n",
      "Topic 5:  tax, to, the, of, and, for, in, are, is, will\n",
      "Topic 6:  my, the, to, pension, of, and, that, been, on, has\n",
      "Topic 7:  hai, ki, se, ke, ka, ko, nahi, mera, bhi, hu\n",
      "Topic 8:  customs, commissioner, of, suspension, tribunal, dated, principal, honble, had, central\n",
      "Topic 9:  scanned, shortly, enclosures, initiated, fully, upon, receipt, hard, along, copy\n",
      "Topic 10:  gst, the, to, of, is, tax, are, and, or, in\n",
      "Topic 11:  gst, be, will, and, to, of, the, in, if, steel\n",
      "Topic 12:  adhar, date, entire, site, extended, for, daythe, linking, same, reason\n",
      "Topic 13:  tds, society, 201516, deposited, my, sahara, deducted, vide, 201819, cooperative\n",
      "Topic 14:  fraud, gst, and, my, the, to, no, this, pan, of\n",
      "Topic 15:  scanned, shortly, enclosures, initiated, fully, upon, receipt, hard, along, copy\n",
      "Topic 16:  osho, international, trustees, disciples, samadhi, of, pune, ashram, oshos, commune\n",
      "Topic 17:  site, upload, mine, resident, trying, dadri, resolve, nagar, theincome, siri\n",
      "Topic 18:  scanned, enclosures, pgrjpetition, initiated, shortly, fully, upon, receipt, hard, along\n",
      "Topic 19:  2013, pray, to, minister, smstl, forfeited, tax, have, heed, spend\n",
      "Topic 20:  gst, to, customers, in, the, is, of, not, input, by\n",
      "Topic 21:  itat, faceless, of, the, in, proceedings, tax, is, be, tribunal\n",
      "Topic 22:  defence, pension, pensioners, of, tax, income, employer, the, bank, section\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_model = BERTopic()\n",
    "docs = descriptions\n",
    "\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "# Extract the topics and their frequencies\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Get the top 10 topics\n",
    "top_topics = topic_info.head(26)  \n",
    "\n",
    "\n",
    "for topic in top_topics['Topic']:\n",
    "    if topic != -1:  # excluding the outlier cluster\n",
    "        # Get top 10 words for the topic\n",
    "        topic_words = topic_model.get_topic(topic)[:10]\n",
    "        # Extract only the words (not the scores)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        # Print the topic and its words\n",
    "        print(f\"Topic {topic}: \", \", \".join(words))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fc8a69b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  the, of, to, and, in, for, is, tax, on, my\n",
      "Topic 1:  the, of, to, and, is, for, in, my, tax, not\n",
      "Topic 2:  to, the, and, of, is, in, for, my, we, are\n",
      "Topic 3:  the, to, and, of, my, in, is, as, not, on\n",
      "Topic 4:  the, to, and, of, in, is, for, my, this, have\n",
      "Topic 5:  the, to, of, and, for, in, is, on, my, have\n",
      "Topic 6:  of, to, the, and, my, in, is, for, on, as\n",
      "Topic 7:  the, for, to, of, and, is, we, in, that, gst\n",
      "Topic 8:  to, the, and, of, is, for, in, on, are, my\n",
      "Topic 9:  to, the, for, not, and, of, is, we, my, in\n",
      "Topic 10:  the, to, not, and, in, my, of, is, for, tax\n",
      "Topic 11:  of, the, osho, to, by, is, in, and, that, for\n",
      "Topic 12:  of, and, to, the, in, is, as, not, by, for\n",
      "Topic 13:  the, and, of, to, in, tax, this, income, is, for\n",
      "Topic 14:  to, and, the, my, of, is, on, in, tax, for\n",
      "Topic 15:  of, in, the, to, his, and, my, family, have, tax\n",
      "Topic 16:  the, is, to, of, and, refund, on, it, insurance, in\n",
      "Topic 17:  the, my, gramalaya, in, for, is, to, of, and, sanitation\n",
      "Topic 18:  and, to, my, the, in, you, from, have, income, is\n",
      "Topic 19:  of, roadlines, and, all, to, agrawal, india, pvt, the, commissioner\n",
      "Topic 20:  and, to, the, of, ed, is, on, gst, in, companies\n",
      "Topic 21:  the, gst, tax, by, deependra, in, is, ca, and, we\n",
      "Topic 22:  we, to, of, hours, are, is, my, and, but, have\n",
      "Topic 23:  of, the, in, kothur, and, to, pe, ur, gst, payment\n",
      "Topic 24:  of, the, to, gpf, and, is, by, on, zao, not\n"
     ]
    }
   ],
   "source": [
    "#1. DORVU - Department of Revenue\n",
    "# topic_model = BERTopic()\n",
    "# docs = filtered_data['description'].tolist()\n",
    "# topics, probs = topic_model.fit_transform(docs, obtained_embeddings)\n",
    "# len(filtered_data['description'].tolist())\n",
    "topic_model = BERTopic()\n",
    "docs = descriptions\n",
    "# Fit the BERTopic model on the dummy documents and obtained embeddings\n",
    "topics, _ = topic_model.fit_transform(docs, obtained_embeddings)\n",
    "\n",
    "# Extract the topics and their frequencies\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Get the top 10 topics\n",
    "top_topics = topic_info.head(26)  # Get 11 as one topic might be -1 (outliers)\n",
    "\n",
    "# Print the top 10 topics and their top 10 words\n",
    "for topic in top_topics['Topic']:\n",
    "    if topic != -1:  # excluding the outlier cluster\n",
    "        # Get top 10 words for the topic\n",
    "        topic_words = topic_model.get_topic(topic)[:10]\n",
    "        # Extract only the words (not the scores)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        # Print the topic and its words\n",
    "        print(f\"Topic {topic}: \", \", \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f850c321-155d-4a47-9600-564be443b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the starting date (YYYY-MM-DD):  2021-01-01\n",
      "Enter the ending date (YYYY-MM-DD):  2021-12-31\n",
      "Enter the initforward string:  1. DORVU - Department of Revenue\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings):\n",
    "    # Convert the 'regdate' column to datetime\n",
    "    data['regdate'] = pd.to_datetime(data['regdate'])\n",
    "\n",
    "    # Filter the data based on the conditions\n",
    "    filtered_data = data[(data['regdate'] >= pd.to_datetime(starting_date)) &\n",
    "                         (data['regdate'] <= pd.to_datetime(ending_date)) &\n",
    "                         (data['initforward'] == initforward_str)]\n",
    "\n",
    "    # Get indices of filtered data\n",
    "    indices = filtered_data.index.tolist()\n",
    "\n",
    "    # Ensure indices are within the range of stored_embeddings length\n",
    "    valid_indices = [index for index in indices if index < len(stored_embeddings)]\n",
    "\n",
    "    # Retrieve embeddings for the filtered data\n",
    "    obtained_embeddings = np.array(stored_embeddings)[valid_indices]\n",
    "\n",
    "    # Retrieve descriptions for the filtered data\n",
    "    descriptions = filtered_data.loc[valid_indices, 'description'].tolist()\n",
    "    filtered_data = filtered_data.loc[valid_indices]\n",
    "\n",
    "    return obtained_embeddings, descriptions, filtered_data\n",
    "\n",
    "# Inputs from user\n",
    "starting_date = input(\"Enter the starting date (YYYY-MM-DD): \")\n",
    "ending_date = input(\"Enter the ending date (YYYY-MM-DD): \")\n",
    "initforward_str = input(\"Enter the initforward string: \")\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'embeddings' is the list/array of stored embeddings\n",
    "data = df[df['initforward'].isna() == False]  # Filter data where 'initforward' is not NaN\n",
    "\n",
    "# Get the embeddings and descriptions\n",
    "obtained_embeddings, descriptions, filtered_data = get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "173e5d9b-4630-42d1-94c9-48002c6bc493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in Cluster 0: sir, gst, gramalaya, refund, government, india, 39, rs, department, sanitation\n",
      "\n",
      "\n",
      "Top words in Cluster 1: tax, office, order, income, kodaikannal, faa, cpio, assistant, commissioner, information\n",
      "\n",
      "\n",
      "Top words in Cluster 2: tax, 39, pay, gst, currency, husband, pf, notice, rs, flat\n",
      "\n",
      "\n",
      "Top words in Cluster 3: officers, india, tax, sir, review, dpc, huge, income, gst, 39\n",
      "\n",
      "\n",
      "Top words in Cluster 4: tax, gst, business, kothur, account, money, number, refund, aadhar, district\n",
      "\n",
      "\n",
      "Top words in Cluster 5: tax, 03, delhi, income, new, aforesaid, number, gst, department, date\n",
      "\n",
      "\n",
      "Top words in Cluster 6: tax, income, years, rs, 2019, 2020, refund, dated, speed, year\n",
      "\n",
      "\n",
      "Top words in Cluster 7: refund, tax, grievance, insurance, income, gst, quot, department, year, government\n",
      "\n",
      "\n",
      "Top words in Cluster 8: tax, card, gst, sir, time, rs, pan, request, 39, income\n",
      "\n",
      "\n",
      "Top words in Cluster 9: tax, income, rs, sir, 2020, gst, refund, 2021, india, cbic\n",
      "\n",
      "\n",
      "Top words in Cluster 10: 160, gst, tax, 2020, rs, ed, father, 01, date, bank\n",
      "\n",
      "\n",
      "Top words in Cluster 11: gst, sir, complaint, 39, department, 2020, request, refund, firms, fraud\n",
      "\n",
      "\n",
      "Top words in Cluster 12: tax, income, return, gst, filed, 2019, sir, office, date, mai\n",
      "\n",
      "\n",
      "Top words in Cluster 13: gst, company, tax, transfer, delhi, guntur, sir, sr, government, payment\n",
      "\n",
      "\n",
      "Top words in Cluster 14: tax, income, gst, sir, 2020, refund, department, request, 39, 2021\n",
      "\n",
      "\n",
      "Top words in Cluster 15: family, indian, return, things, gst, site, income, refund, issue, sent\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "\n",
    "# Apply HDBSCAN to the obtained embeddings\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True)\n",
    "clusters = clusterer.fit_predict(obtained_embeddings)\n",
    "\n",
    "# Filter the original data to include only those rows that were clustered\n",
    "clustered_indices = [i for i, label in enumerate(clusters) if label != -1]\n",
    "filtered_clustered_data = filtered_data.iloc[clustered_indices]\n",
    "\n",
    "# Assign cluster labels to the filtered clustered data\n",
    "filtered_clustered_data['Cluster'] = clusters[clusters != -1]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "for cluster_num in set(clusters) - {-1}:  # Exclude noise (-1)\n",
    "    # print(f\"Documents in Cluster {cluster_num}:\")\n",
    "    cluster_docs = filtered_clustered_data[filtered_clustered_data['Cluster'] == cluster_num]['description']\n",
    "    # print(cluster_docs.head())  # Display a few documents from the cluster\n",
    "    \n",
    "    # Get and print the top words for each cluster\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    print(f\"Top words in Cluster {cluster_num}: {', '.join([word for word, freq in top_words])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae1055b8-302d-49e3-a256-5cc4c4c43652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Assuming 'filtered_data' is a DataFrame that includes a 'preprocessed' column containing the text data\n",
    "# Assuming 'obtained_embeddings' are your precomputed embeddings for the 'filtered_data'\n",
    "\n",
    "def get_top_words(documents, n_top_words=10):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "def recursive_clustering(level, data, embeddings, min_cluster_size=15, min_samples=None, results=None):\n",
    "    if results is None:\n",
    "        results = {}\n",
    "\n",
    "    clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, prediction_data=True)\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    unique_clusters = set(clusters) - {-1}  # Exclude noise if present\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_mask = clusters == cluster_id\n",
    "        cluster_data = data.iloc[cluster_mask]\n",
    "        cluster_embeddings = embeddings[cluster_mask]\n",
    "\n",
    "        # Extract top words for the current cluster\n",
    "        documents = cluster_data['description'].tolist()\n",
    "        top_words = get_top_words(documents)\n",
    "\n",
    "        # Store top words with their frequencies in the results dictionary\n",
    "        cluster_label = f\"{level}{cluster_id}\" if level else str(cluster_id)\n",
    "        results[cluster_label] = {'top_words': [word for word, freq in top_words]}\n",
    "\n",
    "        # Recursive call if the cluster is large enough\n",
    "        if len(cluster_data) > 100:  # Adjust threshold as needed\n",
    "            new_level = f\"{cluster_label}.\"\n",
    "            recursive_clustering(new_level, cluster_data, cluster_embeddings, min_cluster_size, min_samples, results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "hierarchical_topics = recursive_clustering(\"\", filtered_data, obtained_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77f72c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 Topics: tax, income, gst, sir, 2020, 39, pan, refund, rs, department\n",
      "Cluster 0.0 Topics: tax, income, gst, sir, 2020, department, 39, refund, request, pan\n",
      "Cluster 0.1 Topics: tax, gst, rs, income, sir, india, refund, 2021, demand, pan\n",
      "Cluster 1 Topics: tax, gst, sir, 39, income, department, years, refund, request, government\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to display the hierarchical topics\n",
    "def display_topics(hierarchical_topics, level=0):\n",
    "    for cluster_label, info in hierarchical_topics.items():\n",
    "        indent = '  ' * level\n",
    "        # Since top_words is a list of words, directly join them\n",
    "        top_words = ', '.join(info['top_words'])\n",
    "        print(f\"{indent}Cluster {cluster_label} Topics: {top_words}\")\n",
    "\n",
    "# Call the function to display the topics\n",
    "display_topics(hierarchical_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e9f8f20-2fce-4dc9-b48a-3c231fe6a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_clustering(level, data, embeddings, min_cluster_size=15, min_samples=None, results=None):\n",
    "    if results is None:\n",
    "        results = {}\n",
    "\n",
    "    clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, prediction_data=True)\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "unique_clusters = set(clusters) - {-1}  # Exclude noise if present\n",
    "    #unique_clusters = set(clusters)\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_mask = clusters == cluster_id\n",
    "        cluster_data = data.iloc[cluster_mask]\n",
    "        cluster_embeddings = embeddings[cluster_mask]\n",
    "\n",
    "        # Extract top words for the current cluster\n",
    "        documents = cluster_data['description'].tolist()\n",
    "        top_words = get_top_words(documents)\n",
    "\n",
    "        # Store top words, their frequencies, and the number of documents in the cluster\n",
    "        cluster_label = f\"{level}{cluster_id}\" if level else str(cluster_id)\n",
    "        results[cluster_label] = {\n",
    "            'top_words': [word for word, _ in top_words],\n",
    "            'no_of_documents': len(cluster_data)\n",
    "        }\n",
    "\n",
    "        # Recursive call if the cluster is large enough\n",
    "        if len(cluster_data) > 100:  # Adjust threshold as needed\n",
    "            new_level = f\"{cluster_label}.\"\n",
    "            recursive_clustering(new_level, cluster_data, cluster_embeddings, min_cluster_size, min_samples, results)\n",
    "\n",
    "    return results\n",
    "# Assuming 'filtered_data' and 'obtained_embeddings' are defined as per your setup\n",
    "hierarchical_topics = recursive_clustering(\"\", filtered_data, obtained_embeddings)\n",
    "#display_topics(hierarchical_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "637ba710-2c9c-43c3-8f7a-fb03374c25b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (Documents: 1129) Topics: tax, income, gst, sir, 2020, 39, pan, refund, rs, department\n",
      "Cluster 0.0 (Documents: 207) Topics: tax, income, gst, sir, 2020, department, 39, refund, request, pan\n",
      "Cluster 0.1 (Documents: 43) Topics: tax, gst, rs, income, sir, india, refund, 2021, demand, pan\n",
      "Cluster 1 (Documents: 54) Topics: tax, gst, sir, 39, income, department, years, refund, request, government\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def display_topics(hierarchical_topics, level=0):\n",
    "    for cluster_label, info in hierarchical_topics.items():\n",
    "        indent = '  ' * level\n",
    "        top_words = ', '.join(info['top_words'])\n",
    "        no_of_documents = info['no_of_documents']\n",
    "        print(f\"{indent}Cluster {cluster_label} (Documents: {no_of_documents}) Topics: {top_words}\")\n",
    "\n",
    "display_topics(hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6bccf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the starting date (YYYY-MM-DD):  2021-01-01\n",
      "Enter the ending date (YYYY-MM-DD):  2021-12-31\n",
      "Enter the initforward string:  1. DEABD - Department of Financial Services (Banking Division)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (Documents: 51) Topics: bank, sir, loan, account, branch, 2021, rs, 160, help, hai\n",
      "Cluster 1 (Documents: 20) Topics: bank, loan, sir, 2020, help, pay, senior, india, finance, request\n",
      "Cluster 2 (Documents: 19) Topics: bank, 160, loan, 2020, branch, rs, passbook, 39, sir, entry\n",
      "Cluster 3 (Documents: 28) Topics: bank, sir, 2020, 160, loan, rs, salary, request, account, 01\n",
      "Cluster 4 (Documents: 22) Topics: bank, rs, loan, account, sir, 2020, hai, request, help, salary\n",
      "Cluster 5 (Documents: 20) Topics: bank, loan, card, sir, request, account, credit, money, number, pension\n",
      "Cluster 6 (Documents: 21) Topics: bank, branch, account, 39, sir, loan, credit, request, rs, customers\n",
      "Cluster 7 (Documents: 2365) Topics: bank, loan, sir, account, branch, 39, rs, 2020, sbi, request\n",
      "Cluster 7.0 (Documents: 19) Topics: bank, loan, sir, 39, branch, account, 2020, business, card, project\n",
      "Cluster 7.1 (Documents: 16) Topics: bank, sir, 39, pay, hai, loan, help, ki, pension, march\n",
      "Cluster 7.2 (Documents: 96) Topics: bank, loan, sir, branch, account, 39, india, sbi, request, home\n",
      "Cluster 7.3 (Documents: 23) Topics: bank, sir, loan, sbi, request, branch, hai, account, se, 2020\n",
      "Cluster 7.4 (Documents: 301) Topics: bank, loan, sir, account, 39, 2020, branch, india, sbi, help\n",
      "Cluster 7.5 (Documents: 33) Topics: bank, sir, account, loan, 39, 2020, dhfl, request, hai, rbi\n",
      "Cluster 7.6 (Documents: 16) Topics: 160, bank, loan, india, branch, sir, account, 39, request, minor\n",
      "Cluster 8 (Documents: 36) Topics: bank, loan, sir, hai, request, ka, 39, rs, idbi, mr\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings):\n",
    "    # Convert the 'regdate' column to datetime\n",
    "    data['regdate'] = pd.to_datetime(data['regdate'])\n",
    "\n",
    "    # Filter the data based on the conditions\n",
    "    filtered_data = data[(data['regdate'] >= pd.to_datetime(starting_date)) &\n",
    "                         (data['regdate'] <= pd.to_datetime(ending_date)) &\n",
    "                         (data['initforward'] == initforward_str)]\n",
    "\n",
    "    # Get indices of filtered data\n",
    "    indices = filtered_data.index.tolist()\n",
    "\n",
    "    # Ensure indices are within the range of stored_embeddings length\n",
    "    valid_indices = [index for index in indices if index < len(stored_embeddings)]\n",
    "\n",
    "    # Retrieve embeddings for the filtered data\n",
    "    obtained_embeddings = np.array(stored_embeddings)[valid_indices]\n",
    "\n",
    "    # Retrieve descriptions for the filtered data\n",
    "    descriptions = filtered_data.loc[valid_indices, 'description'].tolist()\n",
    "    filtered_data = filtered_data.loc[valid_indices]\n",
    "\n",
    "    return obtained_embeddings, descriptions, filtered_data\n",
    "\n",
    "# Inputs from user\n",
    "starting_date = input(\"Enter the starting date (YYYY-MM-DD): \")\n",
    "ending_date = input(\"Enter the ending date (YYYY-MM-DD): \")\n",
    "initforward_str = input(\"Enter the initforward string: \")\n",
    "\n",
    "\n",
    "data = df[df['initforward'].isna() == False]  # Filter data where 'initforward' is not NaN\n",
    "\n",
    "# Get the embeddings and descriptions\n",
    "obtained_embeddings, descriptions, filtered_data = get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings)\n",
    "\n",
    "\n",
    "def get_top_words(documents, n_top_words=10):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "def recursive_clustering(level, data, embeddings, min_cluster_size=15, min_samples=None, results=None):\n",
    "    if results is None:\n",
    "        results = {}\n",
    "\n",
    "    clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, prediction_data=True)\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    unique_clusters = set(clusters) - {-1}  # Exclude noise if present\n",
    "    #unique_clusters = set(clusters)\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_mask = clusters == cluster_id\n",
    "        cluster_data = data.iloc[cluster_mask]\n",
    "        cluster_embeddings = embeddings[cluster_mask]\n",
    "\n",
    "        # Extract top words for the current cluster\n",
    "        documents = cluster_data['description'].tolist()\n",
    "        top_words = get_top_words(documents)\n",
    "\n",
    "        # Store top words, their frequencies, and the number of documents in the cluster\n",
    "        cluster_label = f\"{level}{cluster_id}\" if level else str(cluster_id)\n",
    "        results[cluster_label] = {\n",
    "            'top_words': [word for word, _ in top_words],\n",
    "            'no_of_documents': len(cluster_data)\n",
    "        }\n",
    "\n",
    "        # Recursive call if the cluster is large enough\n",
    "        if len(cluster_data) > 100:\n",
    "            new_level = f\"{cluster_label}.\"\n",
    "            recursive_clustering(new_level, cluster_data, cluster_embeddings, min_cluster_size, min_samples, results)\n",
    "\n",
    "    return results\n",
    "\n",
    "hierarchical_topics = recursive_clustering(\"\", filtered_data, obtained_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def display_topics(hierarchical_topics, level=0):\n",
    "    for cluster_label, info in hierarchical_topics.items():\n",
    "        indent = '  ' * level\n",
    "        top_words = ', '.join(info['top_words'])\n",
    "        no_of_documents = info['no_of_documents']\n",
    "        print(f\"{indent}Cluster {cluster_label} (Documents: {no_of_documents}) Topics: {top_words}\")\n",
    "\n",
    "display_topics(hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3324424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the starting date (YYYY-MM-DD):  2021-01-01\n",
      "Enter the ending date (YYYY-MM-DD):  2021-12-31\n",
      "Enter the initforward string:  1. DEABD - Department of Financial Services (Banking Division)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (Documents: 51) Topics: bank, sir, loan, 160, account, branch, 2021, rs, hai, help\n",
      "Cluster 1 (Documents: 20) Topics: bank, loan, sir, saab, help, senior, 2020, pay, citizens, finance\n",
      "Cluster 2 (Documents: 19) Topics: bank, 160, passbook, loan, 2020, rs, branch, entry, 39, sir\n",
      "Cluster 3 (Documents: 28) Topics: bank, 160, 2020, sir, loan, salary, rs, salaries, 01, request\n",
      "Cluster 4 (Documents: 22) Topics: bank, rs, loan, account, sir, hai, 2020, request, salary, given\n",
      "Cluster 5 (Documents: 20) Topics: bank, loan, card, idbi, request, sir, pension, rajesh, pai, account\n",
      "Cluster 6 (Documents: 21) Topics: bank, fd, branch, account, sir, 39, loan, credit, customers, holders\n",
      "Cluster 7 (Documents: 2365) Topics: bank, loan, sir, account, branch, 39, 160, rs, 2020, sbi\n",
      "Cluster 7.0 (Documents: 19) Topics: bank, loan, 39, sir, branch, project, account, quot, card, 2020\n",
      "Cluster 7.1 (Documents: 16) Topics: bank, sir, building, meri, pay, hai, 39, march, pension, loan\n",
      "Cluster 7.2 (Documents: 96) Topics: bank, loan, sir, branch, account, 39, india, sbi, home, request\n",
      "Cluster 7.3 (Documents: 23) Topics: bank, sir, loan, sbi, request, branch, da, mey, hai, pnbhfl\n",
      "Cluster 7.4 (Documents: 301) Topics: bank, loan, sir, account, 39, 2020, branch, india, sbi, help\n",
      "Cluster 7.5 (Documents: 33) Topics: bank, sir, account, loan, dhfl, 39, 2020, request, 21, hai\n",
      "Cluster 7.6 (Documents: 16) Topics: 160, bank, loan, india, minor, branch, sir, account, 39, cac\n",
      "Cluster 8 (Documents: 36) Topics: bank, loan, hai, sir, idbi, request, ka, rajesh, pai, nahin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings):\n",
    "    # Convert the 'regdate' column to datetime\n",
    "    data['regdate'] = pd.to_datetime(data['regdate'])\n",
    "\n",
    "    # Filter the data based on the conditions\n",
    "    filtered_data = data[(data['regdate'] >= pd.to_datetime(starting_date)) &\n",
    "                         (data['regdate'] <= pd.to_datetime(ending_date)) &\n",
    "                         (data['initforward'] == initforward_str)]\n",
    "\n",
    "    # Get indices of filtered data\n",
    "    indices = filtered_data.index.tolist()\n",
    "\n",
    "    # Ensure indices are within the range of stored_embeddings length\n",
    "    valid_indices = [index for index in indices if index < len(stored_embeddings)]\n",
    "\n",
    "    # Retrieve embeddings for the filtered data\n",
    "    obtained_embeddings = np.array(stored_embeddings)[valid_indices]\n",
    "\n",
    "    # Retrieve descriptions for the filtered data\n",
    "    descriptions = filtered_data.loc[valid_indices, 'description'].tolist()\n",
    "    filtered_data = filtered_data.loc[valid_indices]\n",
    "\n",
    "    return obtained_embeddings, descriptions, filtered_data\n",
    "\n",
    "# Inputs from user\n",
    "starting_date = input(\"Enter the starting date (YYYY-MM-DD): \")\n",
    "ending_date = input(\"Enter the ending date (YYYY-MM-DD): \")\n",
    "initforward_str = input(\"Enter the initforward string: \")\n",
    "\n",
    "\n",
    "data = df[df['initforward'].isna() == False]  # Filter data where 'initforward' is not NaN\n",
    "\n",
    "# Get the embeddings and descriptions\n",
    "obtained_embeddings, descriptions, filtered_data = get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, stored_embeddings)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_top_words_per_cluster(documents_by_cluster, n_top_words=10):\n",
    "    # Check if documents_by_cluster contains any valid text\n",
    "    if not any(len(doc.strip()) > 0 for doc in documents_by_cluster):\n",
    "        return [[] for _ in documents_by_cluster]\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents_by_cluster)\n",
    "    \n",
    "    top_words_per_cluster = []\n",
    "    for cluster_idx in range(tfidf_matrix.shape[0]):\n",
    "        feature_array = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "        tfidf_scores = tfidf_matrix[cluster_idx].toarray().flatten()\n",
    "        top_indices = tfidf_scores.argsort()[-n_top_words:][::-1]\n",
    "        top_words = feature_array[top_indices]\n",
    "        top_scores = tfidf_scores[top_indices]\n",
    "        top_words_per_cluster.append(list(zip(top_words, top_scores)))\n",
    "    \n",
    "    return top_words_per_cluster\n",
    "\n",
    "\n",
    "def recursive_clustering(level, data, embeddings, min_cluster_size=15, min_samples=None, results=None):\n",
    "    if results is None:\n",
    "        results = {}\n",
    "\n",
    "    clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, prediction_data=True)\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    unique_clusters = set(clusters) - {-1}  # Exclude noise if present\n",
    "\n",
    "    # Prepare documents by cluster for class-based TF-IDF\n",
    "    documents_by_cluster = [' '.join(data.iloc[clusters == cluster_id]['description'].tolist()) for cluster_id in unique_clusters]\n",
    "    top_words_per_cluster = get_top_words_per_cluster(documents_by_cluster)\n",
    "\n",
    "    for i, cluster_id in enumerate(unique_clusters):\n",
    "        cluster_label = f\"{level}{cluster_id}\" if level else str(cluster_id)\n",
    "        cluster_mask = clusters == cluster_id\n",
    "        cluster_data = data.iloc[cluster_mask]\n",
    "\n",
    "        # Store top words, their frequencies, and the number of documents in the cluster\n",
    "        results[cluster_label] = {\n",
    "            'top_words': [word for word, _ in top_words_per_cluster[i]],\n",
    "            'no_of_documents': len(cluster_data)\n",
    "        }\n",
    "\n",
    "        # Recursive call if the cluster is large enough\n",
    "        if len(cluster_data) > 100:\n",
    "            new_level = f\"{cluster_label}.\"\n",
    "            recursive_clustering(new_level, cluster_data, embeddings[cluster_mask], min_cluster_size, min_samples, results)\n",
    "\n",
    "    return results\n",
    "\n",
    "hierarchical_topics = recursive_clustering(\"\", filtered_data, obtained_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def display_topics(hierarchical_topics, level=0):\n",
    "    for cluster_label, info in hierarchical_topics.items():\n",
    "        indent = '  ' * level\n",
    "        top_words = ', '.join(info['top_words'])\n",
    "        no_of_documents = info['no_of_documents']\n",
    "        print(f\"{indent}Cluster {cluster_label} (Documents: {no_of_documents}) Topics: {top_words}\")\n",
    "\n",
    "display_topics(hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "418e2e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  hai, se, ki, ke, ka, ko, nahi, mai, mera, bhi\n",
      "Topic 1:  subsidy, pmay, home, loan, housing, application, for, scheme, id, nhb\n",
      "Topic 2:  account, branch, bank, of, the, is, to, open, not, address\n",
      "Topic 3:  atm, transaction, amount, account, money, but, my, no, bank, debited\n",
      "Topic 4:  mudra, business, loan, start, for, to, am, project, sir, my\n",
      "Topic 5:  banks, privatisation, sector, public, are, private, banking, of, privatization, will\n",
      "Topic 6:  pension, cppc, ppo, of, delhi, no, sbi, from, the, family\n",
      "Topic 7:  loan, my, pay, family, am, help, me, job, to, amount\n",
      "Topic 8:  pmegp, loan, project, under, the, for, application, manager, applied, unit\n",
      "Topic 9:  education, 160, college, loan, my, university, fees, father, for, studies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_model = BERTopic()\n",
    "docs = descriptions\n",
    "\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "# Extract the topics and their frequencies\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Get the top 10 topics\n",
    "top_topics = topic_info.head(11)  # Get 11 as one topic might be -1 (outliers)\n",
    "\n",
    "# Print the top 10 topics and their top 10 words\n",
    "for topic in top_topics['Topic']:\n",
    "    if topic != -1:  # excluding the outlier cluster\n",
    "        # Get top 10 words for the topic\n",
    "        topic_words = topic_model.get_topic(topic)[:10]\n",
    "        # Extract only the words (not the scores)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        # Print the topic and its words\n",
    "        print(f\"Topic {topic}: \", \", \".join(words))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d2506-617f-49bb-92c1-29b04f93f0bd",
   "metadata": {},
   "source": [
    "## embedding using All-Mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b61bec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff253bcbcb047ccaccc7443d1bee98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecd8cf75d64426780496753686f3af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abaf0910c872403890ced8fd1ee9f369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25eaa971b78d441bb8149ab760b0998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b548690e2045dea6e67c275b273f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674067e239874e02b9379febf39f0514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ad22bee9aa42b187ce6e5238afda64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd841cee6ca48b49a20ca210d05072e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbc2227d8824f8cb33c529cbe635e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee08e10e67a491b96c2dd46bb4a97b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681c31ac9e2443d695a1dfaef731812e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a672836ebaf94e8c9b6be60805192289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14badc7a19674205bdfe85e4293c779d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29abe1f43e845eba8b4a2f2c01537a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d683a92065a49f4b9079b6f54e74e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of the first document: [ 2.16074381e-02  1.19706634e-02  8.70135240e-03 -1.62722059e-02\n",
      " -1.23751964e-02 -2.16316953e-02 -2.41901837e-02  7.25022927e-02\n",
      " -7.08178245e-03  3.75240333e-02  7.39339832e-03  6.73705665e-03\n",
      "  5.46423942e-02 -8.31892807e-03 -5.20825237e-02  2.64008921e-02\n",
      " -4.26560342e-02  2.56239846e-02 -1.37550719e-02 -2.77839135e-02\n",
      "  1.80314842e-03  5.69313988e-02 -3.97609733e-02  1.34141222e-02\n",
      "  1.54068591e-02  1.71681978e-02  1.94510762e-02 -6.68101460e-02\n",
      "  1.18961586e-02  2.90017370e-02  4.31473320e-03 -1.45383719e-02\n",
      "  1.20369429e-02 -3.20489854e-02  1.70769590e-06 -3.23383138e-02\n",
      "  5.60662225e-02  1.61482058e-02 -7.08680507e-03  7.71600148e-03\n",
      " -2.58430596e-02 -5.13192154e-02 -3.84506620e-02  9.56505304e-04\n",
      " -1.06872180e-02  7.21483082e-02 -5.71417513e-05  8.16335827e-02\n",
      "  2.42324620e-02  6.68060360e-03  3.94068658e-03 -2.39721723e-02\n",
      "  5.73041253e-02 -4.52790409e-03 -3.49389575e-02 -6.59813806e-02\n",
      " -4.10370640e-02 -5.97784333e-02 -2.05336288e-02 -8.13885704e-02\n",
      " -8.50504171e-03  2.05709040e-02 -3.46970069e-03  3.91118601e-02\n",
      " -1.30914142e-02  1.97487567e-02  1.85064822e-02 -1.94326136e-02\n",
      "  6.31773546e-02 -1.98524985e-02  5.46416491e-02  1.44375702e-02\n",
      "  1.60342921e-02  1.70139931e-02 -1.46901282e-02 -2.02586334e-02\n",
      "  4.53424379e-02  6.60163984e-02 -4.03717384e-02  6.67821558e-04\n",
      "  5.93790412e-02  9.23598092e-03  9.08524822e-03 -3.46378088e-02\n",
      "  3.99909355e-03 -1.16080036e-02  3.53703275e-02 -4.20905976e-03\n",
      " -8.38429201e-03 -5.48387645e-03  3.78796794e-02  1.87965184e-02\n",
      "  5.61089115e-03 -4.94397655e-02  3.47144157e-02  2.23240610e-02\n",
      "  1.46696914e-03 -6.36752322e-02 -1.03220670e-03  6.14727242e-03\n",
      "  1.79892648e-02 -1.27728551e-03  1.45684928e-02  6.23816485e-03\n",
      " -1.07168630e-02  5.33412956e-02 -4.68778498e-02 -5.05464822e-02\n",
      "  2.18702573e-02  3.90841775e-02 -4.92225355e-03  1.41714690e-02\n",
      " -6.03688462e-03  2.24415213e-02  8.96891672e-03 -4.67393035e-03\n",
      " -7.93505758e-02  2.33308636e-02  4.53801826e-02  4.42117155e-02\n",
      "  1.15773737e-01  3.69930193e-02  9.14395973e-03  7.26110069e-03\n",
      " -8.51274878e-02  1.82941388e-02 -3.71174403e-02 -7.14996271e-03\n",
      "  1.73249107e-03 -5.88467754e-02 -9.54950042e-03  2.68230643e-02\n",
      "  1.09187197e-02  1.16300182e-02 -4.27312143e-02  4.51980457e-02\n",
      " -7.30925100e-03  3.83111313e-02 -6.35694191e-02  5.16798533e-02\n",
      "  1.11101065e-02 -5.56530245e-02 -2.07830779e-02  3.85247096e-02\n",
      " -5.89938229e-03  4.29964922e-02 -4.13614959e-02  1.45453233e-02\n",
      " -1.44720648e-03  2.48479936e-03  1.50498729e-02  4.26231185e-03\n",
      "  3.05493120e-02 -1.03406236e-02 -2.30667796e-02  1.96202798e-03\n",
      " -3.63498293e-02 -4.53449599e-02  8.99386965e-03 -4.56608683e-02\n",
      " -2.00071298e-02  1.49315652e-02  1.45622864e-02  6.22563623e-03\n",
      "  5.17360494e-02 -4.05094512e-02  8.12485665e-02  4.79826257e-02\n",
      " -8.77671987e-02  2.55316906e-02  6.69626193e-03  1.62696782e-02\n",
      "  4.52579483e-02 -3.52523215e-02 -3.27469483e-02  3.94857898e-02\n",
      "  3.83174121e-02  1.07817084e-01 -2.71598287e-02  2.12561581e-02\n",
      " -2.53511034e-02 -4.55451151e-03 -3.27076241e-02 -1.05723459e-02\n",
      " -1.36105893e-02  3.72016206e-02  8.97948164e-03 -4.53736484e-02\n",
      "  2.48610787e-02 -1.34892687e-02  2.14999020e-02 -3.51521000e-03\n",
      " -7.49600902e-02 -2.58441884e-02  1.50870029e-02  7.36480802e-02\n",
      "  4.41144174e-03 -5.54239610e-04  5.27783856e-03  5.43693230e-02\n",
      "  9.98038333e-03 -7.19470158e-02  9.39245000e-02 -3.96574177e-02\n",
      "  1.18509568e-02 -8.02870933e-03  2.64748535e-03  1.58959758e-02\n",
      " -2.24211607e-02 -2.82968991e-02 -6.87246919e-02  2.47255396e-02\n",
      "  1.71884391e-02 -9.57487430e-03 -3.67747843e-02  5.03902249e-02\n",
      " -2.52487771e-02  3.29816416e-02 -5.36549184e-03 -5.73639106e-03\n",
      "  9.48347058e-03  1.87864155e-02  5.92039376e-02  1.27417138e-02\n",
      "  3.55968960e-02  1.87795442e-02  5.82689885e-03 -3.77215296e-02\n",
      "  6.08249344e-02 -1.34016536e-02 -8.73464439e-03 -2.57485248e-02\n",
      "  1.97243895e-02 -1.69283543e-02  3.59012522e-02  3.81498523e-02\n",
      " -3.84770408e-02 -5.89415990e-02 -6.28030896e-02 -2.29293481e-02\n",
      "  1.97410285e-02  4.42258418e-02  3.68063115e-02 -1.08648995e-02\n",
      "  8.69770199e-02  2.28116699e-02 -3.69598828e-02 -1.95251554e-02\n",
      "  1.99242923e-02 -1.73822157e-02 -2.16854941e-02 -4.01198491e-02\n",
      "  7.99710676e-02  5.15194237e-02 -1.35935647e-02  1.05815986e-02\n",
      "  9.48508736e-03  3.42763215e-02 -8.01220834e-02  1.72545351e-02\n",
      "  1.23739466e-02  4.33041453e-02 -8.73528328e-03  1.92106050e-02\n",
      " -2.45934576e-02 -1.31855709e-02  1.40352594e-02  1.07425619e-02\n",
      " -3.58625166e-02  1.53784519e-02 -1.44901415e-02 -2.64065787e-02\n",
      " -1.93907265e-02  3.44673768e-02  2.13665166e-03 -9.93200019e-02\n",
      "  5.23607768e-02  4.83800769e-02 -1.23960944e-02  2.11927816e-02\n",
      "  4.44308436e-03  7.60805383e-02  4.44125338e-03  1.95919592e-02\n",
      "  2.13020779e-02 -1.25994114e-02  5.84617583e-03 -4.81634438e-02\n",
      " -7.53395399e-03  5.71569502e-02  8.09404999e-03  1.61840133e-02\n",
      " -3.66733521e-02  2.29119975e-02  2.92436359e-03 -3.98425236e-02\n",
      " -3.85361090e-02 -6.85719168e-03 -1.55809904e-02 -2.37840731e-02\n",
      " -7.74980485e-02  8.52863118e-02  2.62336545e-02 -2.03423742e-02\n",
      "  3.98581236e-04  4.30860557e-02  3.67970690e-02  1.15208901e-01\n",
      " -4.06515487e-02 -8.60856250e-02  1.38288140e-02  4.17284444e-02\n",
      " -2.88334824e-02 -5.16307801e-02  6.36668084e-03  7.25613236e-02\n",
      " -1.55545687e-02 -9.15482827e-03  1.19286049e-02  1.67038515e-02\n",
      " -5.04182745e-03  7.10129924e-03  2.31100153e-02  2.31371038e-02\n",
      " -6.92148134e-02 -2.68308278e-02 -5.27873524e-02 -2.23969258e-02\n",
      "  6.80276677e-02 -3.52331437e-02  4.43956777e-02 -4.63623703e-02\n",
      " -2.89343949e-02 -4.78352159e-02  4.73290542e-03 -4.39567911e-03\n",
      "  9.19767395e-02  2.52421573e-02  5.13684154e-02 -4.34948457e-03\n",
      " -2.13700067e-02 -3.97959799e-02 -2.87648570e-02 -2.67455783e-02\n",
      "  1.11508509e-02  1.47912791e-02  4.14565811e-03 -2.15158146e-02\n",
      "  5.62018901e-03 -1.54945534e-02 -5.93985468e-02 -8.70020222e-03\n",
      " -1.84268579e-02 -1.52740953e-03 -3.82733420e-02  3.49454172e-02\n",
      " -1.04144672e-02  4.09599356e-02  1.36931427e-02  4.66608964e-02\n",
      " -3.72494869e-02 -2.95813102e-02  1.56704970e-02 -3.85930911e-02\n",
      "  1.67439207e-02 -1.17509309e-02 -9.78415273e-03 -4.10368927e-02\n",
      " -3.72078791e-02 -1.98208205e-02 -5.03847077e-02 -1.09362975e-02\n",
      " -2.08886643e-03 -6.63038925e-04 -4.67630029e-02 -2.00171228e-02\n",
      " -3.00551541e-02 -1.40032521e-03 -2.65934579e-02 -5.04634622e-03\n",
      " -4.88990732e-02  5.85698225e-02  2.88098976e-02 -8.53666934e-05\n",
      " -3.17732319e-02  8.88855197e-03  1.08209997e-03 -1.12968711e-02\n",
      "  1.53056290e-02 -2.74305325e-02 -3.31876948e-02  2.34224256e-02\n",
      " -2.98546609e-02  3.97328623e-02 -2.31918339e-02  2.55480353e-02\n",
      " -3.82156409e-02  1.92631613e-02  1.35972798e-02  1.27040315e-02\n",
      " -3.10967788e-02 -3.77959386e-02  5.08437604e-02  1.90209132e-02\n",
      " -4.87205870e-02 -6.11616671e-03 -8.20411835e-03 -4.63189334e-02\n",
      "  2.52172677e-03 -5.06169349e-02  1.11884214e-02 -3.63984704e-02\n",
      " -5.09658866e-02  1.08855469e-02  5.71411848e-03 -1.27941025e-02\n",
      " -1.03798946e-02 -3.78003605e-02  7.69927632e-03 -4.78400625e-02\n",
      "  2.50208341e-02 -2.03703549e-02 -4.40409686e-03 -3.37624885e-02\n",
      " -2.94477008e-02 -5.26187755e-02  3.44971605e-02 -1.62331457e-03\n",
      "  7.04874797e-03 -5.26526347e-02  2.27965750e-02 -1.55212870e-02\n",
      "  2.21686102e-02 -1.60288438e-02  5.48509024e-02  6.04787581e-02\n",
      " -9.08719376e-03  1.23070078e-02 -7.04275863e-03 -1.43554173e-02\n",
      " -3.02440375e-02  5.54046519e-02  4.15135436e-02 -2.61511207e-02\n",
      "  1.56348720e-02 -5.19650290e-03  5.64779760e-03  2.90048234e-02\n",
      " -2.16762926e-02 -4.69812043e-02  1.16735063e-02 -5.78904226e-02\n",
      " -4.79864003e-03  1.98609661e-02 -4.25902242e-03 -7.60438340e-03\n",
      " -8.00800174e-02 -2.65453402e-02 -5.30906348e-03  1.02001261e-02\n",
      " -4.36481051e-02  2.98506790e-03 -5.70815653e-02 -4.22919877e-02\n",
      "  4.60387021e-02  8.12777653e-02 -9.58838221e-03  4.72589061e-02\n",
      " -7.19326548e-03 -7.06694881e-03  4.80052410e-03  1.97783858e-02\n",
      " -1.89995598e-02 -2.46342774e-02  3.70551050e-02 -4.11186330e-02\n",
      " -1.08871311e-02  8.49822909e-03  7.93886632e-02  4.01285253e-02\n",
      "  2.37684790e-02  1.21649997e-02  4.39930893e-02 -5.57678975e-02\n",
      " -3.63135734e-03 -2.03981753e-02  4.20551784e-02 -3.30369477e-03\n",
      " -1.21299941e-02 -3.84032018e-02  4.72362153e-02  6.58108247e-03\n",
      "  2.49097608e-02 -1.21945433e-01  1.45772574e-02 -2.80224439e-02\n",
      "  7.77522922e-02  3.31847672e-03  7.60853216e-02  3.73271331e-02\n",
      "  1.73007753e-02  1.94485970e-02  4.56079021e-02  1.45496735e-02\n",
      " -6.43038824e-02 -1.30869737e-02  4.61735204e-03 -5.98560721e-02\n",
      " -1.47542087e-02 -3.36885788e-02 -2.84077972e-02 -1.62483025e-02\n",
      "  1.72775444e-02  4.03026231e-02  3.60274985e-02  9.90674924e-03\n",
      "  5.57735190e-02 -1.03504965e-02 -4.20922339e-02 -2.82424316e-02\n",
      "  2.82728728e-02  1.28860781e-02  1.95268895e-02  2.20935997e-02\n",
      " -3.76748107e-02  2.74862982e-02  9.70523804e-03 -9.82801896e-03\n",
      "  5.62408492e-02 -3.37169692e-02  1.33685553e-02  5.89493196e-04\n",
      "  2.31602788e-03 -2.50572693e-02  9.42484010e-03 -6.95682392e-02\n",
      " -4.74622622e-02  1.35963717e-02  1.04098124e-02 -4.01450433e-02\n",
      "  2.15184521e-02  2.85811629e-02  2.96581965e-02  2.35276837e-02\n",
      " -2.18467452e-02 -4.99369996e-03  2.83320695e-02 -2.43886001e-02\n",
      "  1.78023130e-02 -2.25182362e-02 -5.19515797e-02 -5.61395567e-03\n",
      " -2.45614648e-02  6.13155402e-03 -3.47157568e-02 -5.37011700e-33\n",
      " -1.64375955e-03  2.34580692e-02  1.17775537e-02  3.20862457e-02\n",
      " -1.21250236e-02  2.64791977e-02 -1.29573578e-02  1.66549478e-02\n",
      "  2.37678154e-03  1.04895951e-02  2.24030646e-03  4.81167473e-02\n",
      " -1.00125943e-03  1.87154941e-03  2.83628665e-02 -5.48949391e-02\n",
      "  8.67432915e-03  5.40919863e-02 -2.92106327e-02 -4.04166467e-02\n",
      " -2.46372689e-02 -1.71650127e-02  2.69128159e-02  2.16077697e-02\n",
      " -5.26935831e-02 -4.84485645e-03  3.03948056e-02 -1.88621730e-02\n",
      " -1.88215356e-02 -1.33971768e-02  2.41513476e-02 -1.88672692e-02\n",
      " -2.79382020e-02  6.29130453e-02  2.27084700e-02  1.76404566e-02\n",
      " -4.34850343e-02  1.77853033e-02  1.94178782e-02  3.60544883e-02\n",
      " -3.87549587e-02 -6.56559877e-03  4.12824973e-02  1.45369172e-02\n",
      " -4.07693535e-02  1.81475896e-02 -2.62237191e-02  4.02506962e-02\n",
      "  4.62839939e-02 -7.16436803e-02 -1.59849320e-02  3.12796533e-02\n",
      "  2.53539402e-02  5.34717999e-02  2.02907883e-02 -6.21722564e-02\n",
      "  3.13528404e-02 -3.53231020e-02 -3.81819531e-02 -5.09293489e-02\n",
      " -7.29549974e-02  2.94523556e-02  3.43887322e-02  7.79216504e-03\n",
      " -4.45701741e-02 -4.54018861e-02 -4.78946529e-02  2.61911843e-02\n",
      " -1.29931010e-02 -5.15183732e-02 -7.81999435e-03  1.42583288e-02\n",
      " -4.29944471e-02  6.17310125e-03 -8.50635860e-03 -4.20428403e-02\n",
      "  1.70860570e-02  3.17640603e-02  1.25247046e-01  7.36139640e-02\n",
      "  4.61801514e-02  3.96415330e-02 -2.05522627e-02  4.80240677e-03\n",
      " -3.84984128e-02  7.49316416e-04 -4.94749509e-02 -1.26855485e-02\n",
      "  3.19389999e-02 -2.08432321e-02  8.98254216e-02  3.42980120e-03\n",
      " -3.37576121e-02 -2.76309205e-03  1.01740941e-01 -1.35062161e-04\n",
      "  3.51991039e-04  3.47508006e-02  2.90728509e-02 -1.35721304e-02\n",
      "  9.66742449e-03 -1.58233978e-02 -1.64308678e-02  2.07725670e-02\n",
      " -6.38505165e-03 -6.10610424e-03  4.65739369e-02 -3.80299846e-03\n",
      "  2.06559841e-02  1.55640850e-02 -3.12884874e-03 -3.86377648e-02\n",
      "  1.99480485e-02 -1.27140563e-02 -5.48200160e-02 -2.78966525e-03\n",
      " -3.17735858e-02 -8.89945254e-02  1.61352735e-02  2.29454432e-02\n",
      "  1.04037281e-02  7.01011419e-02  3.96754667e-02 -5.81390299e-02\n",
      " -2.69976370e-02  4.57285047e-02 -3.41594182e-02  3.30788121e-02\n",
      "  2.07741037e-02  3.94714400e-02  4.00413871e-02  2.29161326e-02\n",
      "  2.41453478e-07 -5.11125252e-02  8.88820812e-02 -5.26114739e-03\n",
      " -2.33603436e-02 -1.88691299e-02 -5.44930063e-02 -4.03994806e-02\n",
      "  2.89522298e-02 -5.48080467e-02  6.00584410e-02  5.40635288e-02\n",
      " -3.02735227e-03 -4.37832996e-02 -1.44487303e-02 -1.02752000e-01\n",
      " -1.11347198e-01  2.05778633e-03  2.37950925e-02 -5.70609123e-02\n",
      "  1.47610996e-02  2.43985523e-02 -2.11631320e-02  7.98835307e-02\n",
      " -2.76876502e-02  3.41812521e-02  1.55134741e-02 -1.51526006e-02\n",
      " -2.46446263e-02 -1.42342150e-02 -1.93056278e-02 -4.15310897e-02\n",
      "  3.80706228e-02 -7.30777206e-03 -1.13528809e-02 -1.19347107e-02\n",
      " -2.49238368e-02  1.77043974e-02  5.24336025e-02 -5.39881038e-03\n",
      " -4.80521563e-03 -5.47087612e-03  7.18191033e-03  1.51710063e-02\n",
      "  4.35312912e-02 -1.51114389e-02  5.73747382e-02 -7.31497630e-02\n",
      " -7.07213208e-02  7.12710107e-03 -2.42118854e-02  9.55344527e-04\n",
      "  5.56530841e-02 -2.21476816e-02 -1.80437732e-02 -1.25142429e-02\n",
      " -4.36885282e-02 -1.55462176e-02 -3.74667235e-02  1.41223799e-02\n",
      "  7.42829666e-02 -2.22850256e-02 -4.46796464e-03 -5.80377970e-03\n",
      " -5.54767139e-02  2.61674989e-02 -4.58333939e-02 -8.05036630e-03\n",
      "  1.97440830e-34  4.08981219e-02 -4.36248910e-03  8.95186048e-03\n",
      " -2.83692181e-02  4.57044914e-02 -9.73294675e-03  3.45567651e-02\n",
      "  1.62936896e-02  2.91745197e-02 -7.33746439e-02  2.37673391e-02]\n",
      "Shape of embeddings: (7958, 768)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "model_name = 'all-mpnet-base-v2'  # This model is more powerful than MiniLM\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True, batch_size=32)\n",
    "\n",
    "print(\"Embedding of the first document:\", embeddings[0])\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c65dac-3fec-431b-996f-0cef149127eb",
   "metadata": {},
   "source": [
    "## using hdbscan on Mpnet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05d8da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in Cluster 0: india, crypto, cryptocurrencies, 39, digital, currency, cryptocurrency, government, bitcoin, like\n",
      "\n",
      "\n",
      "Top words in Cluster 1: ews, certificate, sbi, notification, 2019, 20, fy, valid, 2020, category\n",
      "\n",
      "\n",
      "Top words in Cluster 2: bank, account, india, minor, years, age, state, opened, paheli, child\n",
      "\n",
      "\n",
      "Top words in Cluster 3: industry, cashew, kerala, workers, banks, non, severe, crisis, years, processing\n",
      "\n",
      "\n",
      "Top words in Cluster 4: idfc, bank, 2021, 21, bonds, maturity, money, matter, courier, date\n",
      "\n",
      "\n",
      "Top words in Cluster 5: release, bhatia, non, maturity, proceeds, folio, ca, sameer, jatinder, pal\n",
      "\n",
      "\n",
      "Top words in Cluster 6: bank, january, ramjee, singh, kaladumra, ps, nagar, siwan, 2008, retired\n",
      "\n",
      "\n",
      "Top words in Cluster 7: bank, january, ramjee, singh, kaladumra, ps, nagar, siwan, 2008, retired\n",
      "\n",
      "\n",
      "Top words in Cluster 8: prtpetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "\n",
      "\n",
      "Top words in Cluster 9: rsmpetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "\n",
      "\n",
      "Top words in Cluster 10: sts, petition, fully, scanned, action, initiated, receipt, hard, copy, enclosures\n",
      "\n",
      "\n",
      "Top words in Cluster 11: pg, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "\n",
      "\n",
      "Top words in Cluster 12: pg, petition, fully, scanned, action, initiated, receipt, hard, copy, enclosures\n",
      "\n",
      "\n",
      "Top words in Cluster 13: pg, mdpetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures\n",
      "\n",
      "\n",
      "Top words in Cluster 14: pg, bhipetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures\n",
      "\n",
      "\n",
      "Top words in Cluster 15: copy, sent, petition, fully, scanned, action, initiated, receipt, hard, enclosures\n",
      "\n",
      "\n",
      "Top words in Cluster 16: pg, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "\n",
      "\n",
      "Top words in Cluster 17: pg, akanshapetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures\n",
      "\n",
      "\n",
      "Top words in Cluster 18: pg, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "\n",
      "\n",
      "Top words in Cluster 19: pg, rjspetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures\n",
      "\n",
      "\n",
      "Top words in Cluster 20: bank, money, moratorium, kapole, hard, earned, request, kapol, possible, prime\n",
      "\n",
      "\n",
      "Top words in Cluster 21: rs, charges, moratorium, 2020, bank, refund, 19, period, covid, suffering\n",
      "\n",
      "\n",
      "Top words in Cluster 22: rs, charges, 2020, refund, period, bank, moratorium, suffering, 19, business\n",
      "\n",
      "\n",
      "Top words in Cluster 23: quot, sbi, 2020, branch, email, id, blocked, khagra, mother, 39\n",
      "\n",
      "\n",
      "Top words in Cluster 24: bank, transaction, debited, account, gmail, atm, kumar, withdrawal, money, service\n",
      "\n",
      "\n",
      "Top words in Cluster 25: bank, transaction, debited, account, gmail, aeps, withdrawal, money, service, server\n",
      "\n",
      "\n",
      "Top words in Cluster 26: bank, rrb, vacancies, baroda, sir, ibps, exam, process, bob, vacancy\n",
      "\n",
      "\n",
      "Top words in Cluster 27: oral, representation, received, details, applicant, pmo, 2021, nb, complaints, phone\n",
      "\n",
      "\n",
      "Top words in Cluster 28: account, bank, pm, jan, dhan, open, manager, woman, years, trying\n",
      "\n",
      "\n",
      "Top words in Cluster 29: dhfl, fd, holders, public, ibc, process, rbi, 160, resolution, nhb\n",
      "\n",
      "\n",
      "Top words in Cluster 30: bank, given, branch, proper, account, baroda, banking, solved, bob, complain\n",
      "\n",
      "\n",
      "Top words in Cluster 31: bank, pmc, money, rbi, pm, 39, people, issue, depositors, modi\n",
      "\n",
      "\n",
      "Top words in Cluster 32: kumar, bank, loan, problem, tonmay, kindly, matter, salp, aalo, sir\n",
      "\n",
      "\n",
      "Top words in Cluster 33: bank, loan, problem, salp, kumar, sir, tonmay, emi, hdfc, 2021\n",
      "\n",
      "\n",
      "Top words in Cluster 34: credit, rs, 000, card, situation, waive, ceremony, 50, jobless, kerala\n",
      "\n",
      "\n",
      "Top words in Cluster 35: loan, home, bank, charges, honest, rate, letter, idbi, account, floating\n",
      "\n",
      "\n",
      "Top words in Cluster 36: bank, idbi, loan, request, customer, rbi, mr, rajesh, money, pai\n",
      "\n",
      "\n",
      "Top words in Cluster 37: bank, rbi, customer, idbi, application, officer, property, charges, money, policy\n",
      "\n",
      "\n",
      "Top words in Cluster 38: bank, bipartite, 11th, salary, 2021, kind, notice, paid, 01, month\n",
      "\n",
      "\n",
      "Top words in Cluster 39: bank, salary, arrears, paid, 01, salaries, settlement, iba, board, 2021\n",
      "\n",
      "\n",
      "Top words in Cluster 40: bank, salary, salaries, help, settlement, paid, iba, arrears, sir, month\n",
      "\n",
      "\n",
      "Top words in Cluster 41: bank, salary, salaries, settlement, paid, help, month, arrears, bipartite, signed\n",
      "\n",
      "\n",
      "Top words in Cluster 42: hai, hia, paytm, account, ke, ki, se, maine, kiya, chala\n",
      "\n",
      "\n",
      "Top words in Cluster 43: bank, restructure, card, credit, dated, rbi, hdfcbank, issues, com, process\n",
      "\n",
      "\n",
      "Top words in Cluster 44: banks, bank, public, sector, privatisation, 39, sir, india, private, employees\n",
      "\n",
      "\n",
      "Top words in Cluster 45: loan, bank, pmegp, branch, manager, documents, application, sir, 2020, business\n",
      "\n",
      "\n",
      "Top words in Cluster 46: loan, bank, mudra, 160, business, sir, 39, documents, branch, help\n",
      "\n",
      "\n",
      "Top words in Cluster 47: card, sbi, sir, credit, rs, pay, 2020, payment, charges, request\n",
      "\n",
      "\n",
      "Top words in Cluster 48: hai, bank, number, mera, loan, account, kiya, mein, ke, ka\n",
      "\n",
      "\n",
      "Top words in Cluster 49: loan, bank, emi, pay, sir, account, rs, 39, time, paid\n",
      "\n",
      "\n",
      "Top words in Cluster 50: bank, rs, account, branch, 2020, 000, complaint, 00, number, atm\n",
      "\n",
      "\n",
      "Top words in Cluster 51: student, quot, bank, university, education, indian, lone, vidyalakshmi, dear, sir\n",
      "\n",
      "\n",
      "Top words in Cluster 52: sir, 2020, bhelupura, business, office, july, head, mobile, pmo, submitted\n",
      "\n",
      "\n",
      "Top words in Cluster 53: loan, education, sir, bank, branch, 39, student, nursing, application, able\n",
      "\n",
      "\n",
      "Top words in Cluster 54: loan, education, rs, bank, sir, 39, family, job, repay, father\n",
      "\n",
      "\n",
      "Top words in Cluster 55: bank, father, money, court, account, late, india, barrackpore, branch, punjab\n",
      "\n",
      "\n",
      "Top words in Cluster 56: bank, complaint, office, grievance, pmopg, 2020, 2021, issue, officer, number\n",
      "\n",
      "\n",
      "Top words in Cluster 57: sbi, 4834, grievance, bank, father, 2020, fraud, pmopg, bilaspur, cheating\n",
      "\n",
      "\n",
      "Top words in Cluster 58: sir, pension, account, sbi, delhi, cppc, fraud, chandni, chowk, statement\n",
      "\n",
      "\n",
      "Top words in Cluster 59: da, pension, sbi, account, oct, armed, wef, majesty, hav, almora\n",
      "\n",
      "\n",
      "Top words in Cluster 60: sbi, da, mr, armed, pension, 2020, guards, hav, account, cppc\n",
      "\n",
      "\n",
      "Top words in Cluster 61: pension, bank, family, branch, husband, late, 2020, sir, india, state\n",
      "\n",
      "\n",
      "Top words in Cluster 62: pension, banks, bank, retirees, updation, pensioners, iba, employees, psbs, dated\n",
      "\n",
      "\n",
      "Top words in Cluster 63: subsidy, hdfc, loan, pmay, following, finance, urban, complaint, housing, house\n",
      "\n",
      "\n",
      "Top words in Cluster 64: subsidy, pmay, loan, bank, application, home, sir, id, account, housing\n",
      "\n",
      "\n",
      "Top words in Cluster 65: ke, se, bank, ki, hai, ka, loan, mai, aur, sir\n",
      "\n",
      "\n",
      "Top words in Cluster 66: phalodi, bank, pnb, barnch, mai, hai, loan, aap, ka, kiya\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "\n",
    "# Apply HDBSCAN \n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True)\n",
    "clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# Filter the original data to include only those rows that were clustered\n",
    "clustered_indices = [i for i, label in enumerate(clusters) if label != -1]\n",
    "filtered_clustered_data = filtered_data.iloc[clustered_indices]\n",
    "\n",
    "# Assign cluster labels to the filtered clustered data\n",
    "filtered_clustered_data['Cluster'] = clusters[clusters != -1]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "for cluster_num in set(clusters) - {-1}:  # Exclude noise (-1)\n",
    "    # print(f\"Documents in Cluster {cluster_num}:\")\n",
    "    cluster_docs = filtered_clustered_data[filtered_clustered_data['Cluster'] == cluster_num]['description']\n",
    "    # print(cluster_docs.head())  # Display a few documents from the cluster\n",
    "    \n",
    "    # Get and print the top words for each cluster\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    print(f\"Topics in Cluster {cluster_num}: {', '.join([word for word, freq in top_words])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67610d52-21e2-4d2f-a813-c218c06caf81",
   "metadata": {},
   "source": [
    "## using umap and hdbscan on Mpnet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "904493d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in Cluster 0: industry, cashew, kerala, banks, workers, processing, non, request, taken, severe\n",
      "\n",
      "\n",
      "Top words in Cluster 1: bank, salary, salaries, paid, settlement, help, month, 2020, 01, arrears\n",
      "\n",
      "\n",
      "Top words in Cluster 2: bank, salary, arrears, paid, 01, settlement, iba, board, 2021, future\n",
      "\n",
      "\n",
      "Top words in Cluster 3: pg, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "\n",
      "\n",
      "Top words in Cluster 4: bank, salary, salaries, settlement, paid, help, iba, future, arrears, work\n",
      "\n",
      "\n",
      "Top words in Cluster 5: fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent, shortly\n",
      "\n",
      "\n",
      "Top words in Cluster 6: bank, salary, 01, salaries, paid, 2021, bipartite, settlement, 2020, india\n",
      "\n",
      "\n",
      "Top words in Cluster 7: bank, salary, bipartite, board, india, employees, settlement, management, post, payments\n",
      "\n",
      "\n",
      "Top words in Cluster 8: pg, rjspetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures\n",
      "\n",
      "\n",
      "Top words in Cluster 9: bank, january, ramjee, singh, kaladumra, ps, nagar, siwan, 2008, retired\n",
      "\n",
      "\n",
      "Top words in Cluster 10: bank, loan, problem, salp, kumar, sir, tonmay, emi, hdfc, 2021\n",
      "\n",
      "\n",
      "Top words in Cluster 11: copy, sent, petition, fully, scanned, action, initiated, receipt, hard, enclosures\n",
      "\n",
      "\n",
      "Top words in Cluster 12: pg, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "\n",
      "\n",
      "Top words in Cluster 13: rs, charges, 2020, refund, moratorium, bank, 19, period, covid, 08\n",
      "\n",
      "\n",
      "Top words in Cluster 14: credit, rs, 000, card, situation, bank, waive, ceremony, help, 50\n",
      "\n",
      "\n",
      "Top words in Cluster 15: pg, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "\n",
      "\n",
      "Top words in Cluster 16: pg, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "\n",
      "\n",
      "Top words in Cluster 17: kumar, bank, loan, tonmay, kindly, problem, matter, salp, sir, aalo\n",
      "\n",
      "\n",
      "Top words in Cluster 18: loan, subsidy, bank, pmay, home, sir, application, 39, housing, scheme\n",
      "\n",
      "\n",
      "Top words in Cluster 19: bank, transaction, account, aeps, gmail, debited, withdrawal, number, date, address\n",
      "\n",
      "\n",
      "Top words in Cluster 20: dhfl, fd, holders, public, ibc, process, rbi, resolution, money, 160\n",
      "\n",
      "\n",
      "Top words in Cluster 21: hai, bank, number, mera, loan, account, kiya, mein, ke, ka\n",
      "\n",
      "\n",
      "Top words in Cluster 22: oral, representation, received, details, applicant, 2021, pmo, complaints, nb, phone\n",
      "\n",
      "\n",
      "Top words in Cluster 23: phalodi, bank, pnb, barnch, hai, mai, loan, ka, aap, kiya\n",
      "\n",
      "\n",
      "Top words in Cluster 24: ews, certificate, sbi, notification, 2019, 2020, 20, fy, exam, valid\n",
      "\n",
      "\n",
      "Top words in Cluster 25: bank, job, sir, ki, banks, government, private, 39, ko, india\n",
      "\n",
      "\n",
      "Top words in Cluster 26: banks, bank, public, sector, 39, india, privatisation, sir, government, private\n",
      "\n",
      "\n",
      "Top words in Cluster 27: bank, pmc, money, rbi, depositors, sir, 39, hard, earned, people\n",
      "\n",
      "\n",
      "Top words in Cluster 28: bank, money, kapol, hard, request, moratorium, account, earned, op, kapole\n",
      "\n",
      "\n",
      "Top words in Cluster 29: idfc, bonds, bond, bank, 2021, account, maturity, sir, address, money\n",
      "\n",
      "\n",
      "Top words in Cluster 30: maturity, release, bhatia, non, proceeds, folio, ca, bank, sameer, jatinder\n",
      "\n",
      "\n",
      "Top words in Cluster 31: csp, hai, bank, account, se, sbi, sir, ke, mera, number\n",
      "\n",
      "\n",
      "Top words in Cluster 32: ibps, exam, sir, bank, rrb, interview, vacancies, students, bob, baroda\n",
      "\n",
      "\n",
      "Top words in Cluster 33: 160, interview, 39, sir, bank, recruitment, advertisement, request, category, candidate\n",
      "\n",
      "\n",
      "Top words in Cluster 34: 160, hai, sir, se, ki, ke, bank, loan, ko, ka\n",
      "\n",
      "\n",
      "Top words in Cluster 35: hai, ki, se, bank, ke, sir, ka, loan, ko, nahi\n",
      "\n",
      "\n",
      "Top words in Cluster 36: bank, account, ppf, pf, branch, 2020, sir, kotak, transfer, sbi\n",
      "\n",
      "\n",
      "Top words in Cluster 37: bank, tds, rs, deducted, pan, deposit, tax, branch, 2020, sir\n",
      "\n",
      "\n",
      "Top words in Cluster 38: bank, account, father, branch, money, late, sir, death, india, nominee\n",
      "\n",
      "\n",
      "Top words in Cluster 39: bank, policy, insurance, 160, claim, 39, sir, account, life, icici\n",
      "\n",
      "\n",
      "Top words in Cluster 40: sir, sbi, pension, account, delhi, cppc, fraud, chandni, chowk, statement\n",
      "\n",
      "\n",
      "Top words in Cluster 41: pension, bank, banks, pensioners, employees, retirees, years, updation, rbi, iba\n",
      "\n",
      "\n",
      "Top words in Cluster 42: senior, citizens, income, citizen, rate, 39, request, 15, india, pension\n",
      "\n",
      "\n",
      "Top words in Cluster 43: fastag, sbi, toll, bank, account, kyc, sir, customer, rs, 2021\n",
      "\n",
      "\n",
      "Top words in Cluster 44: bank, axis, account, card, sir, rs, customer, branch, debit, credit\n",
      "\n",
      "\n",
      "Top words in Cluster 45: da, sbi, pension, armed, pay, guards, account, mr, stopped, hav\n",
      "\n",
      "\n",
      "Top words in Cluster 46: da, pension, sbi, account, oct, wef, majesty, request, 2013, hav\n",
      "\n",
      "\n",
      "Top words in Cluster 47: pension, bank, branch, family, 2020, account, sbi, sir, ppo, office\n",
      "\n",
      "\n",
      "Top words in Cluster 48: banks, loan, bank, people, india, loans, 39, money, rs, apps\n",
      "\n",
      "\n",
      "Top words in Cluster 49: bank, 160, branch, village, town, people, banks, minister, request, open\n",
      "\n",
      "\n",
      "Top words in Cluster 50: notes, currency, india, coins, rs, coin, 39, bank, note, silver\n",
      "\n",
      "\n",
      "Top words in Cluster 51: loan, bank, sir, branch, sbi, land, account, 39, 2020, taken\n",
      "\n",
      "\n",
      "Top words in Cluster 52: bank, loan, kcc, sir, branch, kisan, pm, credit, card, 39\n",
      "\n",
      "\n",
      "Top words in Cluster 53: cibil, loan, bank, sir, score, credit, report, 160, branch, loans\n",
      "\n",
      "\n",
      "Top words in Cluster 54: paytm, 160, bank, account, rs, money, kyc, complaint, card, sir\n",
      "\n",
      "\n",
      "Top words in Cluster 55: hai, paytm, ke, se, ki, account, hia, maine, kiya, bank\n",
      "\n",
      "\n",
      "Top words in Cluster 56: bank, card, credit, restructure, dated, rbi, hdfcbank, issues, hdfc, complaint\n",
      "\n",
      "\n",
      "Top words in Cluster 57: bank, scheme, loan, 2020, rs, gecl, india, credit, eclgs, 20\n",
      "\n",
      "\n",
      "Top words in Cluster 58: 160, bank, india, permanent, state, branch, sir, 39, staff, work\n",
      "\n",
      "\n",
      "Top words in Cluster 59: transfer, bank, sir, branch, request, 39, place, working, work, daughter\n",
      "\n",
      "\n",
      "Top words in Cluster 60: bank, sir, family, job, son, request, india, father, 39, help\n",
      "\n",
      "\n",
      "Top words in Cluster 61: loan, 39, cibil, bank, 2020, score, mudra, branch, sir, credit\n",
      "\n",
      "\n",
      "Top words in Cluster 62: bank, hdfc, account, card, credit, rs, 39, number, sir, pay\n",
      "\n",
      "\n",
      "Top words in Cluster 63: bank, customers, india, account, banks, merger, online, union, corporation, branch\n",
      "\n",
      "\n",
      "Top words in Cluster 64: bank, baroda, branch, banking, account, dena, bob, given, proper, problem\n",
      "\n",
      "\n",
      "Top words in Cluster 65: card, credit, bank, rbl, sir, rs, payment, charges, 160, pay\n",
      "\n",
      "\n",
      "Top words in Cluster 66: card, sbi, credit, sir, rs, bank, charges, payment, 39, 2020\n",
      "\n",
      "\n",
      "Top words in Cluster 67: loan, gold, bank, pay, sir, finance, request, time, account, auction\n",
      "\n",
      "\n",
      "Top words in Cluster 68: bank, loan, sir, minister, 39, prime, help, india, request, land\n",
      "\n",
      "\n",
      "Top words in Cluster 69: bank, india, 39, prime, sir, case, minister, action, rs, kindly\n",
      "\n",
      "\n",
      "Top words in Cluster 70: bank, board, directors, patna, number, account, icici, criminal, india, informed\n",
      "\n",
      "\n",
      "Top words in Cluster 71: loan, 160, bank, sir, education, 39, pay, help, job, family\n",
      "\n",
      "\n",
      "Top words in Cluster 72: loan, bank, education, sir, 39, branch, help, applied, student, application\n",
      "\n",
      "\n",
      "Top words in Cluster 73: bank, kotak, loan, emi, sir, pay, request, rs, help, people\n",
      "\n",
      "\n",
      "Top words in Cluster 74: bank, branch, account, patna, atm, indian, main, india, 2021, 160\n",
      "\n",
      "\n",
      "Top words in Cluster 75: sbi, grievance, bank, quot, 2020, pmopg, 4834, father, india, accounts\n",
      "\n",
      "\n",
      "Top words in Cluster 76: bank, loan, 39, job, sir, help, pay, know, don, modiji\n",
      "\n",
      "\n",
      "Top words in Cluster 77: loan, bank, pay, sir, 39, house, time, money, father, property\n",
      "\n",
      "\n",
      "Top words in Cluster 78: bank, cash, digital, banks, payment, charges, card, india, transaction, money\n",
      "\n",
      "\n",
      "Top words in Cluster 79: bank, address, banks, card, account, proof, customer, india, people, kyc\n",
      "\n",
      "\n",
      "Top words in Cluster 80: loan, bank, 39, sir, family, pay, prime, minister, help, request\n",
      "\n",
      "\n",
      "Top words in Cluster 81: loan, bank, 39, sir, pay, house, help, rs, kindly, home\n",
      "\n",
      "\n",
      "Top words in Cluster 82: sbi, 2021, bank, sir, prime, minister, dated, india, kind, 2014\n",
      "\n",
      "\n",
      "Top words in Cluster 83: loan, bank, msme, project, business, sir, banks, govt, help, india\n",
      "\n",
      "\n",
      "Top words in Cluster 84: bank, loan, pmegp, branch, 160, sir, manager, 2020, application, scheme\n",
      "\n",
      "\n",
      "Top words in Cluster 85: google, pay, account, transaction, se, sir, rs, bank, ke, ka\n",
      "\n",
      "\n",
      "Top words in Cluster 86: bank, 39, complaints, complaint, justice, evidences, authorities, prove, sir, corruption\n",
      "\n",
      "\n",
      "Top words in Cluster 87: sir, loan, bank, help, family, financial, rs, pay, want, time\n",
      "\n",
      "\n",
      "Top words in Cluster 88: bank, manager, branch, loan, mr, complaint, account, applicant, nabard, india\n",
      "\n",
      "\n",
      "Top words in Cluster 89: bank, loan, 2020, rs, account, dated, manager, complaint, mr, property\n",
      "\n",
      "\n",
      "Top words in Cluster 90: loan, business, sir, bank, help, 39, india, small, financial, need\n",
      "\n",
      "\n",
      "Top words in Cluster 91: loan, bank, mudra, sir, business, branch, manager, 160, help, 39\n",
      "\n",
      "\n",
      "Top words in Cluster 92: bank, sir, card, pay, credit, help, loan, family, time, dues\n",
      "\n",
      "\n",
      "Top words in Cluster 93: bank, rs, loan, 2020, property, ubi, 2021, letter, land, sir\n",
      "\n",
      "\n",
      "Top words in Cluster 94: loan, bank, hdfc, 39, rs, home, property, sir, account, branch\n",
      "\n",
      "\n",
      "Top words in Cluster 95: bank, loan, axis, rs, emi, people, account, sir, home, time\n",
      "\n",
      "\n",
      "Top words in Cluster 96: bank, cheque, book, account, branch, sir, new, number, savings, old\n",
      "\n",
      "\n",
      "Top words in Cluster 97: bank, canara, account, branch, customer, banking, sir, rs, road, action\n",
      "\n",
      "\n",
      "Top words in Cluster 98: sir, 2020, office, bhelupura, business, head, branch, bank, pmo, july\n",
      "\n",
      "\n",
      "Top words in Cluster 99: bank, branch, manager, account, 2020, letter, complaint, sir, mr, money\n",
      "\n",
      "\n",
      "Top words in Cluster 100: grievance, bank, 2020, pmopg, advise, number, concerns, pnb, matter, issues\n",
      "\n",
      "\n",
      "Top words in Cluster 101: grievance, bank, pmopg, office, closed, case, 2020, registration, complaint, number\n",
      "\n",
      "\n",
      "Top words in Cluster 102: bank, office, complaint, ombudsman, banking, bangalore, rbi, 2021, bo, icici\n",
      "\n",
      "\n",
      "Top words in Cluster 103: bank, passbook, 160, branch, 39, account, new, customer, sir, issue\n",
      "\n",
      "\n",
      "Top words in Cluster 104: bank, account, branch, pnb, rs, sir, national, punjab, card, number\n",
      "\n",
      "\n",
      "Top words in Cluster 105: bank, account, branch, rs, cheque, 2021, 2020, sms, charges, coins\n",
      "\n",
      "\n",
      "Top words in Cluster 106: branch, sbi, bank, account, manager, sir, 39, service, india, customer\n",
      "\n",
      "\n",
      "Top words in Cluster 107: bank, account, money, rs, sir, complaint, 39, fraud, number, help\n",
      "\n",
      "\n",
      "Top words in Cluster 108: loan, 39, pay, sir, quot, bank, dhani, paid, financial, like\n",
      "\n",
      "\n",
      "Top words in Cluster 109: loan, bank, foreclosure, charges, request, letter, rs, sir, account, india\n",
      "\n",
      "\n",
      "Top words in Cluster 110: loan, bank, rate, home, rbi, time, 39, branch, sir, roi\n",
      "\n",
      "\n",
      "Top words in Cluster 111: bank, money, idbi, loan, request, rbi, application, mr, customer, rajesh\n",
      "\n",
      "\n",
      "Top words in Cluster 112: loan, bank, home, idbi, rate, honest, charges, account, letter, rbi\n",
      "\n",
      "\n",
      "Top words in Cluster 113: loan, bajaj, emi, rs, account, sir, 39, finance, bank, paid\n",
      "\n",
      "\n",
      "Top words in Cluster 114: emi, bank, loan, account, charges, rs, 2020, pay, sir, 39\n",
      "\n",
      "\n",
      "Top words in Cluster 115: bank, account, neft, charges, transaction, baroda, rs, rtgs, branch, like\n",
      "\n",
      "\n",
      "Top words in Cluster 116: emi, loan, bank, months, paid, sir, moratorium, pay, rs, emis\n",
      "\n",
      "\n",
      "Top words in Cluster 117: emi, loan, sir, pay, bank, help, 39, job, time, month\n",
      "\n",
      "\n",
      "Top words in Cluster 118: bank, account, atm, money, rs, branch, 39, sbi, 2020, transaction\n",
      "\n",
      "\n",
      "Top words in Cluster 119: loan, restructuring, bank, emi, 2020, sir, home, moratorium, 39, restructure\n",
      "\n",
      "\n",
      "Top words in Cluster 120: sir, emi, loan, bank, pay, time, moratorium, 39, business, prime\n",
      "\n",
      "\n",
      "Top words in Cluster 121: bhim, app, 39, account, sir, mobile, bank, transaction, pay, india\n",
      "\n",
      "\n",
      "Top words in Cluster 122: payment, upi, transaction, account, bank, 2021, rs, money, sbi, rti\n",
      "\n",
      "\n",
      "Top words in Cluster 123: sbi, bank, account, rs, branch, money, transaction, complaint, 2021, sir\n",
      "\n",
      "\n",
      "Top words in Cluster 124: bank, account, rs, india, atm, complaint, number, state, branch, cheque\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size= 10 ,\n",
    "                                                              metric='euclidean',\n",
    "                                                              cluster_selection_method='eom',\n",
    "                                                              prediction_data=True)\n",
    "clusters = clusterer.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Filter the original data to include only those rows that were clustered\n",
    "clustered_indices = [i for i, label in enumerate(clusters) if label != -1]\n",
    "filtered_clustered_data = filtered_data.iloc[clustered_indices]\n",
    "\n",
    "# Assign cluster labels to the filtered clustered data\n",
    "filtered_clustered_data['Cluster'] = clusters[clusters != -1]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "for cluster_num in set(clusters) - {-1}:  # Exclude noise (-1)\n",
    "    # print(f\"Documents in Cluster {cluster_num}:\")\n",
    "    cluster_docs = filtered_clustered_data[filtered_clustered_data['Cluster'] == cluster_num]['description']\n",
    "    # print(cluster_docs.head())  # Display a few documents from the cluster\n",
    "    \n",
    "    # Get and print the top words for each cluster\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    print(f\"Top words in Cluster {cluster_num}: {', '.join([word for word, freq in top_words])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6712c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (Documents: 33) Topics: scanned, enclosures, petition, shortly, pg, fully, receipt, copy, sent, initiated\n",
      "Cluster 1 (Documents: 44) Topics: scanned, enclosures, shortly, pg, fully, copy, receipt, sent, hard, initiated\n",
      "Cluster 2 (Documents: 34) Topics: cashew, industry, workers, kerala, entrepreneurs, physical, factories, dwelling, coercive, collaterals\n",
      "Cluster 3 (Documents: 24) Topics: rs, charges, refund, moratorium, 156, period, 2020, 08, gesture, suffering\n",
      "Cluster 4 (Documents: 15) Topics: rrb, vacancies, baroda, ibps, withdraw, bob, exam, bank, examination, students\n",
      "Cluster 5 (Documents: 16) Topics: oral, representation, applicant, details, received, gap, nb, complaints, नह, follows\n",
      "Cluster 6 (Documents: 28) Topics: dhfl, fd, holders, ibc, resolution, rbi, public, lenders, 160, nhb\n",
      "Cluster 7 (Documents: 74) Topics: banks, privatisation, public, bank, sector, private, psbs, privatization, psu, employees\n",
      "Cluster 8 (Documents: 21) Topics: loan, salp, tonmay, bank, problem, kumar, aalo, manna, handle, donate\n",
      "Cluster 9 (Documents: 47) Topics: idbi, loan, money, rbi, bank, charges, application, rajesh, policy, home\n",
      "Cluster 10 (Documents: 22) Topics: salary, bank, arrears, salaries, inhuman, iba, board, settlement, paid, 01\n",
      "Cluster 11 (Documents: 36) Topics: salaries, bank, salary, settlement, iba, bipartite, unions, abruptly, arrears, revision\n",
      "Cluster 12 (Documents: 62) Topics: salaries, bank, salary, settlement, abruptly, unions, bipartite, arrears, revision, paid\n",
      "Cluster 13 (Documents: 18) Topics: mudra, loan, business, bank, sir, quot, start, manager, documents, 39\n",
      "Cluster 14 (Documents: 146) Topics: pension, da, sbi, bank, account, cppc, branch, armed, office, delhi\n",
      "Cluster 15 (Documents: 49) Topics: loan, education, nursing, vidyalakshmi, student, branch, able, manager, bank, 39\n",
      "Cluster 16 (Documents: 120) Topics: hai, ka, mai, se, ke, phalodi, ki, loan, barnch, bank\n",
      "Cluster 17 (Documents: 74) Topics: subsidy, pmay, loan, bank, application, home, id, sir, applied, account\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_top_words_per_cluster(documents_by_cluster, n_top_words=10):\n",
    "    # Check if documents_by_cluster contains any valid text\n",
    "    if not any(len(doc.strip()) > 0 for doc in documents_by_cluster):\n",
    "        return [[] for _ in documents_by_cluster]\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents_by_cluster)\n",
    "    \n",
    "    top_words_per_cluster = []\n",
    "    for cluster_idx in range(tfidf_matrix.shape[0]):\n",
    "        feature_array = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "        tfidf_scores = tfidf_matrix[cluster_idx].toarray().flatten()\n",
    "        top_indices = tfidf_scores.argsort()[-n_top_words:][::-1]\n",
    "        top_words = feature_array[top_indices]\n",
    "        top_scores = tfidf_scores[top_indices]\n",
    "        top_words_per_cluster.append(list(zip(top_words, top_scores)))\n",
    "    \n",
    "    return top_words_per_cluster\n",
    "\n",
    "\n",
    "def recursive_clustering(level, data, embeddings, min_cluster_size=15, min_samples=None, results=None):\n",
    "    if results is None:\n",
    "        results = {}\n",
    "\n",
    "    clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, prediction_data=True)\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    unique_clusters = set(clusters) - {-1}  # Exclude noise if present\n",
    "\n",
    "    # Prepare documents by cluster for class-based TF-IDF\n",
    "    documents_by_cluster = [' '.join(data.iloc[clusters == cluster_id]['description'].tolist()) for cluster_id in unique_clusters]\n",
    "    top_words_per_cluster = get_top_words_per_cluster(documents_by_cluster)\n",
    "\n",
    "    for i, cluster_id in enumerate(unique_clusters):\n",
    "        cluster_label = f\"{level}{cluster_id}\" if level else str(cluster_id)\n",
    "        cluster_mask = clusters == cluster_id\n",
    "        cluster_data = data.iloc[cluster_mask]\n",
    "\n",
    "        # Store top words, their frequencies, and the number of documents in the cluster\n",
    "        results[cluster_label] = {\n",
    "            'top_words': [word for word, _ in top_words_per_cluster[i]],\n",
    "            'no_of_documents': len(cluster_data)\n",
    "        }\n",
    "\n",
    "        # Recursive call if the cluster is large enough\n",
    "        if len(cluster_data) > 100:\n",
    "            new_level = f\"{cluster_label}.\"\n",
    "            recursive_clustering(new_level, cluster_data, embeddings[cluster_mask], min_cluster_size, min_samples, results)\n",
    "\n",
    "    return results\n",
    "\n",
    "hierarchical_topics = recursive_clustering(\"\", filtered_data, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def display_topics(hierarchical_topics, level=0):\n",
    "    for cluster_label, info in hierarchical_topics.items():\n",
    "        indent = '  ' * level\n",
    "        top_words = ', '.join(info['top_words'])\n",
    "        no_of_documents = info['no_of_documents']\n",
    "        print(f\"{indent}Cluster {cluster_label} (Documents: {no_of_documents}) Topics: {top_words}\")\n",
    "\n",
    "display_topics(hierarchical_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d17f45-d96d-44b1-9d33-f6ebc0297061",
   "metadata": {},
   "source": [
    "## using hdbscan on Mpnet embeddings. Merge similar topics using connected components in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be4aebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-merged Cluster 0 Topics: india, crypto, cryptocurrencies, 39, digital, currency, cryptocurrency, government, bitcoin, like\n",
      "Non-merged Cluster 1 Topics: ews, certificate, sbi, notification, 2019, 20, fy, valid, 2020, category\n",
      "Non-merged Cluster 2 Topics: bank, account, india, minor, years, age, state, opened, paheli, child\n",
      "Non-merged Cluster 3 Topics: industry, cashew, kerala, workers, banks, non, severe, crisis, years, processing\n",
      "Non-merged Cluster 4 Topics: idfc, bank, 2021, 21, bonds, maturity, money, matter, courier, date\n",
      "Non-merged Cluster 5 Topics: release, bhatia, non, maturity, proceeds, folio, ca, sameer, jatinder, pal\n",
      "Merged Clusters [6, 7] Topics (from one of them): bank, january, ramjee, singh, kaladumra, ps, nagar, siwan, 2008, retired\n",
      "Merged Clusters [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] Topics (from one of them): prtpetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "Non-merged Cluster 20 Topics: bank, money, moratorium, kapole, hard, earned, request, kapol, possible, prime\n",
      "Merged Clusters [21, 22] Topics (from one of them): rs, charges, moratorium, 2020, bank, refund, 19, period, covid, suffering\n",
      "Non-merged Cluster 23 Topics: quot, sbi, 2020, branch, email, id, blocked, khagra, mother, 39\n",
      "Merged Clusters [24, 25] Topics (from one of them): bank, transaction, debited, account, gmail, atm, kumar, withdrawal, money, service\n",
      "Non-merged Cluster 26 Topics: bank, rrb, vacancies, baroda, sir, ibps, exam, process, bob, vacancy\n",
      "Non-merged Cluster 27 Topics: oral, representation, received, details, applicant, pmo, 2021, nb, complaints, phone\n",
      "Non-merged Cluster 28 Topics: account, bank, pm, jan, dhan, open, manager, woman, years, trying\n",
      "Non-merged Cluster 29 Topics: dhfl, fd, holders, public, ibc, process, rbi, 160, resolution, nhb\n",
      "Non-merged Cluster 30 Topics: bank, given, branch, proper, account, baroda, banking, solved, bob, complain\n",
      "Non-merged Cluster 31 Topics: bank, pmc, money, rbi, pm, 39, people, issue, depositors, modi\n",
      "Merged Clusters [32, 33] Topics (from one of them): kumar, bank, loan, problem, tonmay, kindly, matter, salp, aalo, sir\n",
      "Non-merged Cluster 34 Topics: credit, rs, 000, card, situation, waive, ceremony, 50, jobless, kerala\n",
      "Non-merged Cluster 35 Topics: loan, home, bank, charges, honest, rate, letter, idbi, account, floating\n",
      "Non-merged Cluster 36 Topics: bank, idbi, loan, request, customer, rbi, mr, rajesh, money, pai\n",
      "Non-merged Cluster 37 Topics: bank, rbi, customer, idbi, application, officer, property, charges, money, policy\n",
      "Non-merged Cluster 38 Topics: bank, bipartite, 11th, salary, 2021, kind, notice, paid, 01, month\n",
      "Merged Clusters [39, 40, 41] Topics (from one of them): bank, salary, arrears, paid, 01, salaries, settlement, iba, board, 2021\n",
      "Non-merged Cluster 42 Topics: hai, hia, paytm, account, ke, ki, se, maine, kiya, chala\n",
      "Non-merged Cluster 43 Topics: bank, restructure, card, credit, dated, rbi, hdfcbank, issues, com, process\n",
      "Non-merged Cluster 44 Topics: banks, bank, public, sector, privatisation, 39, sir, india, private, employees\n",
      "Non-merged Cluster 45 Topics: loan, bank, pmegp, branch, manager, documents, application, sir, 2020, business\n",
      "Non-merged Cluster 46 Topics: loan, bank, mudra, 160, business, sir, 39, documents, branch, help\n",
      "Non-merged Cluster 47 Topics: card, sbi, sir, credit, rs, pay, 2020, payment, charges, request\n",
      "Non-merged Cluster 48 Topics: hai, bank, number, mera, loan, account, kiya, mein, ke, ka\n",
      "Non-merged Cluster 49 Topics: loan, bank, emi, pay, sir, account, rs, 39, time, paid\n",
      "Non-merged Cluster 50 Topics: bank, rs, account, branch, 2020, 000, complaint, 00, number, atm\n",
      "Non-merged Cluster 51 Topics: student, quot, bank, university, education, indian, lone, vidyalakshmi, dear, sir\n",
      "Non-merged Cluster 52 Topics: sir, 2020, bhelupura, business, office, july, head, mobile, pmo, submitted\n",
      "Non-merged Cluster 53 Topics: loan, education, sir, bank, branch, 39, student, nursing, application, able\n",
      "Non-merged Cluster 54 Topics: loan, education, rs, bank, sir, 39, family, job, repay, father\n",
      "Non-merged Cluster 55 Topics: bank, father, money, court, account, late, india, barrackpore, branch, punjab\n",
      "Non-merged Cluster 56 Topics: bank, complaint, office, grievance, pmopg, 2020, 2021, issue, officer, number\n",
      "Non-merged Cluster 57 Topics: sbi, 4834, grievance, bank, father, 2020, fraud, pmopg, bilaspur, cheating\n",
      "Non-merged Cluster 58 Topics: sir, pension, account, sbi, delhi, cppc, fraud, chandni, chowk, statement\n",
      "Non-merged Cluster 59 Topics: da, pension, sbi, account, oct, armed, wef, majesty, hav, almora\n",
      "Non-merged Cluster 60 Topics: sbi, da, mr, armed, pension, 2020, guards, hav, account, cppc\n",
      "Non-merged Cluster 61 Topics: pension, bank, family, branch, husband, late, 2020, sir, india, state\n",
      "Non-merged Cluster 62 Topics: pension, banks, bank, retirees, updation, pensioners, iba, employees, psbs, dated\n",
      "Non-merged Cluster 63 Topics: subsidy, hdfc, loan, pmay, following, finance, urban, complaint, housing, house\n",
      "Non-merged Cluster 64 Topics: subsidy, pmay, loan, bank, application, home, sir, id, account, housing\n",
      "Non-merged Cluster 65 Topics: ke, se, bank, ki, hai, ka, loan, mai, aur, sir\n",
      "Non-merged Cluster 66 Topics: phalodi, bank, pnb, barnch, mai, hai, loan, aap, ka, kiya\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# Apply HDBSCAN to the obtained embeddings\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True)\n",
    "clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# Filter the original data to include only those rows that were clustered\n",
    "clustered_indices = [i for i, label in enumerate(clusters) if label != -1]\n",
    "filtered_clustered_data = filtered_data.iloc[clustered_indices]\n",
    "filtered_clustered_data['Cluster'] = clusters[clusters != -1]\n",
    "\n",
    "# Define function to get top words\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "# Extract top words for each cluster\n",
    "cluster_top_words = []\n",
    "for cluster_num in set(clusters) - {-1}:\n",
    "    cluster_docs = filtered_clustered_data[filtered_clustered_data['Cluster'] == cluster_num]['description']\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    top_words_str = ', '.join([word for word, freq in top_words])\n",
    "    cluster_top_words.append(top_words_str)\n",
    "\n",
    "# Generate TF-IDF matrix for top words of each cluster\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_top_words)\n",
    "\n",
    "# Compute cosine similarity between cluster top words\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Identify clusters to merge by finding connected components in the similarity graph\n",
    "similarity_threshold = 0.6\n",
    "graph = cosine_sim_matrix > similarity_threshold\n",
    "n_components, labels = connected_components(csgraph=csr_matrix(graph), directed=False)\n",
    "\n",
    "# Display top words from merged clusters and remaining non-merged clusters\n",
    "for component in range(n_components):\n",
    "    # Find indices of clusters in the current component\n",
    "    indices = np.where(labels == component)[0]\n",
    "    if len(indices) == 1:\n",
    "        # Non-merged cluster\n",
    "        print(f\"Non-merged Cluster {indices[0]} Topics: {cluster_top_words[indices[0]]}\")\n",
    "    else:\n",
    "        # Merged clusters - displaying top words from one of the merging clusters arbitrarily\n",
    "        print(f\"Merged Clusters {indices.tolist()} Topics (from one of them): {cluster_top_words[indices[0]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "907b926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-merged Cluster 0 Topics: india, crypto, cryptocurrencies, 39, digital, currency, cryptocurrency, government, bitcoin, like\n",
      "Non-merged Cluster 1 Topics: ews, certificate, sbi, notification, 2019, 20, fy, valid, 2020, category\n",
      "Non-merged Cluster 2 Topics: bank, account, india, minor, years, age, state, opened, paheli, child\n",
      "Non-merged Cluster 3 Topics: industry, cashew, kerala, workers, banks, non, severe, crisis, years, processing\n",
      "Non-merged Cluster 4 Topics: idfc, bank, 2021, 21, bonds, maturity, money, matter, courier, date\n",
      "Non-merged Cluster 5 Topics: release, bhatia, non, maturity, proceeds, folio, ca, sameer, jatinder, pal\n",
      "Merged Clusters [6, 7] Topics (from one of them): bank, january, ramjee, singh, kaladumra, ps, nagar, siwan, 2008, retired\n",
      "Merged Clusters [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] Topics (from one of them): prtpetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent\n",
      "Non-merged Cluster 20 Topics: bank, money, moratorium, kapole, hard, earned, request, kapol, possible, prime\n",
      "Merged Clusters [21, 22] Topics (from one of them): rs, charges, moratorium, 2020, bank, refund, 19, period, covid, suffering\n",
      "Non-merged Cluster 23 Topics: quot, sbi, 2020, branch, email, id, blocked, khagra, mother, 39\n",
      "Merged Clusters [24, 25] Topics (from one of them): bank, transaction, debited, account, gmail, atm, kumar, withdrawal, money, service\n",
      "Non-merged Cluster 26 Topics: bank, rrb, vacancies, baroda, sir, ibps, exam, process, bob, vacancy\n",
      "Non-merged Cluster 27 Topics: oral, representation, received, details, applicant, pmo, 2021, nb, complaints, phone\n",
      "Non-merged Cluster 28 Topics: account, bank, pm, jan, dhan, open, manager, woman, years, trying\n",
      "Non-merged Cluster 29 Topics: dhfl, fd, holders, public, ibc, process, rbi, 160, resolution, nhb\n",
      "Non-merged Cluster 30 Topics: bank, given, branch, proper, account, baroda, banking, solved, bob, complain\n",
      "Non-merged Cluster 31 Topics: bank, pmc, money, rbi, pm, 39, people, issue, depositors, modi\n",
      "Merged Clusters [32, 33] Topics (from one of them): kumar, bank, loan, problem, tonmay, kindly, matter, salp, aalo, sir\n",
      "Non-merged Cluster 34 Topics: credit, rs, 000, card, situation, waive, ceremony, 50, jobless, kerala\n",
      "Non-merged Cluster 35 Topics: loan, home, bank, charges, honest, rate, letter, idbi, account, floating\n",
      "Non-merged Cluster 36 Topics: bank, idbi, loan, request, customer, rbi, mr, rajesh, money, pai\n",
      "Non-merged Cluster 37 Topics: bank, rbi, customer, idbi, application, officer, property, charges, money, policy\n",
      "Non-merged Cluster 38 Topics: bank, bipartite, 11th, salary, 2021, kind, notice, paid, 01, month\n",
      "Merged Clusters [39, 40, 41] Topics (from one of them): bank, salary, arrears, paid, 01, salaries, settlement, iba, board, 2021\n",
      "Non-merged Cluster 42 Topics: hai, hia, paytm, account, ke, ki, se, maine, kiya, chala\n",
      "Non-merged Cluster 43 Topics: bank, restructure, card, credit, dated, rbi, hdfcbank, issues, com, process\n",
      "Non-merged Cluster 44 Topics: banks, bank, public, sector, privatisation, 39, sir, india, private, employees\n",
      "Non-merged Cluster 45 Topics: loan, bank, pmegp, branch, manager, documents, application, sir, 2020, business\n",
      "Non-merged Cluster 46 Topics: loan, bank, mudra, 160, business, sir, 39, documents, branch, help\n",
      "Non-merged Cluster 47 Topics: card, sbi, sir, credit, rs, pay, 2020, payment, charges, request\n",
      "Non-merged Cluster 48 Topics: hai, bank, number, mera, loan, account, kiya, mein, ke, ka\n",
      "Non-merged Cluster 49 Topics: loan, bank, emi, pay, sir, account, rs, 39, time, paid\n",
      "Non-merged Cluster 50 Topics: bank, rs, account, branch, 2020, 000, complaint, 00, number, atm\n",
      "Non-merged Cluster 51 Topics: student, quot, bank, university, education, indian, lone, vidyalakshmi, dear, sir\n",
      "Non-merged Cluster 52 Topics: sir, 2020, bhelupura, business, office, july, head, mobile, pmo, submitted\n",
      "Non-merged Cluster 53 Topics: loan, education, sir, bank, branch, 39, student, nursing, application, able\n",
      "Non-merged Cluster 54 Topics: loan, education, rs, bank, sir, 39, family, job, repay, father\n",
      "Non-merged Cluster 55 Topics: bank, father, money, court, account, late, india, barrackpore, branch, punjab\n",
      "Non-merged Cluster 56 Topics: bank, complaint, office, grievance, pmopg, 2020, 2021, issue, officer, number\n",
      "Non-merged Cluster 57 Topics: sbi, 4834, grievance, bank, father, 2020, fraud, pmopg, bilaspur, cheating\n",
      "Non-merged Cluster 58 Topics: sir, pension, account, sbi, delhi, cppc, fraud, chandni, chowk, statement\n",
      "Non-merged Cluster 59 Topics: da, pension, sbi, account, oct, armed, wef, majesty, hav, almora\n",
      "Non-merged Cluster 60 Topics: sbi, da, mr, armed, pension, 2020, guards, hav, account, cppc\n",
      "Non-merged Cluster 61 Topics: pension, bank, family, branch, husband, late, 2020, sir, india, state\n",
      "Non-merged Cluster 62 Topics: pension, banks, bank, retirees, updation, pensioners, iba, employees, psbs, dated\n",
      "Non-merged Cluster 63 Topics: subsidy, hdfc, loan, pmay, following, finance, urban, complaint, housing, house\n",
      "Non-merged Cluster 64 Topics: subsidy, pmay, loan, bank, application, home, sir, id, account, housing\n",
      "Non-merged Cluster 65 Topics: ke, se, bank, ki, hai, ka, loan, mai, aur, sir\n",
      "Non-merged Cluster 66 Topics: phalodi, bank, pnb, barnch, mai, hai, loan, aap, ka, kiya\n",
      "TIME:  252.60124492645264\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "# Apply HDBSCAN to the obtained embeddings\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True)\n",
    "clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# Filter the original data to include only those rows that were clustered\n",
    "clustered_indices = [i for i, label in enumerate(clusters) if label != -1]\n",
    "filtered_clustered_data = filtered_data.iloc[clustered_indices]\n",
    "filtered_clustered_data['Cluster'] = clusters[clusters != -1]\n",
    "\n",
    "# Define function to get top words\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "# Extract top words for each cluster\n",
    "cluster_top_words = []\n",
    "for cluster_num in set(clusters) - {-1}:\n",
    "    cluster_docs = filtered_clustered_data[filtered_clustered_data['Cluster'] == cluster_num]['description']\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    top_words_str = ', '.join([word for word, freq in top_words])\n",
    "    cluster_top_words.append(top_words_str)\n",
    "\n",
    "# Generate TF-IDF matrix for top words of each cluster\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_top_words)\n",
    "\n",
    "# Compute cosine similarity between cluster top words\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Identify clusters to merge by finding connected components in the similarity graph\n",
    "similarity_threshold = 0.6\n",
    "graph = cosine_sim_matrix > similarity_threshold\n",
    "n_components, labels = connected_components(csgraph=csr_matrix(graph), directed=False)\n",
    "\n",
    "# Display top words from merged clusters and remaining non-merged clusters\n",
    "for component in range(n_components):\n",
    "    # Find indices of clusters in the current component\n",
    "    indices = np.where(labels == component)[0]\n",
    "    if len(indices) == 1:\n",
    "        # Non-merged cluster\n",
    "        print(f\"Non-merged Cluster {indices[0]} Topics: {cluster_top_words[indices[0]]}\")\n",
    "    else:\n",
    "        # Merged clusters - displaying top words from one of the merging clusters arbitrarily\n",
    "        print(f\"Merged Clusters {indices.tolist()} Topics (from one of them): {cluster_top_words[indices[0]]}\")\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"TIME: \", t2-t1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526dc4b9-41fc-42f8-977e-a63cc6408835",
   "metadata": {},
   "source": [
    "## Increasing the cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40abc981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Clusters [0, 1] Topics (from one of them): pg, petition, fully, scanned, action, initiated, receipt, hard, copy, enclosures\n",
      "Non-merged Cluster 2 Topics: industry, cashew, kerala, workers, banks, non, severe, crisis, years, processing\n",
      "Non-merged Cluster 3 Topics: rs, charges, 2020, refund, moratorium, bank, 19, period, covid, 08\n",
      "Non-merged Cluster 4 Topics: bank, rrb, vacancies, baroda, sir, ibps, bob, withdraw, examination, exam\n",
      "Non-merged Cluster 5 Topics: oral, representation, received, details, applicant, pmo, 2021, nb, complaints, phone\n",
      "Non-merged Cluster 6 Topics: dhfl, fd, holders, public, ibc, process, resolution, rbi, 160, nhb\n",
      "Non-merged Cluster 7 Topics: banks, bank, public, sector, privatisation, 39, sir, private, india, employees\n",
      "Non-merged Cluster 8 Topics: bank, loan, problem, kumar, tonmay, salp, sir, kindly, matter, 2021\n",
      "Non-merged Cluster 9 Topics: bank, loan, idbi, money, request, rbi, application, charges, home, officer\n",
      "Merged Clusters [10, 11, 12] Topics (from one of them): bank, salary, arrears, paid, 01, salaries, settlement, iba, board, 2021\n",
      "Non-merged Cluster 13 Topics: loan, bank, mudra, business, sir, quot, start, help, 39, documents\n",
      "Non-merged Cluster 14 Topics: pension, bank, sbi, account, da, sir, branch, 2020, office, cppc\n",
      "Non-merged Cluster 15 Topics: loan, education, bank, sir, branch, 39, student, able, nursing, application\n",
      "Non-merged Cluster 16 Topics: bank, hai, loan, ki, se, ka, mai, ke, phalodi, pnb\n",
      "Non-merged Cluster 17 Topics: subsidy, pmay, loan, bank, sir, application, home, id, account, received\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# Apply HDBSCAN to the obtained embeddings\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, prediction_data=True)\n",
    "clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# Filter the original data to include only those rows that were clustered\n",
    "clustered_indices = [i for i, label in enumerate(clusters) if label != -1]\n",
    "filtered_clustered_data = filtered_data.iloc[clustered_indices]\n",
    "filtered_clustered_data['Cluster'] = clusters[clusters != -1]\n",
    "\n",
    "# Define function to get top words\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "# Extract top words for each cluster\n",
    "cluster_top_words = []\n",
    "for cluster_num in set(clusters) - {-1}:\n",
    "    cluster_docs = filtered_clustered_data[filtered_clustered_data['Cluster'] == cluster_num]['description']\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    top_words_str = ', '.join([word for word, freq in top_words])\n",
    "    cluster_top_words.append(top_words_str)\n",
    "\n",
    "# Generate TF-IDF matrix for top words of each cluster\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_top_words)\n",
    "\n",
    "# Compute cosine similarity between cluster top words\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Identify clusters to merge by finding connected components in the similarity graph\n",
    "similarity_threshold = 0.6\n",
    "graph = cosine_sim_matrix > similarity_threshold\n",
    "n_components, labels = connected_components(csgraph=csr_matrix(graph), directed=False)\n",
    "\n",
    "# Display top words from merged clusters and remaining non-merged clusters\n",
    "for component in range(n_components):\n",
    "    # Find indices of clusters in the current component\n",
    "    indices = np.where(labels == component)[0]\n",
    "    if len(indices) == 1:\n",
    "        # Non-merged cluster\n",
    "        print(f\"Non-merged Cluster {indices[0]} Topics: {cluster_top_words[indices[0]]}\")\n",
    "    else:\n",
    "        # Merged clusters - displaying top words from one of the merging clusters arbitrarily\n",
    "        print(f\"Merged Clusters {indices.tolist()} Topics (from one of them): {cluster_top_words[indices[0]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7cc63-56d4-4f5d-964c-3214a7976e9b",
   "metadata": {},
   "source": [
    "## Bertopic on mpnet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5fb162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['subsidy', 'pmay', 'home', 'loan', 'housing', 'scheme', 'application', 'have', 'for', 'from'], 1: ['hai', 'ki', 'se', 'ke', 'ka', 'ko', 'nahi', 'kar', 'bhi', 'aur'], 2: ['account', 'transaction', 'atm', 'amount', 'money', 'bank', 'rs', 'my', 'on', 'no'], 3: ['education', 'loan', 'student', 'college', 'my', 'university', 'applied', 'for', 'fees', 'studies'], 4: ['pmegp', 'loan', 'project', 'unit', 'msme', 'manager', 'for', 'under', 'the', 'scheme'], 5: ['banks', 'privatisation', 'sector', 'public', 'privatization', 'private', 'psu', 'psbs', 'employees', 'govt'], 6: ['branch', 'manager', 'account', 'the', 'of', 'bank', 'he', 'to', 'she', 'and'], 7: ['mudra', 'loan', 'business', 'start', 'for', 'sir', 'manager', 'my', 'have', 'am'], 8: ['business', 'we', 'for', 'loan', 'to', 'start', 'small', 'help', 'am', 'my'], 9: ['pension', 'ppo', 'cppc', 'family', 'no', 'pensioner', 'branch', 'my', 'railway', 'of'], 10: ['hdfc', 'card', 'credit', 'account', 'my', 'bank', 'and', 'payment', 'they', 'me'], 11: ['card', 'sbi', 'credit', 'charges', 'payment', 'cards', 'call', 'me', 'they', 'to'], 12: ['ibps', 'exam', 'interview', 'rrb', 'vacancies', 'mains', 'recruitment', 'examination', 'candidates', 'candidate'], 13: ['160', 'education', 'loan', 'job', 'interest', 'my', 'amount', 'pay', 'am', 'family'], 14: ['emi', 'pay', 'amount', 'paid', 'my', 'they', 'moratorium', 'from', 'charges', 'month'], 15: ['pnb', 'passbook', 'national', 'branch', 'punjab', 'account', 'no', 'not', 'atm', 'and'], 16: ['dhfl', 'fd', 'holders', 'ibc', 'resolution', 'public', 'invested', 'process', 'nhb', 'administrator'], 17: ['us', 'salaries', 'were', 'salary', 'settlement', 'abruptly', 'unions', 'our', 'revision', 'bipartite'], 18: ['axis', 'account', 'bank', 'debit', 'card', 'address', 'my', 'customer', 'this', 'minimum'], 19: ['transfer', 'my', 'in', 'place', 'am', 'leave', 'posting', 'he', 'her', 'to'], 20: ['nominee', 'father', 'late', 'death', 'court', 'his', 'barrackpore', 'legal', 'of', 'the'], 21: ['phalodi', 'barnch', 'pnb', 'mai', 'davra', 'yozna', 'mudhra', 'pardhanmantri', 'kiya', 'aap'], 22: ['hai', '160', 'ke', 'se', 'ki', 'nhi', 'ko', 'meri', 'ka', 'ek'], 23: ['cibil', 'score', 'report', 'loan', 'loans', 'they', 'credit', 'in', 'to', 'record'], 24: ['emi', 'pay', 'job', 'my', 'am', 'loan', 'due', 'help', 'in', 'situation']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "topic_model = BERTopic()\n",
    "docs = descriptions\n",
    "doc_id = list(range(1, 7959))\n",
    "# Fit the BERTopic model on the dummy documents and obtained embeddings\n",
    "t1 = time.time()\n",
    "topics, _ = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# Extract the topics and their frequencies\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Get the top 10 topics\n",
    "top_topics = topic_info.head(26)  # Get 11 as one topic might be -1 (outliers)\n",
    "\n",
    "topic_dict = {}\n",
    "# Print the top 25 topics and their top 10 words\n",
    "for topic in top_topics['Topic']:\n",
    "    if topic != -1:  # excluding the outlier cluster\n",
    "        # Get top 10 words for the topic\n",
    "        topic_words = topic_model.get_topic(topic)[:10]\n",
    "        # Extract only the words (not the scores)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        # Print the topic and its words\n",
    "        #print(f\"Topic {topic}: \", \", \".join(words))\n",
    "        topic_dict[topic] = words\n",
    "        \n",
    "print(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5cf58c3-bb10-444a-8c84-e35bb23e596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'words': ['subsidy', 'pmay', 'home', 'loan', 'housing', 'scheme', 'application', 'have', 'for', 'from'], 'doc_ids': [9, 33, 38, 53, 63, 84, 85, 87, 92, 114, 119, 122, 136, 151, 154, 161, 202, 217, 218, 220, 226, 231, 245, 247, 257, 275, 283, 288, 301, 303, 308, 311, 313, 323, 328, 334, 348, 368, 380, 388, 396, 398, 407, 414, 442, 465, 482, 492, 504, 506, 535, 567, 604, 625, 664, 670, 685, 702, 708, 727, 738, 742, 749, 750, 765, 768, 780, 794, 797, 807, 809, 845, 854, 891, 914, 918, 922, 930, 941, 944, 945, 957, 970, 973, 992, 1004, 1028, 1057, 1063, 1072, 1091, 1119, 1121, 1143, 1152, 1165, 1176, 1186, 1189, 1202, 1209, 1216, 1217, 1220, 1224, 1237, 1238, 1254, 1281, 1282, 1351, 1382, 1404, 1410, 1421, 1453, 1457, 1474, 1477, 1489, 1502, 1517, 1523, 1527, 1528, 1531, 1540, 1549, 1583, 1597, 1601, 1602, 1603, 1618, 1622, 1626, 1634, 1639, 1659, 1667, 1687, 1708, 1716, 1739, 1754, 1756, 1761, 1766, 1770, 1772, 1773, 1777, 1786, 1803, 1822, 1833, 1837, 1842, 1862, 1864, 1873, 1888, 1900, 1902, 1909, 1916, 1922, 1951, 1967, 1970, 1974, 1976, 1980, 1983, 2005, 2014, 2020, 2027, 2040, 2059, 2089, 2112, 2128, 2135, 2141, 2154, 2161, 2166, 2172, 2176, 2186, 2193, 2202, 2218, 2233, 2242, 2245, 2247, 2250, 2257, 2258, 2260, 2279, 2292, 2294, 2300, 2307, 2323, 2325, 2358, 2378, 2383, 2398, 2400, 2404, 2413, 2433, 2443, 2448, 2452, 2463, 2464, 2474, 2480, 2485, 2514, 2517, 2523, 2531, 2536, 2545, 2552, 2556, 2572, 2606, 2611, 2630, 2632, 2641, 2645, 2668, 2676, 2677, 2678, 2679, 2685, 2689, 2711, 2728, 2739, 2751, 2782, 2791, 2810, 2812, 2829, 2839, 2849, 2853, 2977, 2979, 3009, 3032, 3043, 3058, 3061, 3081, 3090, 3091, 3102, 3110, 3115, 3121, 3125, 3128, 3142, 3148, 3172, 3190, 3210, 3214, 3221, 3222, 3235, 3259, 3263, 3264, 3285, 3287, 3289, 3307, 3324, 3327, 3342, 3344, 3353, 3364, 3369, 3376, 3378, 3390, 3420, 3423, 3424, 3425, 3426, 3429, 3443, 3472, 3522, 3561, 3567, 3575, 3580, 3583, 3594, 3599, 3604, 3607, 3615, 3619, 3622, 3623, 3624, 3627, 3671, 3675, 3680, 3684, 3691, 3700, 3708, 3713, 3716, 3732, 3735, 3772, 3781, 3791, 3795, 3807, 3816, 3820, 3824, 3829, 3849, 3864, 3871, 3896, 3918, 3938, 3965, 3979, 3999, 4012, 4024, 4028, 4033, 4051, 4064, 4065, 4075, 4079, 4090, 4102, 4139, 4171, 4175, 4201, 4229, 4239, 4247, 4254, 4261, 4298, 4301, 4305, 4339, 4351, 4361, 4369, 4403, 4408, 4411, 4412, 4438, 4472, 4477, 4505, 4526, 4535, 4556, 4567, 4591, 4637, 4670, 4688, 4738, 4743, 4759, 4797, 4819, 4856, 4859, 4895, 4906, 4921, 4938, 4941, 4946, 4947, 4954, 4965, 4972, 4980, 4987, 4988, 4992, 4996, 5031, 5059, 5061, 5091, 5096, 5099, 5119, 5120, 5163, 5183, 5196, 5217, 5221, 5237, 5239, 5243, 5277, 5296, 5326, 5329, 5333, 5357, 5363, 5371, 5373, 5376, 5388, 5404, 5409, 5416, 5439, 5442, 5456, 5465, 5476, 5479, 5487, 5489, 5496, 5514, 5516, 5524, 5528, 5529, 5535, 5561, 5591, 5593, 5620, 5625, 5632, 5633, 5634, 5637, 5639, 5652, 5700, 5704, 5740, 5744, 5756, 5762, 5769, 5775, 5805, 5818, 5827, 5832, 5839, 5840, 5841, 5854, 5858, 5863, 5875, 5884, 5888, 5901, 5903, 5913, 5917, 5933, 5943, 5945, 5953, 5972, 5976, 5982, 5986, 5987, 5991, 5996, 6013, 6014, 6016, 6017, 6029, 6066, 6098, 6105, 6108, 6113, 6118, 6119, 6121, 6122, 6123, 6125, 6146, 6152, 6156, 6169, 6171, 6176, 6178, 6179, 6187, 6190, 6202, 6213, 6216, 6227, 6253, 6254, 6255, 6263, 6268, 6269, 6274, 6289, 6290, 6292, 6297, 6303, 6305, 6306, 6334, 6345, 6360, 6374, 6379, 6392, 6398, 6421, 6437, 6439, 6444, 6459, 6463, 6476, 6503, 6512, 6522, 6525, 6540, 6548, 6550, 6567, 6573, 6586, 6610, 6620, 6629, 6653, 6662, 6669, 6683, 6730, 6732, 6734, 6735, 6736, 6740, 6756, 6769, 6771, 6796, 6799, 6803, 6806, 6807, 6820, 6822, 6843, 6867, 6868, 6914, 6923, 6925, 6934, 6937, 6974, 6980, 6983, 6987, 6992, 6995, 7024, 7031, 7041, 7042, 7046, 7059, 7060, 7092, 7128, 7142, 7163, 7177, 7218, 7220, 7225, 7240, 7241, 7243, 7244, 7270, 7277, 7278, 7279, 7303, 7307, 7314, 7326, 7327, 7335, 7338, 7353, 7361, 7371, 7372, 7374, 7386, 7395, 7404, 7414, 7416, 7418, 7425, 7431, 7442, 7444, 7461, 7476, 7506, 7516, 7527, 7529, 7544, 7552, 7565, 7580, 7595, 7611, 7618, 7631, 7643, 7662, 7663, 7672, 7696, 7706, 7713, 7722, 7734, 7760, 7770, 7780, 7788, 7796, 7799, 7803, 7804, 7805, 7816, 7817, 7829, 7835, 7838, 7866, 7887, 7900, 7903, 7911, 7912, 7915, 7926, 7930, 7941, 7951]}, 1: {'words': ['banks', 'privatisation', 'sector', 'public', 'privatization', 'private', 'psu', 'govt', 'psbs', 'employees'], 'doc_ids': [846, 1429, 1447, 1550, 1917, 2806, 3022, 3117, 3262, 3373, 3416, 3789, 3873, 3924, 4108, 4231, 4276, 4299, 4343, 4387, 4616, 4619, 4621, 4622, 4624, 4628, 4633, 4645, 4646, 4652, 4656, 4660, 4679, 4682, 4683, 4685, 4691, 4693, 4700, 4705, 4708, 4713, 4724, 4731, 4733, 4735, 4737, 4740, 4742, 4744, 4745, 4746, 4747, 4754, 4755, 4762, 4768, 4771, 4772, 4773, 4780, 4781, 4783, 4784, 4786, 4787, 4789, 4791, 4793, 4794, 4802, 4803, 4804, 4806, 4811, 4814, 4816, 4817, 4821, 4823, 4826, 4827, 4831, 4832, 4833, 4835, 4836, 4837, 4843, 4845, 4851, 4861, 4867, 4875, 4882, 4893, 4900, 4911, 4917, 4924, 4928, 4935, 4939, 4961, 4973, 4974, 4975, 4995, 5002, 5003, 5007, 5010, 5020, 5023, 5030, 5034, 5046, 5047, 5048, 5072, 5102, 5286, 5330, 5341, 5358, 5382, 5387, 5414, 5417, 5454, 5462, 5467, 5520, 6422, 6472, 6479, 6645, 6679, 6710, 6801, 6827, 6928, 7245, 7260, 7288, 7674, 7892]}, 2: {'words': ['hai', 'se', 'ki', 'mera', 'ka', 'nahi', 'mere', 'ko', 'ke', 'bhi'], 'doc_ids': [22, 30, 195, 212, 274, 285, 361, 362, 394, 423, 527, 728, 729, 856, 874, 939, 1021, 1071, 1114, 1207, 1288, 1310, 1339, 1374, 1401, 1472, 1532, 1571, 1574, 1593, 1599, 1694, 1727, 1758, 1760, 1828, 1831, 1878, 1977, 1979, 1991, 2012, 2123, 2125, 2151, 2185, 2244, 2271, 2311, 2334, 2397, 2424, 2482, 2516, 2542, 2550, 2643, 2658, 2695, 2825, 3072, 3129, 3150, 3250, 3272, 3312, 3440, 3897, 3932, 3967, 3968, 4055, 4097, 4141, 4246, 4260, 4315, 4464, 4468, 4540, 4570, 4579, 4635, 4799, 4860, 4868, 5033, 5063, 5066, 5164, 5185, 5227, 5310, 5324, 5336, 5432, 5464, 5503, 5562, 5582, 5585, 5592, 5648, 5673, 5695, 5724, 5771, 5812, 5826, 6006, 6167, 6200, 6221, 6286, 6313, 6318, 6357, 6409, 6488, 6558, 6575, 6582, 6613, 6707, 6828, 6848, 6854, 6874, 6886, 6894, 6952, 6978, 7040, 7112, 7127, 7230, 7391, 7436, 7475, 7567, 7568, 7627, 7692, 7754, 7855, 7907]}, 3: {'words': ['pension', 'ppo', 'cppc', 'family', 'of', 'branch', 'pensioner', 'to', 'no', 'the'], 'doc_ids': [1, 62, 252, 304, 309, 347, 354, 426, 431, 508, 608, 628, 716, 761, 788, 1049, 1070, 1122, 1193, 1195, 1276, 1342, 1343, 1507, 1542, 1661, 1702, 1725, 1819, 1848, 1994, 2008, 2082, 2208, 2225, 2232, 2365, 2380, 2555, 2571, 2743, 2793, 2800, 2803, 3033, 3042, 3083, 3166, 3180, 3197, 3283, 3314, 3360, 3521, 3677, 3688, 3730, 3751, 3770, 3808, 4050, 4073, 4283, 4284, 4322, 4371, 4380, 4441, 4459, 4502, 4552, 4560, 4617, 4750, 4822, 4848, 4902, 4930, 5018, 5038, 5045, 5052, 5199, 5206, 5288, 5290, 5362, 5525, 5555, 5595, 5627, 5685, 5691, 5713, 5780, 5855, 5873, 5898, 5965, 6030, 6046, 6148, 6160, 6186, 6189, 6220, 6259, 6291, 6295, 6346, 6350, 6417, 6442, 6445, 6467, 6509, 6585, 6644, 6749, 6893, 6912, 6924, 7038, 7053, 7091, 7134, 7271, 7324, 7354, 7384, 7409, 7463, 7477, 7543, 7577, 7585, 7659, 7727, 7765, 7789, 7883]}, 4: {'words': ['mudra', 'loan', 'business', 'start', 'for', 'manager', 'my', 'sir', 'have', 'to'], 'doc_ids': [23, 24, 80, 169, 183, 236, 306, 358, 446, 461, 631, 848, 1045, 1221, 1233, 1313, 1316, 1463, 1555, 1656, 1657, 1742, 1799, 1817, 1855, 1907, 1965, 2001, 2090, 2092, 2278, 2337, 2428, 2436, 2483, 2553, 2554, 2704, 2721, 2875, 2951, 3012, 3146, 3171, 3356, 3421, 3535, 3536, 3590, 3629, 3698, 3783, 3887, 3929, 3933, 4017, 4143, 4209, 4210, 4211, 4349, 4358, 4444, 4446, 4448, 4541, 4554, 4672, 4732, 4888, 5004, 5029, 5068, 5155, 5235, 5271, 5415, 5536, 5559, 5707, 5723, 5741, 5777, 5778, 5788, 5806, 5815, 6001, 6008, 6076, 6087, 6106, 6302, 6384, 6453, 6529, 6542, 6676, 6678, 6788, 6821, 6871, 6906, 6946, 7016, 7020, 7043, 7054, 7213, 7344, 7363, 7509, 7603, 7756]}, 5: {'words': ['pmegp', 'loan', 'project', 'manager', 'rejected', 'applied', 'unit', 'branch', 'proposal', 'under'], 'doc_ids': [60, 120, 205, 224, 505, 510, 597, 680, 981, 990, 1005, 1046, 1074, 1234, 1338, 1344, 1458, 1569, 1664, 1859, 2226, 2290, 2349, 2351, 2379, 2561, 2568, 2576, 2744, 2790, 2828, 2913, 3039, 3082, 3253, 3268, 3361, 3431, 3459, 3461, 3558, 3562, 3597, 3648, 3650, 3660, 3725, 3771, 3799, 3804, 3884, 3907, 4016, 4032, 4326, 4337, 4386, 4407, 4445, 4504, 4550, 4587, 4698, 4957, 5024, 5037, 5321, 5421, 5488, 5606, 5718, 5719, 5783, 5797, 5810, 5813, 6035, 6062, 6071, 6193, 6426, 6526, 6641, 6657, 6797, 6833, 6916, 6970, 7398, 7440, 7457, 7458, 7763, 7832, 7840, 7848, 7853, 7864]}, 6: {'words': ['hdfc', 'card', 'credit', 'account', 'and', 'bank', 'my', 'payment', 'they', 'with'], 'doc_ids': [44, 55, 142, 173, 237, 248, 291, 495, 760, 779, 924, 1102, 1133, 1268, 1280, 1431, 1432, 1446, 1582, 1804, 2030, 2035, 2088, 2114, 2131, 2174, 2222, 2251, 2301, 2403, 2422, 2548, 3021, 3159, 3187, 3242, 3292, 3317, 3330, 3395, 3397, 3456, 3527, 3572, 3702, 3726, 3823, 4011, 4052, 4129, 4288, 4341, 4368, 4454, 4601, 4644, 4775, 5054, 5058, 5089, 5116, 5188, 5202, 5225, 5291, 5307, 5428, 5547, 5572, 5604, 5681, 5774, 5787, 5861, 6077, 6387, 6395, 6450, 6577, 6579, 6688, 6713, 7037, 7061, 7166, 7236, 7434, 7520, 7575, 7774, 7812, 7833, 7913]}, 7: {'words': ['business', 'we', 'small', 'for', 'loan', 'startup', 'have', 'started', 'am', 'start'], 'doc_ids': [71, 168, 318, 322, 600, 682, 748, 842, 853, 1018, 1099, 1461, 1544, 1712, 1879, 1886, 1963, 1973, 2003, 2051, 2229, 2388, 2754, 2947, 3137, 3157, 3202, 3260, 3303, 3366, 3370, 3396, 3415, 3445, 3601, 3737, 3801, 3826, 4072, 4084, 4153, 4238, 4255, 4399, 4413, 4460, 4521, 4562, 4634, 4721, 4761, 4824, 4849, 4903, 4955, 4958, 4963, 5056, 5128, 5174, 5180, 5285, 5322, 5347, 5423, 5470, 5544, 5565, 5699, 5946, 6099, 6101, 6137, 6155, 6192, 6237, 6325, 6689, 6708, 6744, 6805, 6876, 6965, 7174, 7207, 7695, 7813]}, 8: {'words': ['card', 'sbi', 'credit', 'charges', 'payment', 'cards', 'call', 'me', 'outstanding', 'they'], 'doc_ids': [50, 178, 255, 266, 522, 579, 633, 707, 745, 751, 772, 804, 818, 857, 982, 991, 1154, 1198, 1298, 1309, 1353, 1368, 1375, 1402, 1585, 1614, 1753, 1841, 1844, 1861, 2038, 2160, 2205, 2293, 2705, 2809, 3126, 3131, 3186, 3220, 3400, 3512, 3647, 3714, 3786, 3914, 4031, 4040, 4233, 4401, 4417, 4476, 4503, 4559, 4563, 4729, 4869, 4877, 4918, 5114, 5308, 5359, 5570, 5575, 5679, 5809, 5939, 5940, 6070, 6203, 6207, 6352, 6420, 6717, 6731, 6863, 6988, 7126, 7200, 7208, 7446, 7460, 7616, 7827, 7859]}, 9: {'words': ['ibps', 'interview', 'exam', 'rrb', 'vacancies', 'recruitment', 'mains', 'examination', 'candidates', 'candidate'], 'doc_ids': [118, 330, 560, 601, 736, 793, 864, 901, 917, 925, 961, 969, 971, 984, 985, 986, 993, 997, 998, 1007, 1012, 1014, 1019, 1024, 1032, 1054, 1118, 1184, 1187, 1250, 1636, 1838, 1890, 1934, 1962, 3156, 3643, 3687, 3729, 3734, 3750, 3769, 3788, 3842, 3889, 3991, 4044, 4057, 4089, 4121, 4304, 4345, 5065, 5093, 5108, 5195, 5259, 5297, 5313, 5325, 5350, 5351, 5377, 5383, 5393, 5424, 5616, 5869, 6080, 6154, 6340, 6403, 6489, 6544, 6563, 7090, 7115, 7700, 7740, 7890]}, 10: {'words': ['pnb', 'passbook', 'national', 'punjab', 'branch', 'account', 'not', 'no', 'atm', 'is'], 'doc_ids': [47, 108, 190, 215, 470, 483, 490, 590, 652, 834, 844, 1720, 1778, 1813, 1836, 1846, 1871, 1903, 2206, 2348, 2350, 2361, 2423, 2494, 2617, 3007, 3038, 3077, 3241, 3311, 3359, 3547, 3669, 3670, 3681, 3724, 3815, 3861, 3863, 4168, 4179, 4230, 4262, 4470, 4620, 4876, 5008, 5113, 5158, 5275, 5276, 5375, 5485, 5486, 5560, 5736, 5871, 6180, 6359, 6381, 6480, 6814, 6960, 7028, 7125, 7192, 7197, 7214, 7306, 7397, 7468, 7528, 7546, 7715, 7719, 7772, 7906]}, 11: {'words': ['dhfl', 'fd', 'holders', 'ibc', 'resolution', 'public', 'invested', 'process', 'nhb', 'administrator'], 'doc_ids': [182, 369, 463, 481, 500, 512, 513, 514, 523, 539, 543, 545, 550, 554, 565, 574, 575, 576, 582, 584, 585, 588, 591, 594, 648, 657, 675, 677, 689, 703, 704, 717, 724, 777, 1001, 1027, 1048, 1090, 1136, 1153, 1297, 1377, 1380, 1395, 1406, 1545, 1612, 1693, 1738, 1918, 1930, 2070, 2284, 2377, 2931, 3261, 3284, 3326, 3630, 3653, 3814, 3913, 3942, 4418, 4475, 4568, 4680, 4785, 5097, 5445, 5737, 6078]}, 12: {'words': ['160', 'education', 'loan', 'job', 'interest', 'my', 'pay', 'family', 'amount', 'am'], 'doc_ids': [36, 233, 297, 700, 747, 929, 1029, 1084, 1225, 1235, 1533, 1745, 1850, 1949, 2075, 2094, 2254, 2360, 2416, 2593, 2671, 2722, 2724, 2731, 2814, 2982, 3005, 3111, 3113, 3145, 3192, 3229, 3240, 3320, 3492, 3595, 3668, 3696, 3797, 3875, 4002, 4109, 4182, 4457, 4583, 4909, 4948, 5245, 5450, 5474, 5493, 5878, 5879, 5928, 5936, 5955, 5977, 6056, 6096, 6311, 6668, 6783, 7081, 7323, 7500, 7505, 7511, 7549, 7599, 7782, 7806, 7869]}, 13: {'words': ['us', 'salaries', 'were', 'salary', 'settlement', 'abruptly', 'unions', 'revision', 'bipartite', 'burden'], 'doc_ids': [2763, 2766, 2769, 2770, 2771, 2772, 2774, 2777, 2781, 2784, 2785, 2786, 2794, 2797, 2798, 2804, 2816, 2820, 2824, 2827, 2831, 2835, 2836, 2837, 2838, 2844, 2846, 2847, 2848, 2851, 2859, 2861, 2864, 2876, 2878, 2884, 2886, 2893, 2898, 2899, 2908, 2911, 2912, 2921, 2935, 2941, 2942, 2952, 2955, 2956, 2958, 2970, 2974, 2978, 2980, 2981, 2994, 2996, 2997, 2999, 3001, 3002, 3003, 3004, 3006, 3050, 3051, 3101, 6020]}, 14: {'words': ['kcc', 'land', 'kisan', 'agriculture', 'farmers', 'loan', 'farmer', 'for', 'we', 'the'], 'doc_ids': [115, 260, 366, 464, 473, 517, 587, 754, 831, 1020, 1271, 1273, 1294, 1387, 1451, 1483, 1490, 1843, 1849, 1912, 1913, 2055, 2241, 2277, 2535, 2624, 3211, 3257, 3379, 3591, 3638, 3800, 4155, 4164, 4291, 4316, 4348, 4458, 4524, 4631, 4940, 5133, 5394, 5425, 5426, 5438, 5475, 5517, 5579, 5671, 5781, 5971, 6037, 6231, 6899, 7111, 7141, 7170, 7171, 7302, 7400, 7402, 7650, 7750, 7825]}, 15: {'words': ['axis', 'account', 'debit', 'bank', 'card', 'address', 'customer', 'balance', 'my', 'minimum'], 'doc_ids': [11, 97, 332, 609, 1013, 1168, 1249, 1346, 1503, 1504, 1632, 1688, 1717, 2069, 2426, 2560, 2654, 3095, 3556, 3637, 4056, 4081, 4083, 4088, 4120, 4225, 4263, 4428, 4451, 4739, 4910, 4952, 5150, 5176, 5247, 5251, 5269, 5758, 5767, 5803, 5808, 5876, 5995, 6007, 6052, 6149, 6191, 6201, 6246, 6266, 6356, 6448, 6483, 6530, 6643, 6652, 6775, 6920, 7199, 7315, 7683]}, 16: {'words': ['nursing', 'education', 'vidyalakshmi', 'application', 'student', 'gnm', 'bangalorel', 'loan', 'modigovt', 'portalvidyalakshmi'], 'doc_ids': [52, 91, 101, 530, 531, 649, 775, 838, 1096, 1423, 1705, 1751, 1937, 1960, 2167, 2710, 2897, 3017, 3023, 3066, 3067, 3068, 3089, 3245, 3280, 3281, 3282, 3290, 3291, 3697, 3867, 4021, 4054, 4340, 4429, 4927, 4929, 5067, 5209, 5211, 5212, 5283, 5378, 5540, 5823, 6049, 6709, 6835, 6836, 6846, 6883, 6884, 7107, 7108, 7284, 7301, 7472, 7724, 7747]}, 17: {'words': ['property', 'premises', 'the', 'ubi', 'premier', 'land', 'of', 'to', 'and', 'by'], 'doc_ids': [10, 39, 40, 132, 219, 559, 603, 651, 681, 686, 1033, 1137, 1386, 1452, 1570, 1697, 1748, 2026, 2100, 2101, 2102, 2155, 2329, 3298, 4110, 4150, 4189, 4248, 4484, 4516, 4581, 4853, 5395, 5463, 5477, 5566, 5612, 5656, 5661, 5722, 6019, 6067, 6124, 6181, 6196, 6251, 6496, 6701, 6794, 6870, 6877, 6972, 7021, 7066, 7427, 7453, 7482, 7489, 7510]}, 18: {'words': ['description', 'family', 'prime', 'minister', 'we', 'my', 'you', 'loan', 'pay', 'am'], 'doc_ids': [19, 121, 360, 730, 1069, 1092, 1466, 1701, 1709, 1882, 1883, 1905, 2019, 2048, 2236, 2304, 2369, 2582, 2603, 2612, 2634, 3139, 3244, 3313, 3316, 3337, 3569, 3764, 3838, 3891, 3966, 3978, 4490, 4607, 4828, 4908, 5218, 5223, 5519, 5728, 5786, 5844, 5877, 5882, 5883, 6204, 6407, 7001, 7011, 7047, 7105, 7131, 7294, 7342, 7421, 7470, 7586, 7779]}, 19: {'words': ['sbi', 'grievance', '4834', 'of', 'the', 'by', 'justice', 'fathers', 'and', 'complaints'], 'doc_ids': [57, 58, 156, 157, 158, 171, 563, 743, 871, 1026, 1232, 1372, 1513, 1557, 1811, 1826, 2044, 2386, 2471, 2487, 2541, 2865, 3270, 3301, 3469, 3478, 3480, 3720, 4146, 4158, 4364, 4479, 4594, 4764, 4931, 4984, 5304, 5348, 5396, 5584, 5672, 5739, 5748, 5984, 5993, 6199, 6328, 6404, 6674, 6750, 6907, 7253, 7276, 7612, 7670, 7761, 7844]}, 20: {'words': ['idbi', 'roi', 'floating', 'rate', 'rajesh', 'ltd', 'pai', 'honest', 'money', 'policy'], 'doc_ids': [15, 109, 294, 295, 314, 315, 479, 489, 1080, 1081, 1256, 1340, 1580, 1581, 1609, 1726, 1946, 2129, 2285, 2288, 2289, 2446, 2447, 2532, 2672, 3092, 3175, 3182, 3238, 3246, 3247, 3341, 3446, 3908, 4014, 4015, 4068, 4070, 4124, 4125, 4309, 4415, 4416, 4522, 4523, 4627, 4854, 4896, 5079, 5080, 5081, 6559, 6910]}, 21: {'words': ['hai', 'ki', 'se', 'ke', 'ka', 'tha', 'main', 'liye', 'bhi', 'ho'], 'doc_ids': [243, 353, 356, 371, 402, 638, 955, 1289, 1290, 1433, 1526, 1592, 1653, 1775, 2546, 2587, 2589, 2732, 2858, 3839, 3911, 4030, 4078, 4176, 4218, 4292, 4359, 4365, 4455, 4463, 4603, 4741, 5006, 5365, 5664, 5852, 5885, 5963, 6085, 6304, 6621, 6635, 6921, 6932, 6950, 6958, 7033, 7096, 7157, 7257, 7526, 7847, 7894]}, 22: {'words': ['education', 'college', 'fees', 'loan', 'father', 'my', 'admission', 'for', 'university', 'help'], 'doc_ids': [134, 269, 471, 532, 613, 660, 758, 799, 938, 1399, 1731, 2066, 2119, 2203, 2266, 2374, 2583, 3014, 3155, 3333, 3439, 3834, 3852, 4010, 4085, 4130, 4140, 4222, 4367, 4378, 4604, 4638, 4661, 4678, 5104, 5238, 5249, 5355, 5384, 5443, 5738, 6018, 6069, 6086, 6353, 6490, 6832, 7030, 7048, 7262, 7408, 7449, 7591]}, 23: {'words': ['hai', 'ki', 'ka', 'ke', 'se', 'mein', 'ko', 'hain', 'kiya', 'aur'], 'doc_ids': [544, 715, 1306, 1354, 1703, 1747, 1815, 2117, 2143, 2317, 2686, 3112, 3116, 3275, 3309, 3310, 3374, 3427, 3579, 3582, 3758, 3995, 4062, 4720, 4751, 4763, 4839, 4842, 4872, 4953, 5117, 5172, 5190, 5311, 5380, 5420, 5497, 5929, 6083, 6168, 6337, 6515, 6606, 6737, 7019, 7077, 7140, 7309, 7318, 7512, 7699, 7863]}, 24: {'words': ['emi', 'charges', 'dmi', 'ecs', 'paid', 'pay', 'they', 'moratorium', 'month', 'my'], 'doc_ids': [66, 107, 185, 227, 228, 374, 627, 721, 921, 1240, 1270, 1508, 1556, 1594, 1682, 1683, 1741, 1779, 1853, 2079, 2309, 2371, 3045, 3160, 3184, 3432, 3454, 3498, 3608, 3715, 3785, 3902, 4128, 4599, 4655, 4778, 4834, 5126, 5131, 5320, 5763, 5802, 5924, 6182, 6234, 6279, 6664, 6752, 6931, 7224, 7642, 7920]}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "topic_model = BERTopic()\n",
    "docs = descriptions\n",
    "doc_id = list(range(1, 7959))  # List of registration Numbers.....\n",
    "\n",
    "topics, _ = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "\n",
    "# Extract the topics and their frequencies\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Get the top 10 topics\n",
    "top_topics = topic_info.head(26)  # Get 26 as one topic might be -1 (outliers)\n",
    "\n",
    "topic_dict = {}\n",
    "doc_topic_dict = {}  # Initialize a new dictionary to store document IDs for each topic\n",
    "\n",
    "# Populate doc_topic_dict with document IDs for each topic\n",
    "for doc_index, topic_number in enumerate(topics):\n",
    "    if topic_number != -1:  # excluding the outlier cluster\n",
    "        if topic_number in doc_topic_dict:\n",
    "            doc_topic_dict[topic_number].append(doc_id[doc_index])\n",
    "        else:\n",
    "            doc_topic_dict[topic_number] = [doc_id[doc_index]]\n",
    "\n",
    "# Iterate through the top topics and add document IDs to topic_dict\n",
    "for topic in top_topics['Topic']:\n",
    "    if topic != -1:  # excluding the outlier cluster\n",
    "        # Get top 10 words for the topic\n",
    "        topic_words = topic_model.get_topic(topic)[:10]\n",
    "        # Extract only the words (not the scores)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        # Combine words and document IDs\n",
    "        topic_info = {\n",
    "            'words': words,\n",
    "            'doc_ids': doc_topic_dict.get(topic, [])\n",
    "        }\n",
    "        topic_dict[topic] = topic_info\n",
    "\n",
    "print(topic_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1805bef-d33b-46e1-a17d-6c9416ce3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, embeddings):\n",
    "    # Convert the 'regdate' column to datetime\n",
    "    data['regdate'] = pd.to_datetime(data['regdate'])\n",
    "\n",
    "    # Filter the data based on the conditions\n",
    "    filtered_data = data[(data['regdate'] >= pd.to_datetime(starting_date)) &\n",
    "                         (data['regdate'] <= pd.to_datetime(ending_date)) &\n",
    "                         (data['initforward'] == initforward_str)]\n",
    "\n",
    "    # Get indices of filtered data\n",
    "    indices = filtered_data.index.tolist()\n",
    "\n",
    "    # Ensure indices are within the range of stored_embeddings length\n",
    "    valid_indices = [index for index in indices if index < len(stored_embeddings)]\n",
    "\n",
    "    # Retrieve embeddings for the filtered data\n",
    "    obtained_embeddings = np.array(stored_embeddings)[valid_indices]\n",
    "\n",
    "    # Retrieve descriptions for the filtered data\n",
    "    descriptions = filtered_data.loc[valid_indices, 'description'].tolist()\n",
    "    filtered_data = filtered_data.loc[valid_indices]\n",
    "\n",
    "    return obtained_embeddings, descriptions\n",
    "\n",
    "\n",
    "\n",
    "def custom_rca():\n",
    "    # Inputs from user\n",
    "    starting_date = input(\"Enter the starting date (YYYY-MM-DD): \")\n",
    "    ending_date = input(\"Enter the ending date (YYYY-MM-DD): \")\n",
    "    initforward_str = input(\"Enter the initforward string: \")\n",
    "    data = df[df['initforward'].isna() == False]  # Filter data where 'initforward' is not NaN\n",
    "    # Get the embeddings and descriptions\n",
    "    obtained_embeddings, descriptions = get_embeddings_by_date_and_initforward(starting_date, ending_date, initforward_str, data, embeddings)\n",
    "    \n",
    "    topic_model = BERTopic()\n",
    "    docs = descriptions\n",
    "    doc_id = list(range(1, 7959))  # List of registration Numbers.....\n",
    "\n",
    "    \n",
    "    # Fit the BERTopic model on the dummy documents and obtained embeddings\n",
    "    \n",
    "    topics, _ = topic_model.fit_transform(docs, obtained_embeddings)\n",
    "    \n",
    "    # Extract the topics and their frequencies\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Get the top 25 topics\n",
    "    top_topics = topic_info.head(26)  # Get 26 as one topic might be -1 (outliers)\n",
    "    \n",
    "    topic_dict = {}\n",
    "    doc_topic_dict = {}  # Initialize a new dictionary to store document IDs for each topic\n",
    "    \n",
    "    # Populate doc_topic_dict with document IDs for each topic\n",
    "    for doc_index, topic_number in enumerate(topics):\n",
    "        if topic_number != -1:  # excluding the outlier cluster\n",
    "            if topic_number in doc_topic_dict:\n",
    "                doc_topic_dict[topic_number].append(doc_id[doc_index])\n",
    "            else:\n",
    "                doc_topic_dict[topic_number] = [doc_id[doc_index]]\n",
    "    \n",
    "    # Iterate through the top topics and add document IDs to topic_dict\n",
    "    for topic in top_topics['Topic']:\n",
    "        if topic != -1:  # excluding the outlier cluster\n",
    "            # Get top 10 words for the topic\n",
    "            topic_words = topic_model.get_topic(topic)[:10]\n",
    "            # Extract only the words (not the scores)\n",
    "            words = [word for word, _ in topic_words]\n",
    "            # Combine words and document IDs\n",
    "            topic_info = {\n",
    "                'words': words,\n",
    "                'doc_ids': doc_topic_dict.get(topic, [])\n",
    "            }\n",
    "            topic_dict[topic] = topic_info\n",
    "\n",
    "    return topic_dict\n",
    "\n",
    "results = custom_rca()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c4cd8-5e8b-42f6-91c3-35a8b0fd3c20",
   "metadata": {},
   "source": [
    "# Berttopic on documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef27e0-f992-457c-b6e2-767b561b93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic()\n",
    "docs = descriptions\n",
    "# Fit the BERTopic model on the dummy documents and obtained embeddings\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "# Extract the topics and their frequencies\n",
    "topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "193ccc7d-0830-40d2-9fde-b64ca4f258ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  hai, se, ki, ke, ka, ko, nahi, mai, mera, bhi\n",
      "Topic 1:  atm, account, transaction, money, amount, my, bank, the, but, no\n",
      "Topic 2:  pension, cppc, of, the, pensioners, ppo, to, no, sbi, from\n",
      "Topic 3:  subsidy, pmay, home, application, id, loan, housing, have, my, from\n",
      "Topic 4:  banks, privatisation, sector, public, private, are, privatization, banking, employees, of\n",
      "Topic 5:  mudra, business, loan, start, for, project, to, sir, my, am\n",
      "Topic 6:  my, loan, pay, family, am, help, me, to, now, amount\n",
      "Topic 7:  pmegp, loan, project, under, rejected, for, the, applied, application, manager\n",
      "Topic 8:  hdfc, card, credit, restructure, bank, charges, dated, complaint, and, my\n",
      "Topic 9:  cheque, account, book, cheques, bank, of, number, hdfc, the, branch\n",
      "Topic 10:  card, sbi, credit, cards, charges, payment, to, they, have, call\n",
      "Topic 11:  dhfl, fd, holders, ibc, resolution, public, process, invested, nhb, fds\n",
      "Topic 12:  transfer, my, in, daughter, he, her, family, is, of, to\n",
      "Topic 13:  us, salaries, were, salary, settlement, abruptly, unions, revision, our, bipartite\n",
      "Topic 14:  nominee, father, death, late, claim, account, of, bank, the, nomination\n",
      "Topic 15:  nursing, education, vidyalakshmi, application, gnm, bangalorel, deniedrespected, loan, student, lifethanking\n",
      "Topic 16:  us, salaries, salary, were, iba, settlement, bipartite, our, revision, arrears\n",
      "Topic 17:  address, card, account, proof, banks, bank, is, book, identity, open\n",
      "Topic 18:  education, college, father, fees, my, study, loan, admission, help, studies\n",
      "Topic 19:  sbi, account, transaction, branch, rs, to, no, my, 11223391613, the\n",
      "Topic 20:  awas, pradhan, mantri, yojana, yojna, subsidy, loan, home, scheme, housing\n",
      "Topic 21:  icici, citi, card, complaint, why, office, credit, customer, the, banking\n",
      "Topic 22:  charges, cash, digital, transaction, neft, banks, charge, debit, are, money\n",
      "Topic 23:  kcc, kisan, agriculture, kissan, card, pm, branch, interest, loan, land\n",
      "Topic 24:  cibil, score, report, loan, they, transunion, loans, credit, my, to\n"
     ]
    }
   ],
   "source": [
    "# Get the top 10 topics\n",
    "top_topics = topic_info.head(26)  # Get 11 as one topic might be -1 (outliers)\n",
    "\n",
    "# Print the top 10 topics and their top 10 words\n",
    "for topic in top_topics['Topic']:\n",
    "    if topic != -1:  # excluding the outlier cluster\n",
    "        # Get top 10 words for the topic\n",
    "        topic_words = topic_model.get_topic(topic)[:10]\n",
    "        # Extract only the words (not the scores)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        # Print the topic and its words\n",
    "        print(f\"Topic {topic}: \", \", \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a3b8f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-merged Cluster 0 Topics: industry, cashew, kerala, banks, workers, non, severe, crisis, years, processing\n",
      "Merged Clusters [1, 2, 4, 5, 9, 19, 20] Topics (from one of them): fully, scanned, action, initiated, receipt, hard, copy, enclosures, sent, shortly\n",
      "Merged Clusters [3, 6, 7, 11] Topics (from one of them): bank, salary, salaries, paid, settlement, help, month, 2020, 01, arrears\n",
      "Merged Clusters [8, 10] Topics (from one of them): bank, loan, problem, salp, kumar, sir, tonmay, emi, hdfc, 2021\n",
      "Non-merged Cluster 12 Topics: bank, salary, bipartite, board, management, employees, india, 11th, post, payments\n",
      "Non-merged Cluster 13 Topics: oral, representation, received, details, applicant, 2021, pmo, complaints, nb, phone\n",
      "Non-merged Cluster 14 Topics: credit, rs, 000, card, situation, bank, waive, ceremony, help, 50\n",
      "Non-merged Cluster 15 Topics: rs, charges, 2020, refund, bank, moratorium, 19, period, covid, 08\n",
      "Non-merged Cluster 16 Topics: loan, subsidy, bank, pmay, home, sir, application, 39, housing, scheme\n",
      "Non-merged Cluster 17 Topics: bank, transaction, account, aeps, gmail, debited, withdrawal, number, date, address\n",
      "Non-merged Cluster 18 Topics: dhfl, fd, holders, public, ibc, process, rbi, resolution, money, 160\n",
      "Non-merged Cluster 21 Topics: banks, bank, public, sector, india, 39, sir, government, privatisation, private\n",
      "Non-merged Cluster 22 Topics: hai, mera, number, bank, loan, account, kiya, ki, mein, ke\n",
      "Non-merged Cluster 23 Topics: 160, bank, ibps, exam, sir, interview, 39, rrb, vacancies, baroda\n",
      "Non-merged Cluster 24 Topics: ews, certificate, sbi, notification, 2019, 20, fy, valid, 2020, exam\n",
      "Non-merged Cluster 25 Topics: csp, hai, account, se, sbi, bank, sir, ke, mera, number\n",
      "Non-merged Cluster 26 Topics: bank, pmc, money, rbi, depositors, sir, 39, hard, earned, pm\n",
      "Non-merged Cluster 27 Topics: bank, money, kapol, request, hard, moratorium, earned, account, op, kapole\n",
      "Non-merged Cluster 28 Topics: release, bhatia, maturity, non, proceeds, folio, ca, sameer, jatinder, pal\n",
      "Non-merged Cluster 29 Topics: idfc, bank, 2021, bond, bonds, maturity, address, money, sir, received\n",
      "Merged Clusters [30, 48, 52, 53, 55, 56, 57] Topics (from one of them): 160, hai, sir, se, ke, ki, loan, bank, ka, ko\n",
      "Non-merged Cluster 31 Topics: bank, tds, rs, deducted, pan, deposit, tax, branch, 2020, sir\n",
      "Non-merged Cluster 32 Topics: bank, axis, account, card, sir, customer, branch, debit, credit, address\n",
      "Non-merged Cluster 33 Topics: fastag, sbi, toll, bank, account, sir, kyc, customer, rs, 2021\n",
      "Non-merged Cluster 34 Topics: bank, account, ppf, pf, branch, 2020, sir, kotak, sbi, transfer\n",
      "Non-merged Cluster 35 Topics: sir, sbi, pension, account, delhi, cppc, fraud, chandni, chowk, statement\n",
      "Non-merged Cluster 36 Topics: notes, currency, india, coins, rs, coin, 39, bank, note, silver\n",
      "Merged Clusters [37, 40] Topics (from one of them): phalodi, bank, pnb, barnch, mai, hai, loan, kiya, aap, ka\n",
      "Non-merged Cluster 38 Topics: da, sbi, pension, armed, pay, guards, account, mr, stopped, hav\n",
      "Non-merged Cluster 39 Topics: bank, policy, insurance, claim, 160, 39, sir, account, 2020, icici\n",
      "Non-merged Cluster 41 Topics: pension, bank, banks, pensioners, employees, retirees, years, updation, rbi, iba\n",
      "Non-merged Cluster 42 Topics: senior, citizens, pension, citizen, income, rate, retired, 39, india, request\n",
      "Non-merged Cluster 43 Topics: bank, account, father, branch, late, money, sir, death, nominee, india\n",
      "Non-merged Cluster 44 Topics: banks, loan, bank, people, india, loans, 39, money, apps, nbfcs\n",
      "Non-merged Cluster 45 Topics: pension, bank, branch, account, family, 2020, sbi, sir, ppo, cppc\n",
      "Non-merged Cluster 46 Topics: da, pension, sbi, account, oct, majesty, wef, request, hav, armed\n",
      "Non-merged Cluster 47 Topics: bank, 160, branch, village, town, people, banks, minister, request, open\n",
      "Non-merged Cluster 49 Topics: bank, card, credit, restructure, dated, rbi, hdfcbank, issues, hdfc, complaint\n",
      "Non-merged Cluster 50 Topics: bank, loan, sir, branch, kcc, land, 39, account, credit, pm\n",
      "Merged Clusters [51, 54] Topics (from one of them): hai, bank, se, account, sir, mera, ka, ki, ke, nahi\n",
      "Non-merged Cluster 58 Topics: bank, account, customers, india, banks, merger, online, branch, union, corporation\n",
      "Non-merged Cluster 59 Topics: bank, baroda, banking, branch, account, dena, given, bob, proper, problem\n",
      "Non-merged Cluster 60 Topics: bank, scheme, loan, 2020, rs, gecl, india, credit, eclgs, 20\n",
      "Non-merged Cluster 61 Topics: cibil, loan, bank, sir, score, credit, report, 160, branch, loans\n",
      "Non-merged Cluster 62 Topics: bank, hdfc, account, card, credit, rs, 39, number, sir, pay\n",
      "Non-merged Cluster 63 Topics: bank, sir, business, 160, loan, limit, cc, branch, india, hai\n",
      "Non-merged Cluster 64 Topics: loan, 39, cibil, bank, 2020, score, mudra, branch, sir, credit\n",
      "Non-merged Cluster 65 Topics: 160, bank, permanent, india, state, staff, branch, life, sir, sub\n",
      "Non-merged Cluster 66 Topics: bank, sir, transfer, 39, request, branch, family, working, india, work\n",
      "Non-merged Cluster 67 Topics: paytm, hai, account, bank, ke, se, ki, hia, sir, maine\n",
      "Merged Clusters [68, 93] Topics (from one of them): loan, bank, sir, minister, 39, prime, help, india, request, land\n",
      "Non-merged Cluster 69 Topics: branch, bank, patna, indian, main, account, atm, mentally, challenged, bihar\n",
      "Non-merged Cluster 70 Topics: bank, account, number, board, directors, patna, criminal, icici, india, informed\n",
      "Non-merged Cluster 71 Topics: bank, branch, canara, account, customer, rs, service, 2021, manager, banking\n",
      "Non-merged Cluster 72 Topics: loan, gold, bank, pay, sir, finance, request, time, auction, account\n",
      "Non-merged Cluster 73 Topics: bank, india, 39, sir, prime, case, minister, action, rs, kindly\n",
      "Non-merged Cluster 74 Topics: sbi, grievance, quot, bank, 2020, pmopg, 4834, father, india, accounts\n",
      "Non-merged Cluster 75 Topics: bank, complaint, 2020, office, grievance, pmopg, 2021, sir, pmo, banking\n",
      "Non-merged Cluster 76 Topics: bank, complaints, 39, evidences, complaint, authorities, justice, prove, corruption, disposed\n",
      "Non-merged Cluster 77 Topics: sbi, 2021, bank, sir, prime, minister, dated, india, kind, 2014\n",
      "Non-merged Cluster 78 Topics: bank, 2020, rs, loan, property, ubi, land, 2021, letter, premises\n",
      "Non-merged Cluster 79 Topics: loan, bank, hdfc, 39, rs, home, property, sir, account, branch\n",
      "Non-merged Cluster 80 Topics: google, pay, account, transaction, se, sir, bank, rs, ke, ka\n",
      "Non-merged Cluster 81 Topics: bank, manager, loan, branch, documents, property, mr, india, applicant, account\n",
      "Non-merged Cluster 82 Topics: bank, loan, 2020, dated, rs, complaint, officials, property, rti, account\n",
      "Non-merged Cluster 83 Topics: loan, bank, rate, home, roi, housing, rbi, 39, finance, complaint\n",
      "Merged Clusters [84, 85] Topics (from one of them): card, sbi, credit, sir, bank, rs, charges, payment, 39, 2020\n",
      "Non-merged Cluster 86 Topics: loan, 160, bank, sir, education, 39, pay, help, job, family\n",
      "Non-merged Cluster 87 Topics: loan, bank, education, sir, 39, branch, help, applied, student, application\n",
      "Non-merged Cluster 88 Topics: loan, home, bank, idbi, rate, account, honest, charges, letter, roi\n",
      "Non-merged Cluster 89 Topics: bank, money, idbi, loan, request, rbi, application, mr, customer, rajesh\n",
      "Non-merged Cluster 90 Topics: bank, address, banks, card, account, proof, people, customer, india, kyc\n",
      "Non-merged Cluster 91 Topics: bank, cash, digital, charges, card, payment, banks, money, debit, india\n",
      "Non-merged Cluster 92 Topics: loan, bank, pay, sir, 39, time, house, help, money, property\n",
      "Non-merged Cluster 94 Topics: loan, 39, bank, pay, help, sir, house, father, family, request\n",
      "Non-merged Cluster 95 Topics: bank, pay, help, loan, sir, credit, family, card, money, time\n",
      "Merged Clusters [96, 112, 113] Topics (from one of them): card, bank, credit, sir, pay, dues, time, request, job, help\n",
      "Non-merged Cluster 97 Topics: branch, bank, account, sbi, manager, sir, 39, service, india, asked\n",
      "Non-merged Cluster 98 Topics: bank, branch, account, manager, 2020, letter, complaint, matter, 39, 2021\n",
      "Non-merged Cluster 99 Topics: loan, bank, foreclosure, charges, request, letter, sir, rs, account, close\n",
      "Non-merged Cluster 100 Topics: bank, cheque, account, rs, branch, 2021, payment, customer, 21, 2020\n",
      "Non-merged Cluster 101 Topics: loan, bank, sir, business, mudra, help, 39, india, start, branch\n",
      "Non-merged Cluster 102 Topics: bank, branch, account, pnb, sir, national, punjab, passbook, rs, 160\n",
      "Non-merged Cluster 103 Topics: bank, cheque, book, account, sir, address, branch, new, 2021, savings\n",
      "Non-merged Cluster 104 Topics: bank, account, rs, sbi, branch, transaction, money, atm, 2020, sir\n",
      "Non-merged Cluster 105 Topics: bank, account, 160, payment, rs, number, fraud, money, transaction, details\n",
      "Non-merged Cluster 106 Topics: bank, money, sir, 39, account, online, company, complaint, fraud, 21\n",
      "Merged Clusters [107, 110] Topics (from one of them): bajaj, finance, rs, loan, 39, account, emi, paid, sir, bank\n",
      "Non-merged Cluster 108 Topics: loan, msme, bank, sir, 2021, india, scheme, minister, new, project\n",
      "Non-merged Cluster 109 Topics: bank, loan, pmegp, branch, sir, 160, manager, 2020, application, scheme\n",
      "Non-merged Cluster 111 Topics: loan, restructuring, bank, 2020, emi, home, sir, covid, moratorium, restructure\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=384, min_dist=0.0, metric='cosine')\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "# Apply HDBSCAN to the obtained embeddings\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size= 10 ,\n",
    "                                                              metric='euclidean',\n",
    "                                                              cluster_selection_method='eom',\n",
    "                                                              prediction_data=True)\n",
    "clusters = clusterer.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Filter the original data to include only those rows that were clustered\n",
    "clustered_indices = [i for i, label in enumerate(clusters) if label != -1]\n",
    "filtered_clustered_data = filtered_data.iloc[clustered_indices]\n",
    "filtered_clustered_data['Cluster'] = clusters[clusters != -1]\n",
    "\n",
    "# Define function to get top words\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "# Extract top words for each cluster\n",
    "cluster_top_words = []\n",
    "for cluster_num in set(clusters) - {-1}:\n",
    "    cluster_docs = filtered_clustered_data[filtered_clustered_data['Cluster'] == cluster_num]['description']\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    top_words_str = ', '.join([word for word, freq in top_words])\n",
    "    cluster_top_words.append(top_words_str)\n",
    "\n",
    "# Generate TF-IDF matrix for top words of each cluster\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_top_words)\n",
    "\n",
    "# Compute cosine similarity between cluster top words\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Identify clusters to merge by finding connected components in the similarity graph\n",
    "similarity_threshold = 0.6\n",
    "graph = cosine_sim_matrix > similarity_threshold\n",
    "n_components, labels = connected_components(csgraph=csr_matrix(graph), directed=False)\n",
    "\n",
    "# Display top words from merged clusters and remaining non-merged clusters\n",
    "for component in range(n_components):\n",
    "    # Find indices of clusters in the current component\n",
    "    indices = np.where(labels == component)[0]\n",
    "    if len(indices) == 1:\n",
    "        # Non-merged cluster\n",
    "        print(f\"Non-merged Cluster {indices[0]} Topics: {cluster_top_words[indices[0]]}\")\n",
    "    else:\n",
    "        # Merged clusters - displaying top words from one of the merging clusters arbitrarily\n",
    "        print(f\"Merged Clusters {indices.tolist()} Topics (from one of them): {cluster_top_words[indices[0]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90f6eae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-merged Cluster 0 Topics: industry, cashew, kerala, banks, workers, processing, non, request, taken, severe\n",
      "Non-merged Cluster 1 Topics: bank, january, ramjee, singh, kaladumra, ps, nagar, siwan, 2008, retired\n",
      "Merged Clusters [2, 8] Topics (from one of them): bank, salary, 01, paid, arrears, 2021, settlement, iba, salaries, board\n",
      "Merged Clusters [3, 22] Topics (from one of them): bank, loan, problem, salp, kumar, sir, tonmay, emi, hdfc, 2021\n",
      "Non-merged Cluster 4 Topics: credit, rs, 000, card, situation, bank, waive, ceremony, help, 50\n",
      "Non-merged Cluster 5 Topics: bank, salary, salaries, paid, settlement, help, month, sir, 2020, future\n",
      "Non-merged Cluster 6 Topics: oral, representation, received, details, applicant, 2021, pmo, complaints, nb, phone\n",
      "Non-merged Cluster 7 Topics: dhfl, fd, holders, public, ibc, process, rbi, resolution, money, 160\n",
      "Non-merged Cluster 9 Topics: bank, salary, bipartite, india, board, employees, 2021, settlement, 11th, payments\n",
      "Merged Clusters [10, 11, 12, 13, 14] Topics (from one of them): pg, rjspetition, fully, scanned, action, initiated, receipt, hard, copy, enclosures\n",
      "Non-merged Cluster 15 Topics: bank, moratorium, kapole, money, possible, expenses, continues, request, hard, earned\n",
      "Non-merged Cluster 16 Topics: banks, bank, public, sector, 39, privatisation, india, sir, private, govt\n",
      "Non-merged Cluster 17 Topics: bank, job, sir, ki, government, 39, india, day, private, modi\n",
      "Non-merged Cluster 18 Topics: ews, certificate, sbi, notification, 2019, 20, fy, category, valid, exam\n",
      "Non-merged Cluster 19 Topics: 160, bank, ibps, exam, sir, interview, 39, rrb, vacancies, baroda\n",
      "Non-merged Cluster 20 Topics: loan, home, sir, bank, house, 39, family, banks, help, pay\n",
      "Non-merged Cluster 21 Topics: rs, charges, 2020, bank, refund, moratorium, card, 19, period, covid\n",
      "Non-merged Cluster 23 Topics: loan, subsidy, bank, pmay, home, sir, application, housing, scheme, account\n",
      "Non-merged Cluster 24 Topics: 160, subsidy, bank, loan, education, account, request, branch, government, scheme\n",
      "Non-merged Cluster 25 Topics: hai, paytm, account, ki, ke, hia, se, maine, bank, kiya\n",
      "Non-merged Cluster 26 Topics: phalodi, bank, pnb, barnch, hai, mai, loan, ka, aap, kiya\n",
      "Non-merged Cluster 27 Topics: hai, bank, number, mera, loan, account, kiya, ke, mein, ka\n",
      "Merged Clusters [28, 29] Topics (from one of them): 160, hai, se, sir, ki, ke, loan, bank, ko, ka\n",
      "Non-merged Cluster 30 Topics: bank, sir, transfer, 39, request, branch, office, working, family, india\n",
      "Non-merged Cluster 31 Topics: bank, maturity, bond, bonds, idfc, 2021, account, folio, 2020, proceeds\n",
      "Non-merged Cluster 32 Topics: bank, transaction, account, aeps, gmail, debited, withdrawal, number, date, address\n",
      "Non-merged Cluster 33 Topics: bank, axis, account, card, sir, branch, debit, customer, rs, time\n",
      "Non-merged Cluster 34 Topics: fastag, toll, sbi, bank, account, customer, rs, sir, plaza, 2021\n",
      "Non-merged Cluster 35 Topics: bank, account, ppf, pf, branch, 2020, kotak, sir, transfer, sbi\n",
      "Non-merged Cluster 36 Topics: bank, tds, deducted, pan, deposit, branch, tax, rs, income, department\n",
      "Non-merged Cluster 37 Topics: bank, pmc, rbi, money, depositors, sir, 39, hard, people, earned\n",
      "Non-merged Cluster 38 Topics: bank, kapol, money, account, op, hard, request, sir, years, earned\n",
      "Non-merged Cluster 39 Topics: bank, account, father, branch, money, late, india, death, nominee, court\n",
      "Non-merged Cluster 40 Topics: bank, account, claim, sir, branch, scheme, insurance, years, 160, policy\n",
      "Non-merged Cluster 41 Topics: policy, insurance, bank, icici, company, health, india, covid, life, nri\n",
      "Non-merged Cluster 42 Topics: sir, pension, account, sbi, delhi, cppc, fraud, chandni, chowk, statement\n",
      "Non-merged Cluster 43 Topics: pension, bank, banks, pensioners, employees, retirees, years, updation, rbi, iba\n",
      "Non-merged Cluster 44 Topics: senior, citizens, citizen, income, rate, pension, retired, india, request, life\n",
      "Non-merged Cluster 45 Topics: pension, bank, branch, family, 2020, sbi, account, sir, cppc, ppo\n",
      "Non-merged Cluster 46 Topics: da, pension, sbi, account, armed, pay, hav, guards, oct, request\n",
      "Non-merged Cluster 47 Topics: loan, banks, bank, people, loans, india, 39, apps, financial, money\n",
      "Non-merged Cluster 48 Topics: bank, loan, kcc, sir, kisan, branch, pm, 39, credit, card\n",
      "Non-merged Cluster 49 Topics: bank, loan, scheme, 2020, rs, gecl, india, credit, 20, msme\n",
      "Non-merged Cluster 50 Topics: cibil, loan, bank, sir, score, credit, report, loans, 39, help\n",
      "Non-merged Cluster 51 Topics: bank, card, credit, restructure, dated, rbi, hdfcbank, issues, hdfc, complaint\n",
      "Merged Clusters [52, 81, 82] Topics (from one of them): bank, hdfc, card, account, credit, 39, rs, sir, pay, payment\n",
      "Non-merged Cluster 53 Topics: loan, bank, sir, help, business, india, land, prime, minister, project\n",
      "Non-merged Cluster 54 Topics: sbi, quot, branch, 2020, grievance, pmopg, bank, email, id, khagra\n",
      "Non-merged Cluster 55 Topics: bank, 160, branch, village, town, people, banks, minister, like, open\n",
      "Non-merged Cluster 56 Topics: bank, customers, india, merger, banks, account, online, union, corporation, banking\n",
      "Non-merged Cluster 57 Topics: bank, baroda, banking, branch, dena, bob, account, given, proper, problem\n",
      "Non-merged Cluster 58 Topics: bank, banks, account, address, card, proof, people, accounts, india, customer\n",
      "Non-merged Cluster 59 Topics: bank, cash, digital, banks, card, payment, charges, money, transaction, india\n",
      "Non-merged Cluster 60 Topics: account, sbi, bank, 160, branch, number, sir, mobile, card, manager\n",
      "Non-merged Cluster 61 Topics: bank, account, india, number, board, directors, patna, branch, icici, manager\n",
      "Non-merged Cluster 62 Topics: bank, cheque, book, account, branch, sir, new, number, savings, old\n",
      "Non-merged Cluster 63 Topics: india, bank, 2021, prime, minister, thank, kindly, sbi, 39, dated\n",
      "Non-merged Cluster 64 Topics: bank, branch, manager, 2020, letter, account, mr, complaint, baroda, sir\n",
      "Non-merged Cluster 65 Topics: bank, 39, complaints, evidences, rbi, complaint, prove, justice, disposed, corruption\n",
      "Non-merged Cluster 66 Topics: loan, 160, sir, bank, education, 39, help, pay, job, family\n",
      "Non-merged Cluster 67 Topics: loan, bank, education, sir, 39, branch, help, applied, student, application\n",
      "Non-merged Cluster 68 Topics: paytm, bank, account, rs, money, help, sir, 39, transaction, pay\n",
      "Non-merged Cluster 69 Topics: loan, gold, bank, sir, finance, pay, time, request, auction, year\n",
      "Non-merged Cluster 70 Topics: bank, rs, 2020, property, loan, land, branch, office, case, 10\n",
      "Non-merged Cluster 71 Topics: loan, bank, sir, 39, family, pay, prime, minister, help, request\n",
      "Non-merged Cluster 72 Topics: loan, bank, hdfc, 39, home, rs, branch, property, housing, sir\n",
      "Non-merged Cluster 73 Topics: bank, loan, idbi, rbi, money, request, home, rate, application, charges\n",
      "Non-merged Cluster 74 Topics: bank, help, pay, sir, credit, time, card, people, loan, 39\n",
      "Non-merged Cluster 75 Topics: grievance, bank, 2020, pmopg, 2021, complaint, number, office, sir, registration\n",
      "Non-merged Cluster 76 Topics: bank, complaint, office, ombudsman, banking, bangalore, rbi, 2020, bo, pmopg\n",
      "Non-merged Cluster 77 Topics: sbi, branch, manager, account, service, 39, bank, rbi, sir, said\n",
      "Non-merged Cluster 78 Topics: bank, account, branch, rs, 39, manager, money, customer, sms, 2020\n",
      "Non-merged Cluster 79 Topics: bank, branch, account, pnb, national, punjab, sir, rs, passbook, card\n",
      "Non-merged Cluster 80 Topics: bank, account, rs, sbi, transaction, money, branch, number, atm, sir\n",
      "Merged Clusters [83, 89] Topics (from one of them): loan, 39, pay, sir, paid, financial, action, family, want, bank\n",
      "Non-merged Cluster 84 Topics: emi, bank, loan, rs, account, pay, sir, 39, charges, paid\n",
      "Non-merged Cluster 85 Topics: loan, emi, bank, sir, pay, 39, help, time, job, request\n",
      "Non-merged Cluster 86 Topics: loan, bank, pmegp, branch, sir, 160, manager, application, scheme, 2020\n",
      "Non-merged Cluster 87 Topics: loan, bank, 39, pay, help, sir, house, job, father, rs\n",
      "Non-merged Cluster 88 Topics: loan, bank, mudra, sir, business, branch, manager, 39, 160, help\n",
      "Non-merged Cluster 90 Topics: loan, business, sir, bank, help, 39, india, banks, start, small\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=45, n_components=384, min_dist=0.0, metric='cosine')\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "# Apply HDBSCAN to the obtained embeddings\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size= 10 ,\n",
    "                                                              metric='euclidean',\n",
    "                                                              cluster_selection_method='eom',\n",
    "                                                              prediction_data=True)\n",
    "clusters = clusterer.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Filter the original data to include only those rows that were clustered\n",
    "clustered_indices = [i for i, label in enumerate(clusters) if label != -1]\n",
    "filtered_clustered_data = filtered_data.iloc[clustered_indices]\n",
    "filtered_clustered_data['Cluster'] = clusters[clusters != -1]\n",
    "\n",
    "# Define function to get top words\n",
    "def get_top_words(documents, n_top_words):\n",
    "    vec = CountVectorizer(stop_words='english').fit(documents)\n",
    "    bag_of_words = vec.transform(documents)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n_top_words]\n",
    "\n",
    "# Extract top words for each cluster\n",
    "cluster_top_words = []\n",
    "for cluster_num in set(clusters) - {-1}:\n",
    "    cluster_docs = filtered_clustered_data[filtered_clustered_data['Cluster'] == cluster_num]['description']\n",
    "    top_words = get_top_words(cluster_docs, 10)\n",
    "    top_words_str = ', '.join([word for word, freq in top_words])\n",
    "    cluster_top_words.append(top_words_str)\n",
    "\n",
    "# Generate TF-IDF matrix for top words of each cluster\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_top_words)\n",
    "\n",
    "# Compute cosine similarity between cluster top words\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Identify clusters to merge by finding connected components in the similarity graph\n",
    "similarity_threshold = 0.6\n",
    "graph = cosine_sim_matrix > similarity_threshold\n",
    "n_components, labels = connected_components(csgraph=csr_matrix(graph), directed=False)\n",
    "\n",
    "# Display top words from merged clusters and remaining non-merged clusters\n",
    "for component in range(n_components):\n",
    "    # Find indices of clusters in the current component\n",
    "    indices = np.where(labels == component)[0]\n",
    "    if len(indices) == 1:\n",
    "        # Non-merged cluster\n",
    "        print(f\"Non-merged Cluster {indices[0]} Topics: {cluster_top_words[indices[0]]}\")\n",
    "    else:\n",
    "        # Merged clusters - displaying top words from one of the merging clusters arbitrarily\n",
    "        print(f\"Merged Clusters {indices.tolist()} Topics (from one of them): {cluster_top_words[indices[0]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf55bd0-2dc6-42e4-afe6-1d590bbbef7a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## FINAL EXPERIMENTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "model_name = 'all-mpnet-base-v2'  # This model is more powerful than MiniLM\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "embeddings = embedding_model.encode(data['description'].tolist(), show_progress_bar=True, batch_size=32)\n",
    "\n",
    "print(\"Embedding of the first document:\", embeddings[0])\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1209a06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (Documents: 33) Topics: scanned, enclosures, petition, shortly, pg, fully, receipt, copy, sent, initiated\n",
      "Cluster 1 (Documents: 44) Topics: scanned, enclosures, shortly, pg, fully, copy, receipt, sent, hard, initiated\n",
      "Cluster 2 (Documents: 34) Topics: cashew, industry, workers, kerala, entrepreneurs, physical, factories, dwelling, coercive, collaterals\n",
      "Cluster 3 (Documents: 24) Topics: rs, charges, refund, moratorium, 156, period, 2020, 08, gesture, suffering\n",
      "Cluster 4 (Documents: 15) Topics: rrb, vacancies, baroda, ibps, withdraw, bob, exam, bank, examination, students\n",
      "Cluster 5 (Documents: 16) Topics: oral, representation, applicant, details, received, gap, nb, complaints, नह, follows\n",
      "Cluster 6 (Documents: 28) Topics: dhfl, fd, holders, ibc, resolution, rbi, public, lenders, 160, nhb\n",
      "Cluster 7 (Documents: 74) Topics: banks, privatisation, public, bank, sector, private, psbs, privatization, psu, employees\n",
      "Cluster 8 (Documents: 21) Topics: loan, salp, tonmay, bank, problem, kumar, aalo, manna, handle, donate\n",
      "Cluster 9 (Documents: 47) Topics: idbi, loan, money, rbi, bank, charges, application, rajesh, policy, home\n",
      "Cluster 10 (Documents: 22) Topics: salary, bank, arrears, salaries, inhuman, iba, board, settlement, paid, 01\n",
      "Cluster 11 (Documents: 36) Topics: salaries, bank, salary, settlement, iba, bipartite, unions, abruptly, arrears, revision\n",
      "Cluster 12 (Documents: 62) Topics: salaries, bank, salary, settlement, abruptly, unions, bipartite, arrears, revision, paid\n",
      "Cluster 13 (Documents: 18) Topics: mudra, loan, business, bank, sir, quot, start, manager, documents, 39\n",
      "Cluster 14 (Documents: 146) Topics: pension, da, sbi, bank, account, cppc, branch, armed, office, delhi\n",
      "Cluster 15 (Documents: 49) Topics: loan, education, nursing, vidyalakshmi, student, branch, able, manager, bank, 39\n",
      "Cluster 16 (Documents: 120) Topics: hai, ka, mai, se, ke, phalodi, ki, loan, barnch, bank\n",
      "Cluster 17 (Documents: 74) Topics: subsidy, pmay, loan, bank, application, home, id, sir, applied, account\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_top_words_per_cluster(documents_by_cluster, n_top_words=10):\n",
    "    # Check if documents_by_cluster contains any valid text\n",
    "    if not any(len(doc.strip()) > 0 for doc in documents_by_cluster):\n",
    "        return [[] for _ in documents_by_cluster]\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents_by_cluster)\n",
    "    \n",
    "    top_words_per_cluster = []\n",
    "    for cluster_idx in range(tfidf_matrix.shape[0]):\n",
    "        feature_array = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "        tfidf_scores = tfidf_matrix[cluster_idx].toarray().flatten()\n",
    "        top_indices = tfidf_scores.argsort()[-n_top_words:][::-1]\n",
    "        top_words = feature_array[top_indices]\n",
    "        top_scores = tfidf_scores[top_indices]\n",
    "        top_words_per_cluster.append(list(zip(top_words, top_scores)))\n",
    "    \n",
    "    return top_words_per_cluster\n",
    "\n",
    "\n",
    "def recursive_clustering(level, data, embeddings, min_cluster_size=15, min_samples=None, results=None):\n",
    "    if results is None:\n",
    "        results = {}\n",
    "\n",
    "    clusterer = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, prediction_data=True)\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    unique_clusters = set(clusters) - {-1}  # Exclude noise if present\n",
    "\n",
    "    # Prepare documents by cluster for class-based TF-IDF\n",
    "    documents_by_cluster = [' '.join(data.iloc[clusters == cluster_id]['description'].tolist()) for cluster_id in unique_clusters]\n",
    "    top_words_per_cluster = get_top_words_per_cluster(documents_by_cluster)\n",
    "\n",
    "    for i, cluster_id in enumerate(unique_clusters):\n",
    "        cluster_label = f\"{level}{cluster_id}\" if level else str(cluster_id)\n",
    "        cluster_mask = clusters == cluster_id\n",
    "        cluster_data = data.iloc[cluster_mask]\n",
    "\n",
    "        # Store top words, their frequencies, and the number of documents in the cluster\n",
    "        results[cluster_label] = {\n",
    "            'top_words': [word for word, _ in top_words_per_cluster[i]],\n",
    "            'no_of_documents': len(cluster_data)\n",
    "        }\n",
    "\n",
    "        # Recursive call if the cluster is large enough\n",
    "        if len(cluster_data) > 100:\n",
    "            new_level = f\"{cluster_label}.\"\n",
    "            recursive_clustering(new_level, cluster_data, embeddings[cluster_mask], min_cluster_size, min_samples, results)\n",
    "\n",
    "    return results\n",
    "\n",
    "hierarchical_topics = recursive_clustering(\"\", filtered_data, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def display_topics(hierarchical_topics, level=0):\n",
    "    for cluster_label, info in hierarchical_topics.items():\n",
    "        indent = '  ' * level\n",
    "        top_words = ', '.join(info['top_words'])\n",
    "        no_of_documents = info['no_of_documents']\n",
    "        print(f\"{indent}Cluster {cluster_label} (Documents: {no_of_documents}) Topics: {top_words}\")\n",
    "\n",
    "display_topics(hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac9bef19-0c71-4218-a507-167577aed747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cfa61dc-4b06-4214-9a48-c5b5d21d6d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d4957e8-c33e-4adf-898d-9962f4bf4cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7958, 768)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3960beeb-0a05-4964-af44-7fbeed9eacc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7958\n"
     ]
    }
   ],
   "source": [
    "# embeddings[0].shape\n",
    "doc_id = list(range(1, 7959))\n",
    "print(len(doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487d8d3-405d-4650-b9c4-73ec51d6954e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e6de8-43b4-4ca4-a6ec-1b6ab560944e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca3ac1-3c2f-48b0-9527-4999d09de32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40269cec-f0d3-4a8e-b979-765c2cc729eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
